---
title: æ•°æ®æ¸…æ´—ä¸SQLè¯­æ³•ç»ƒä¹ 
date: 2025-11-18
tag: ç»ƒä¹ 
categories: æ•°æ®ç§‘å­¦
---

# æ•°æ®æ¸…æ´—ä¸SQLè¯­æ³•ç»ƒä¹ 

## åˆçº§é¢˜ç›®

1. ### ç¼ºå¤±å€¼å¤„ç†

   - ç¼–å†™Pythonè„šæœ¬ï¼Œè¯»å–åŒ…å«å­¦ç”Ÿæˆç»©çš„CSVæ–‡ä»¶ï¼Œæ£€æµ‹å¹¶å¤„ç†ç¼ºå¤±å€¼:
     - æ•°å€¼å‹å­—æ®µç”¨è¯¥åˆ—å‡å€¼å¡«å……
     - æ–‡æœ¬å‹å­—æ®µç”¨"æœªçŸ¥"å¡«å……
     - è¾“å‡ºå¤„ç†å‰åçš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯

   ```python
   import pandas as pd
   import numpy as np
   
   def handle_missing_values():
       """
       å¤„ç†å­¦ç”Ÿæˆç»©æ•°æ®ä¸­çš„ç¼ºå¤±å€¼
       - æ•°å€¼å‹å­—æ®µç”¨è¯¥åˆ—å‡å€¼å¡«å……
       - æ–‡æœ¬å‹å­—æ®µç”¨"æœªçŸ¥"å¡«å……
       - è¾“å‡ºå¤„ç†å‰åçš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯
       """
       
       # åˆ›å»ºæµ‹è¯•æ•°æ®
       data = {
           'id': [1, 2, 3, 4, 5],
           'name': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', None, 'èµµå…­'],
           'subject': ['æ•°å­¦', 'æ•°å­¦', 'è‹±è¯­', 'æ•°å­¦', 'è‹±è¯­'],
           'score': [85, None, 92, 78, 88],
           'exam_date': ['2023-06-01', '2023-06-01', None, '2023-06-01', '2023-06-02']
       }
       
       # åˆ›å»ºDataFrame
       df = pd.DataFrame(data)
       
       print("=" * 50)
       print("åŸå§‹æ•°æ®:")
       print("=" * 50)
       print(df)
       print("\nåŸå§‹æ•°æ®ä¿¡æ¯:")
       print(df.info())
       
       # ç»Ÿè®¡ç¼ºå¤±å€¼
       print("\n" + "=" * 50)
       print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
       print("=" * 50)
       missing_before = df.isnull().sum()
       print("å¤„ç†å‰ç¼ºå¤±å€¼:")
       for col, count in missing_before.items():
           print(f"{col}: {count}ä¸ªç¼ºå¤±å€¼")
       
       # å¤åˆ¶æ•°æ®ç”¨äºå¤„ç†
       df_cleaned = df.copy()
       
       # å¤„ç†æ•°å€¼å‹å­—æ®µçš„ç¼ºå¤±å€¼ï¼ˆç”¨å‡å€¼å¡«å……ï¼‰
       numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns
       for col in numeric_columns:
           if df_cleaned[col].isnull().sum() > 0:
               mean_value = df_cleaned[col].mean()
               df_cleaned[col].fillna(mean_value, inplace=True)
               print(f"\næ•°å€¼å­—æ®µ '{col}' ç”¨å‡å€¼ {mean_value:.2f} å¡«å……")
       
       # å¤„ç†æ–‡æœ¬å‹å­—æ®µçš„ç¼ºå¤±å€¼ï¼ˆç”¨"æœªçŸ¥"å¡«å……ï¼‰
       text_columns = df_cleaned.select_dtypes(include=['object']).columns
       for col in text_columns:
           if df_cleaned[col].isnull().sum() > 0:
               df_cleaned[col].fillna('æœªçŸ¥', inplace=True)
               print(f"æ–‡æœ¬å­—æ®µ '{col}' ç”¨ 'æœªçŸ¥' å¡«å……")
       
       # ç»Ÿè®¡å¤„ç†åçš„ç¼ºå¤±å€¼
       missing_after = df_cleaned.isnull().sum()
       
       print("\n" + "=" * 50)
       print("å¤„ç†åçš„æ•°æ®:")
       print("=" * 50)
       print(df_cleaned)
       
       print("\n" + "=" * 50)
       print("æ•°æ®ç»Ÿè®¡ä¿¡æ¯å¯¹æ¯”:")
       print("=" * 50)
       
       # å¤„ç†å‰åçš„æè¿°æ€§ç»Ÿè®¡
       print("\nå¤„ç†å‰æ•°æ®æè¿°:")
       print(df.describe(include='all'))
       
       print("\nå¤„ç†åæ•°æ®æè¿°:")
       print(df_cleaned.describe(include='all'))
       
       # ç¼ºå¤±å€¼å¤„ç†æ€»ç»“
       print("\n" + "=" * 50)
       print("ç¼ºå¤±å€¼å¤„ç†æ€»ç»“:")
       print("=" * 50)
       for col in df.columns:
           before = missing_before[col]
           after = missing_after[col]
           print(f"{col}: å¤„ç†å‰ {before}ä¸ªç¼ºå¤±å€¼ â†’ å¤„ç†å {after}ä¸ªç¼ºå¤±å€¼")
       
       # éªŒè¯æ•°æ®ç±»å‹
       print("\n" + "=" * 50)
       print("å¤„ç†åæ•°æ®ç±»å‹:")
       print("=" * 50)
       print(df_cleaned.dtypes)
       
       return df, df_cleaned
   
   def save_test_data():
       """ä¿å­˜æµ‹è¯•æ•°æ®åˆ°CSVæ–‡ä»¶"""
       data = {
           'id': [1, 2, 3, 4, 5],
           'name': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', None, 'èµµå…­'],
           'subject': ['æ•°å­¦', 'æ•°å­¦', 'è‹±è¯­', 'æ•°å­¦', 'è‹±è¯­'],
           'score': [85, None, 92, 78, 88],
           'exam_date': ['2023-06-01', '2023-06-01', None, '2023-06-01', '2023-06-02']
       }
       
       df = pd.DataFrame(data)
       df.to_csv('students.csv', index=False, encoding='utf-8-sig')
       print("æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ° 'students.csv'")
   
   def load_and_process_from_file():
       """ä»æ–‡ä»¶åŠ è½½å¹¶å¤„ç†æ•°æ®"""
       try:
           # ä»CSVæ–‡ä»¶è¯»å–æ•°æ®
           df = pd.read_csv('students.csv')
           
           print("=" * 50)
           print("ä»æ–‡ä»¶åŠ è½½çš„æ•°æ®:")
           print("=" * 50)
           print(df)
           
           # å¤„ç†ç¼ºå¤±å€¼
           df_cleaned = df.copy()
           
           # æ•°å€¼å‹å­—æ®µç”¨å‡å€¼å¡«å……
           numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns
           for col in numeric_cols:
               if df_cleaned[col].isnull().any():
                   mean_val = df_cleaned[col].mean()
                   df_cleaned[col].fillna(mean_val, inplace=True)
                   print(f"å¡«å……æ•°å€¼å­—æ®µ '{col}': å‡å€¼ = {mean_val:.2f}")
           
           # æ–‡æœ¬å‹å­—æ®µç”¨"æœªçŸ¥"å¡«å……
           text_cols = df_cleaned.select_dtypes(include=['object']).columns
           for col in text_cols:
               if df_cleaned[col].isnull().any():
                   df_cleaned[col].fillna('æœªçŸ¥', inplace=True)
                   print(f"å¡«å……æ–‡æœ¬å­—æ®µ '{col}': ç”¨ 'æœªçŸ¥'")
           
           print("\n" + "=" * 50)
           print("æ–‡ä»¶æ•°æ®å¤„ç†ç»“æœ:")
           print("=" * 50)
           print(df_cleaned)
           
           return df_cleaned
           
       except FileNotFoundError:
           print("æ–‡ä»¶ 'students.csv' æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ save_test_data() åˆ›å»ºæµ‹è¯•æ–‡ä»¶")
           return None
   
   if __name__ == "__main__":
       # ä¿å­˜æµ‹è¯•æ•°æ®
       save_test_data()
       
       print("\n" + "=" * 80)
       print("ä¸»å¤„ç†ç¨‹åº")
       print("=" * 80)
       
       # ä¸»è¦å¤„ç†é€»è¾‘
       original_df, cleaned_df = handle_missing_values()
       
       print("\n" + "=" * 80)
       print("æ–‡ä»¶å¤„ç†æµ‹è¯•")
       print("=" * 80)
       
       # ä»æ–‡ä»¶å¤„ç†çš„æµ‹è¯•
       load_and_process_from_file()
       
       print("\n" + "=" * 80)
       print("å¤„ç†å®Œæˆï¼")
       print("=" * 80)
   ```

2. ### é‡å¤æ•°æ®å¤„ç†

   - ç¼–å†™è„šæœ¬è¯†åˆ«å¹¶åˆ é™¤å®Œå…¨é‡å¤çš„è®°å½•ï¼ŒåŸºäº"å­¦å·"+"ç§‘ç›®"ç»„åˆåˆ é™¤éƒ¨åˆ†é‡å¤è®°å½•ï¼Œä¿ç•™æœ€æ–°æ—¥æœŸçš„è®°å½•

   ```python
   import pandas as pd
   import numpy as np
   from datetime import datetime
   
   def create_test_data():
       """åˆ›å»ºæµ‹è¯•æ•°æ®"""
       data = {
           'student_id': [1001, 1001, 1002, 1001, 1003, 1002, 1004, 1001, 1003],
           'name': ['å¼ ä¸‰', 'å¼ ä¸‰', 'æå››', 'å¼ ä¸‰', 'ç‹äº”', 'æå››', 'èµµå…­', 'å¼ ä¸‰', 'ç‹äº”'],
           'subject': ['æ•°å­¦', 'æ•°å­¦', 'è‹±è¯­', 'æ•°å­¦', 'æ•°å­¦', 'è‹±è¯­', 'ç‰©ç†', 'è‹±è¯­', 'æ•°å­¦'],
           'score': [85, 90, 78, 85, 92, 82, 88, 87, 95],
           'record_date': ['2023-01-15', '2023-02-20', '2023-01-15', '2023-01-15', 
                          '2023-01-16', '2023-02-18', '2023-01-20', '2023-03-10', '2023-02-25']
       }
       
       df = pd.DataFrame(data)
       df.to_csv('student_records.csv', index=False, encoding='utf-8-sig')
       print("æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ° 'student_records.csv'")
       return df
   
   def detect_and_remove_duplicates(df):
       """
       æ£€æµ‹å’Œå¤„ç†é‡å¤æ•°æ®
       1. åˆ é™¤å®Œå…¨é‡å¤çš„è®°å½•
       2. åŸºäº"å­¦å·+ç§‘ç›®"ç»„åˆåˆ é™¤éƒ¨åˆ†é‡å¤è®°å½•ï¼Œä¿ç•™æœ€æ–°æ—¥æœŸçš„è®°å½•
       """
       
       print("=" * 60)
       print("åŸå§‹æ•°æ®:")
       print("=" * 60)
       print(df)
       print(f"\nåŸå§‹æ•°æ®è®°å½•æ•°: {len(df)}")
       
       # 1. æ£€æµ‹å®Œå…¨é‡å¤çš„è®°å½•
       complete_duplicates = df[df.duplicated(keep=False)]
       print("\n" + "=" * 60)
       print("å®Œå…¨é‡å¤çš„è®°å½•:")
       print("=" * 60)
       if not complete_duplicates.empty:
           print(complete_duplicates)
       else:
           print("æ²¡æœ‰æ‰¾åˆ°å®Œå…¨é‡å¤çš„è®°å½•")
       
       # åˆ é™¤å®Œå…¨é‡å¤çš„è®°å½•
       df_cleaned = df.drop_duplicates()
       print(f"\nåˆ é™¤å®Œå…¨é‡å¤è®°å½•åï¼Œå‰©ä½™è®°å½•æ•°: {len(df_cleaned)}")
       
       # 2. æ£€æµ‹åŸºäº"å­¦å·+ç§‘ç›®"çš„éƒ¨åˆ†é‡å¤è®°å½•
       print("\n" + "=" * 60)
       print("åŸºäº'å­¦å·+ç§‘ç›®'çš„é‡å¤è®°å½•åˆ†æ:")
       print("=" * 60)
       
       # æ‰¾å‡ºæœ‰é‡å¤çš„ç»„åˆ
       duplicate_combinations = df_cleaned[df_cleaned.duplicated(subset=['student_id', 'subject'], keep=False)]
       
       if not duplicate_combinations.empty:
           print("å‘ç°ä»¥ä¸‹é‡å¤ç»„åˆ:")
           # æŒ‰å­¦å·å’Œç§‘ç›®åˆ†ç»„æ˜¾ç¤º
           grouped = duplicate_combinations.groupby(['student_id', 'subject'])
           for (stu_id, subj), group in grouped:
               print(f"\nå­¦å·: {stu_id}, ç§‘ç›®: {subj}")
               print(group[['name', 'score', 'record_date']].to_string(index=False))
       else:
           print("æ²¡æœ‰æ‰¾åˆ°åŸºäº'å­¦å·+ç§‘ç›®'çš„é‡å¤è®°å½•")
       
       # 3. å¤„ç†éƒ¨åˆ†é‡å¤è®°å½•ï¼šä¿ç•™æ¯ä¸ªå­¦å·+ç§‘ç›®ç»„åˆä¸­æ—¥æœŸæœ€æ–°çš„è®°å½•
       print("\n" + "=" * 60)
       print("å¤„ç†éƒ¨åˆ†é‡å¤è®°å½•ï¼ˆä¿ç•™æœ€æ–°æ—¥æœŸï¼‰:")
       print("=" * 60)
       
       # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
       df_cleaned['record_date'] = pd.to_datetime(df_cleaned['record_date'])
       
       # æŒ‰å­¦å·å’Œç§‘ç›®åˆ†ç»„ï¼Œä¿ç•™æ¯ä¸ªç»„å†…æ—¥æœŸæœ€å¤§çš„è®°å½•
       df_final = df_cleaned.sort_values('record_date', ascending=False).drop_duplicates(
           subset=['student_id', 'subject'], 
           keep='first'
       ).sort_values(['student_id', 'subject'])
       
       print(f"å¤„ç†éƒ¨åˆ†é‡å¤åï¼Œæœ€ç»ˆè®°å½•æ•°: {len(df_final)}")
       
       return df_cleaned, df_final
   
   def analyze_duplicates(df_cleaned, df_final):
       """åˆ†æé‡å¤æ•°æ®å¤„ç†ç»“æœ"""
       
       print("\n" + "=" * 60)
       print("é‡å¤æ•°æ®å¤„ç†ç»“æœåˆ†æ:")
       print("=" * 60)
       
       # è®¡ç®—å„ç§é‡å¤æ•°é‡
       complete_dup_count = len(df_cleaned) - len(df_cleaned.drop_duplicates())
       partial_dup_count = len(df_cleaned) - len(df_final)
       total_dup_count = len(df_cleaned) - len(df_final) + complete_dup_count
       
       print(f"å®Œå…¨é‡å¤è®°å½•æ•°: {complete_dup_count}")
       print(f"éƒ¨åˆ†é‡å¤è®°å½•æ•°: {partial_dup_count}")
       print(f"æ€»åˆ é™¤è®°å½•æ•°: {total_dup_count}")
       print(f"æœ€ç»ˆæœ‰æ•ˆè®°å½•æ•°: {len(df_final)}")
       
       # æ˜¾ç¤ºæœ€ç»ˆæ•°æ®
       print("\n" + "=" * 60)
       print("æœ€ç»ˆå¤„ç†ç»“æœ:")
       print("=" * 60)
       print(df_final.sort_values(['student_id', 'subject']).to_string(index=False))
       
       # éªŒè¯å¤„ç†ç»“æœ
       print("\n" + "=" * 60)
       print("æ•°æ®è´¨é‡éªŒè¯:")
       print("=" * 60)
       
       # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å®Œå…¨é‡å¤
       remaining_complete_dups = df_final[df_final.duplicated()]
       if remaining_complete_dups.empty:
           print("âœ“ æ²¡æœ‰å®Œå…¨é‡å¤è®°å½•")
       else:
           print("âœ— ä»ç„¶å­˜åœ¨å®Œå…¨é‡å¤è®°å½•")
           print(remaining_complete_dups)
       
       # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éƒ¨åˆ†é‡å¤
       remaining_partial_dups = df_final[df_final.duplicated(subset=['student_id', 'subject'])]
       if remaining_partial_dups.empty:
           print("âœ“ æ²¡æœ‰'å­¦å·+ç§‘ç›®'é‡å¤è®°å½•")
       else:
           print("âœ— ä»ç„¶å­˜åœ¨'å­¦å·+ç§‘ç›®'é‡å¤è®°å½•")
           print(remaining_partial_dups)
       
       return total_dup_count
   
   def save_cleaned_data(df_final):
       """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
       df_final['record_date'] = df_final['record_date'].dt.strftime('%Y-%m-%d')
       df_final.to_csv('student_records_cleaned.csv', index=False, encoding='utf-8-sig')
       print(f"\næ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ° 'student_records_cleaned.csv'")
   
   def advanced_duplicate_detection(df):
       """é«˜çº§é‡å¤æ£€æµ‹åŠŸèƒ½"""
       print("\n" + "=" * 60)
       print("é«˜çº§é‡å¤æ£€æµ‹:")
       print("=" * 60)
       
       # æ£€æµ‹å¯èƒ½çš„è¿‘ä¼¼é‡å¤ï¼ˆåŸºäºå§“åå’Œç§‘ç›®ï¼‰
       df['name_subject'] = df['name'] + '_' + df['subject']
       name_subject_duplicates = df[df.duplicated(subset=['name_subject'], keep=False)]
       
       if not name_subject_duplicates.empty:
           print("åŸºäº'å§“å+ç§‘ç›®'çš„å¯èƒ½é‡å¤:")
           print(name_subject_duplicates[['student_id', 'name', 'subject', 'score', 'record_date']].to_string(index=False))
       
       # æ£€æµ‹å¼‚å¸¸åˆ†æ•°å˜åŒ–
       df_sorted = df.sort_values(['student_id', 'subject', 'record_date'])
       df_sorted['score_change'] = df_sorted.groupby(['student_id', 'subject'])['score'].diff()
       
       large_changes = df_sorted[np.abs(df_sorted['score_change']) > 15]  # åˆ†æ•°å˜åŒ–è¶…è¿‡15åˆ†
       if not large_changes.empty:
           print("\næ£€æµ‹åˆ°å¼‚å¸¸åˆ†æ•°å˜åŒ–:")
           print(large_changes[['student_id', 'name', 'subject', 'score', 'score_change', 'record_date']].to_string(index=False))
   
   def main():
       """ä¸»å‡½æ•°"""
       print("é‡å¤æ•°æ®å¤„ç†è„šæœ¬")
       print("=" * 80)
       
       # åˆ›å»ºæµ‹è¯•æ•°æ®
       original_df = create_test_data()
       
       # å¤„ç†é‡å¤æ•°æ®
       df_cleaned, df_final = detect_and_remove_duplicates(original_df)
       
       # åˆ†æç»“æœ
       total_removed = analyze_duplicates(df_cleaned, df_final)
       
       # ä¿å­˜ç»“æœ
       save_cleaned_data(df_final)
       
       # é«˜çº§æ£€æµ‹
       advanced_duplicate_detection(original_df)
       
       print("\n" + "=" * 80)
       print("å¤„ç†å®Œæˆ!")
       print(f"æ€»å…±åˆ é™¤äº† {total_removed} æ¡é‡å¤è®°å½•")
       print(f"æ•°æ®æ¸…æ´—ç‡: {total_removed/len(original_df)*100:.1f}%")
       print("=" * 80)
   
   if __name__ == "__main__":
       main()
   ```

3. ### æ•°æ®ç±»å‹è½¬æ¢

   - ç¼–å†™è„šæœ¬å°†æ•°æ®ä¸­çš„æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡ï¼Œæ•°å€¼å­—ç¬¦ä¸²è½¬æ¢ä¸ºfloatç±»å‹ï¼Œå¤„ç†è½¬æ¢å¼‚å¸¸

## ä¸­çº§é¢˜ç›®

1. ### å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†

   - ç¼–å†™è„šæœ¬æ£€æµ‹å­¦ç”Ÿæˆç»©ä¸­çš„å¼‚å¸¸å€¼ï¼ˆå¦‚<0æˆ–>100ï¼‰,ç”¨3ÏƒåŸåˆ™æˆ–IQRæ–¹æ³•è¯†åˆ«å¼‚å¸¸ï¼Œåˆ†åˆ«ç”¨åˆ é™¤ã€æˆªæ–­å’Œå‡å€¼æ›¿æ¢ä¸‰ç§æ–¹å¼å¤„ç†

   ```python
   import pandas as pd
   import numpy as np
   from datetime import datetime
   import re
   
   def create_test_data():
       """åˆ›å»ºåŒ…å«æ··åˆæ•°æ®ç±»å‹çš„æµ‹è¯•æ•°æ®"""
       data = {
           'id': [1, 2, 3, 4, 5, 6],
           'date_str': ['2023-01-15', '2023/02/20', '2023-03-25', 'invalid_date', '2023-05-10', '15-06-2023'],
           'price_str': ['125.50', 'abc', '78.00', '45.30', '120.00', '99,99'],
           'quantity': ['10', '5', '8', '12', '15', 'twenty'],
           'status': ['active', 'inactive', 'pending', 'active', 'completed', 'active'],
           'mixed_numeric': ['100', '200.5', '300', 'N/A', '500.75', '600']
       }
       
       df = pd.DataFrame(data)
       df.to_csv('mixed_data.csv', index=False, encoding='utf-8-sig')
       print("æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ° 'mixed_data.csv'")
       return df
   
   def analyze_data_types(df):
       """åˆ†ææ•°æ®çš„å½“å‰ç±»å‹å’Œå†…å®¹"""
       print("=" * 70)
       print("æ•°æ®åˆå§‹çŠ¶æ€åˆ†æ:")
       print("=" * 70)
       print(df)
       
       print("\n" + "=" * 70)
       print("æ•°æ®ç±»å‹ä¿¡æ¯:")
       print("=" * 70)
       print(df.info())
       
       print("\n" + "=" * 70)
       print "å„åˆ—å”¯ä¸€å€¼ç¤ºä¾‹:")
       print("=" * 70)
       for column in df.columns:
           unique_values = df[column].unique()
           print(f"{column}: {list(unique_values[:5])} {'...' if len(unique_values) > 5 else ''}")
   
   def convert_date_column(df, column_name):
       """
       å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡
       æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼
       """
       print(f"\nå¤„ç†æ—¥æœŸåˆ—: {column_name}")
       original_values = df[column_name].copy()
       
       # é¢„å¤„ç†æ—¥æœŸå­—ç¬¦ä¸²
       df[column_name + '_cleaned'] = df[column_name].astype(str).str.strip()
       
       # å®šä¹‰å¤šç§æ—¥æœŸæ ¼å¼å°è¯•
       date_formats = [
           '%Y-%m-%d',     # 2023-01-15
           '%Y/%m/%d',     # 2023/01/15
           '%d-%m-%Y',     # 15-01-2023
           '%m/%d/%Y',     # 01/15/2023
           '%d.%m.%Y',     # 15.01.2023
       ]
       
       # å°è¯•è½¬æ¢æ—¥æœŸ
       converted_dates = []
       conversion_status = []
       
       for value in df[column_name + '_cleaned']:
           converted = None
           status = "å¤±è´¥"
           
           if pd.isna(value) or value.lower() in ['', 'nan', 'none', 'invalid_date']:
               converted = pd.NaT
               status = "ç©ºå€¼"
           else:
               # å°è¯•æ¯ç§æ ¼å¼
               for fmt in date_formats:
                   try:
                       converted = datetime.strptime(value, fmt)
                       status = "æˆåŠŸ"
                       break
                   except ValueError:
                       continue
           
           converted_dates.append(converted)
           conversion_status.append(status)
       
       # æ·»åŠ è½¬æ¢ç»“æœåˆ°DataFrame
       df[column_name + '_converted'] = converted_dates
       df[column_name + '_conversion_status'] = conversion_status
       
       # ç»Ÿè®¡è½¬æ¢ç»“æœ
       success_count = conversion_status.count("æˆåŠŸ")
       failure_count = conversion_status.count("å¤±è´¥")
       null_count = conversion_status.count("ç©ºå€¼")
       
       print(f"æ—¥æœŸè½¬æ¢ç»“æœ: æˆåŠŸ={success_count}, å¤±è´¥={failure_count}, ç©ºå€¼={null_count}")
       
       # æ˜¾ç¤ºè½¬æ¢è¯¦æƒ…
       print("\næ—¥æœŸè½¬æ¢è¯¦æƒ…:")
       conversion_details = pd.DataFrame({
           'åŸå§‹å€¼': original_values,
           'è½¬æ¢å': df[column_name + '_converted'],
           'çŠ¶æ€': conversion_status
       })
       print(conversion_details.to_string(index=False))
       
       return df
   
   def convert_numeric_column(df, column_name):
       """
       å°†æ•°å€¼å­—ç¬¦ä¸²è½¬æ¢ä¸ºfloatç±»å‹
       å¤„ç†å„ç§æ•°å€¼æ ¼å¼å’Œå¼‚å¸¸æƒ…å†µ
       """
       print(f"\nå¤„ç†æ•°å€¼åˆ—: {column_name}")
       original_values = df[column_name].copy()
       
       # é¢„å¤„ç†æ•°å€¼å­—ç¬¦ä¸²
       df[column_name + '_cleaned'] = df[column_name].astype(str).str.strip()
       
       # å¤„ç†å¸¸è§çš„æ•°å€¼æ ¼å¼é—®é¢˜
       df[column_name + '_cleaned'] = df[column_name + '_cleaned'].str.replace(',', '.')  # é€—å·è½¬ç‚¹
       df[column_name + '_cleaned'] = df[column_name + '_cleaned'].str.replace(r'[^\d.-]', '', regex=True)  # ç§»é™¤éæ•°å­—å­—ç¬¦
       
       # å°è¯•è½¬æ¢æ•°å€¼
       converted_values = []
       conversion_status = []
       
       for value in df[column_name + '_cleaned']:
           converted = None
           status = "å¤±è´¥"
           
           if pd.isna(value) or value.lower() in ['', 'nan', 'none', 'n/a']:
               converted = np.nan
               status = "ç©ºå€¼"
           else:
               try:
                   # å°è¯•ç›´æ¥è½¬æ¢
                   converted = float(value)
                   status = "æˆåŠŸ"
               except ValueError:
                   # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•æå–æ•°å­—
                   numbers = re.findall(r'-?\d+\.?\d*', value)
                   if numbers:
                       converted = float(numbers[0])
                       status = "éƒ¨åˆ†æˆåŠŸï¼ˆæå–æ•°å­—ï¼‰"
                   else:
                       converted = np.nan
                       status = "å¤±è´¥"
           
           converted_values.append(converted)
           conversion_status.append(status)
       
       # æ·»åŠ è½¬æ¢ç»“æœåˆ°DataFrame
       df[column_name + '_converted'] = converted_values
       df[column_name + '_conversion_status'] = conversion_status
       
       # ç»Ÿè®¡è½¬æ¢ç»“æœ
       success_count = conversion_status.count("æˆåŠŸ") + conversion_status.count("éƒ¨åˆ†æˆåŠŸï¼ˆæå–æ•°å­—ï¼‰")
       failure_count = conversion_status.count("å¤±è´¥")
       null_count = conversion_status.count("ç©ºå€¼")
       
       print(f"æ•°å€¼è½¬æ¢ç»“æœ: æˆåŠŸ={success_count}, å¤±è´¥={failure_count}, ç©ºå€¼={null_count}")
       
       # æ˜¾ç¤ºè½¬æ¢è¯¦æƒ…
       print("\næ•°å€¼è½¬æ¢è¯¦æƒ…:")
       conversion_details = pd.DataFrame({
           'åŸå§‹å€¼': original_values,
           'è½¬æ¢å': df[column_name + '_converted'],
           'çŠ¶æ€': conversion_status
       })
       print(conversion_details.to_string(index=False))
       
       return df
   
   def handle_conversion_errors(df):
       """å¤„ç†è½¬æ¢é”™è¯¯ï¼Œæä¾›é”™è¯¯æŠ¥å‘Šå’Œä¿®å¤å»ºè®®"""
       print("\n" + "=" * 70)
       print("è½¬æ¢é”™è¯¯æŠ¥å‘Š:")
       print("=" * 70)
       
       error_report = []
       
       # æ£€æŸ¥æ—¥æœŸè½¬æ¢é”™è¯¯
       date_columns = [col for col in df.columns if '_conversion_status' in col and 'date' in col]
       for col in date_columns:
           base_col = col.replace('_conversion_status', '')
           failed_mask = df[col] == 'å¤±è´¥'
           if failed_mask.any():
               failed_data = df[failed_mask][[base_col, base_col + '_converted']]
               for idx, row in failed_data.iterrows():
                   error_report.append({
                       'è¡Œå·': idx,
                       'åˆ—å': base_col,
                       'åŸå§‹å€¼': row[base_col],
                       'é—®é¢˜ç±»å‹': 'æ—¥æœŸæ ¼å¼æ— æ³•è¯†åˆ«',
                       'å»ºè®®': 'æ£€æŸ¥æ—¥æœŸæ ¼å¼æ˜¯å¦ç¬¦åˆæ ‡å‡†æ ¼å¼'
                   })
       
       # æ£€æŸ¥æ•°å€¼è½¬æ¢é”™è¯¯
       numeric_columns = [col for col in df.columns if '_conversion_status' in col and 'date' not in col]
       for col in numeric_columns:
           base_col = col.replace('_conversion_status', '')
           failed_mask = df[col] == 'å¤±è´¥'
           if failed_mask.any():
               failed_data = df[failed_mask][[base_col, base_col + '_converted']]
               for idx, row in failed_data.iterrows():
                   error_report.append({
                       'è¡Œå·': idx,
                       'åˆ—å': base_col,
                       'åŸå§‹å€¼': row[base_col],
                       'é—®é¢˜ç±»å‹': 'æ•°å€¼æ ¼å¼æ— æ³•è½¬æ¢',
                       'å»ºè®®': 'æ£€æŸ¥æ˜¯å¦åŒ…å«éæ•°å­—å­—ç¬¦'
                   })
       
       if error_report:
           error_df = pd.DataFrame(error_report)
           print("å‘ç°ä»¥ä¸‹è½¬æ¢é”™è¯¯:")
           print(error_df.to_string(index=False))
       else:
           print("æœªå‘ç°è½¬æ¢é”™è¯¯")
       
       return error_report
   
   def create_final_dataframe(df):
       """åˆ›å»ºæœ€ç»ˆçš„æ•°æ®æ¡†ï¼ŒåŒ…å«è½¬æ¢åçš„æ•°æ®"""
       print("\n" + "=" * 70)
       print("åˆ›å»ºæœ€ç»ˆæ•°æ®æ¡†:")
       print("=" * 70)
       
       # é€‰æ‹©åŸå§‹åˆ—å’Œè½¬æ¢åçš„åˆ—
       original_columns = [col for col in df.columns if not any(x in col for x in ['_cleaned', '_converted', '_conversion_status'])]
       converted_columns = [col for col in df.columns if '_converted' in col and '_conversion_status' not in col]
       
       # åˆ›å»ºæœ€ç»ˆæ•°æ®æ¡†
       final_data = {}
       
       # æ·»åŠ åŸå§‹çš„éè½¬æ¢åˆ—
       for col in original_columns:
           if 'date_str' not in col and 'price_str' not in col and 'quantity' not in col and 'mixed_numeric' not in col:
               final_data[col] = df[col]
       
       # æ·»åŠ è½¬æ¢åçš„åˆ—ï¼ˆé‡å‘½åä¸ºåŸå§‹åˆ—åï¼‰
       for conv_col in converted_columns:
           original_name = conv_col.replace('_converted', '')
           final_data[original_name] = df[conv_col]
       
       final_df = pd.DataFrame(final_data)
       
       print("æœ€ç»ˆæ•°æ®æ¡†:")
       print(final_df)
       print("\næœ€ç»ˆæ•°æ®ç±»å‹:")
       print(final_df.dtypes)
       
       return final_df
   
   def generate_conversion_summary(df, final_df):
       """ç”Ÿæˆè½¬æ¢æ€»ç»“æŠ¥å‘Š"""
       print("\n" + "=" * 70)
       print("æ•°æ®ç±»å‹è½¬æ¢æ€»ç»“:")
       print("=" * 70)
       
       # åŸå§‹æ•°æ®ç±»å‹
       print("\nåŸå§‹æ•°æ®ç±»å‹:")
       for col in df.select_dtypes(include=[object]).columns:
           if not any(x in col for x in ['_cleaned', '_converted', '_conversion_status']):
               sample = df[col].head(3).tolist()
               print(f"  {col}: {df[col].dtype} (ç¤ºä¾‹: {sample})")
       
       # è½¬æ¢åæ•°æ®ç±»å‹
       print("\nè½¬æ¢åæ•°æ®ç±»å‹:")
       for col in final_df.columns:
           sample = final_df[col].head(3).tolist() if len(final_df) > 0 else []
           print(f"  {col}: {final_df[col].dtype} (ç¤ºä¾‹: {sample})")
       
       # è½¬æ¢ç»Ÿè®¡
       date_cols = [col for col in df.columns if 'date_str' in col and '_conversion_status' in col]
       numeric_cols = [col for col in df.columns if 'date_str' not in col and '_conversion_status' in col]
       
       print("\nè½¬æ¢ç»Ÿè®¡:")
       for col in date_cols + numeric_cols:
           base_col = col.replace('_conversion_status', '')
           status_counts = df[col].value_counts()
           print(f"\n{base_col}:")
           for status, count in status_counts.items():
               print(f"  {status}: {count}")
   
   def save_results(final_df, error_report):
       """ä¿å­˜è½¬æ¢ç»“æœå’Œé”™è¯¯æŠ¥å‘Š"""
       # ä¿å­˜æœ€ç»ˆæ•°æ®
       final_df.to_csv('converted_data.csv', index=False, encoding='utf-8-sig')
       print(f"\næœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ° 'converted_data.csv'")
       
       # ä¿å­˜é”™è¯¯æŠ¥å‘Š
       if error_report:
           error_df = pd.DataFrame(error_report)
           error_df.to_csv('conversion_errors.csv', index=False, encoding='utf-8-sig')
           print(f"é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜åˆ° 'conversion_errors.csv'")
       
       # ä¿å­˜è¯¦ç»†è½¬æ¢æ—¥å¿—
       with open('conversion_log.txt', 'w', encoding='utf-8') as f:
           f.write("æ•°æ®ç±»å‹è½¬æ¢æ—¥å¿—\n")
           f.write("=" * 50 + "\n")
           f.write(f"è½¬æ¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
           f.write(f"æ€»è®°å½•æ•°: {len(final_df)}\n")
           f.write("\næœ€ç»ˆæ•°æ®ç±»å‹:\n")
           for col in final_df.columns:
               f.write(f"  {col}: {final_df[col].dtype}\n")
   
   def main():
       """ä¸»å‡½æ•°"""
       print("æ•°æ®ç±»å‹è½¬æ¢è„šæœ¬")
       print("=" * 80)
       
       # åˆ›å»ºæµ‹è¯•æ•°æ®
       df = create_test_data()
       
       # åˆ†æåˆå§‹æ•°æ®
       analyze_data_types(df)
       
       # è½¬æ¢æ—¥æœŸåˆ—
       df = convert_date_column(df, 'date_str')
       
       # è½¬æ¢æ•°å€¼åˆ—
       df = convert_numeric_column(df, 'price_str')
       df = convert_numeric_column(df, 'quantity')
       df = convert_numeric_column(df, 'mixed_numeric')
       
       # å¤„ç†è½¬æ¢é”™è¯¯
       error_report = handle_conversion_errors(df)
       
       # åˆ›å»ºæœ€ç»ˆæ•°æ®æ¡†
       final_df = create_final_dataframe(df)
       
       # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
       generate_conversion_summary(df, final_df)
       
       # ä¿å­˜ç»“æœ
       save_results(final_df, error_report)
       
       print("\n" + "=" * 80)
       print("æ•°æ®ç±»å‹è½¬æ¢å®Œæˆ!")
       print("=" * 80)
   
   if __name__ == "__main__":
       main()
   ```

2. ### æ–‡æœ¬æ•°æ®æ¸…æ´—

   - ç¼–å†™è„šæœ¬æ¸…æ´—åŒ…å«ç”¨æˆ·è¯„ä»·çš„æ•°æ®ï¼š
     - å»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
     - ç»Ÿä¸€æ—¥æœŸæ ¼å¼ï¼ˆå¦‚"2023-01-01"ï¼‰
     - æ ‡å‡†åŒ–æ‰‹æœºå·æ ¼å¼
     - æå–ç”µå­é‚®ä»¶åŸŸå

   ```python
   import pandas as pd
   import numpy as np
   import re
   from datetime import datetime
   import html
   
   def create_test_data():
       """åˆ›å»ºåŒ…å«å„ç§æ–‡æœ¬æ•°æ®é—®é¢˜çš„æµ‹è¯•æ•°æ®"""
       data = {
           'user_id': [1, 2, 3, 4, 5, 6, 7],
           'comment': [
               'è¿™ä¸ªäº§å“<em>å¾ˆå¥½</em>! æˆ‘éå¸¸å–œæ¬¢!!!',
               'ä¸æ»¡æ„!!! è´¨é‡å¤ªå·®äº†ğŸ˜¡',
               'ä¸€èˆ¬èˆ¬å§ï¼Œ<div>æ²¡ä»€ä¹ˆç‰¹åˆ«</div>',
               '<div>æ¨èè´­ä¹°</div> æ€§ä»·æ¯”é«˜ğŸ‘',
               'æœåŠ¡æ€åº¦<strong>æå·®</strong>ï¼ï¼ï¼',
               'äº§å“ä¸é”™ï¼Œä½†é€è´§<em>å¤ªæ…¢</em>äº†ğŸ’”',
               'éå¸¸æ»¡æ„ï¼ä¼šå†æ¬¡è´­ä¹°~'
           ],
           'phone': [
               '13812345678',
               '135-1234-5678',
               '123456',
               '13800138000',
               '+86-136-1234-5678',
               '1371234abcd',
               '139 1234 5678'
           ],
           'email': [
               'user1@example.com',
               'user2@gmail.com',
               'user3',
               'user4@qq.com',
               'test.user@company-domain.org',
               'invalid-email',
               'admin@website.gov.cn'
           ],
           'comment_date': [
               '2023/01/15',
               '2023å¹´2æœˆ20æ—¥',
               '2023-03-25',
               '2023-04-10',
               '2023.05.15',
               '15/06/2023',
               'July 1, 2023'
           ]
       }
       
       df = pd.DataFrame(data)
       df.to_csv('user_comments.csv', index=False, encoding='utf-8-sig')
       print("æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ° 'user_comments.csv'")
       return df
   
   def analyze_text_data(df):
       """åˆ†ææ–‡æœ¬æ•°æ®çš„åˆå§‹çŠ¶æ€"""
       print("=" * 70)
       print("æ–‡æœ¬æ•°æ®åˆå§‹çŠ¶æ€åˆ†æ:")
       print("=" * 70)
       print(df)
       
       print("\n" + "=" * 70)
       print("å„åˆ—æ•°æ®ç¤ºä¾‹:")
       print("=" * 70)
       for column in df.columns:
           if column != 'user_id':
               print(f"\n{column}åˆ—:")
               for i, value in enumerate(df[column].head(3)):
                   print(f"  {i+1}. {value}")
   
   def remove_html_tags(text):
       """å»é™¤HTMLæ ‡ç­¾"""
       if pd.isna(text):
           return text
       
       # æ–¹æ³•1: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
       clean_text = re.sub(r'<[^>]+>', '', str(text))
       
       # æ–¹æ³•2: ä½¿ç”¨html.unescapeå¤„ç†HTMLå®ä½“
       clean_text = html.unescape(clean_text)
       
       return clean_text
   
   def remove_special_characters(text, keep_chinese=True):
       """
       å»é™¤ç‰¹æ®Šå­—ç¬¦
       keep_chinese: æ˜¯å¦ä¿ç•™ä¸­æ–‡å­—ç¬¦
       """
       if pd.isna(text):
           return text
       
       text = str(text)
       
       if keep_chinese:
           # ä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—ã€å¸¸è§æ ‡ç‚¹
           pattern = r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.\,\!\?\;\\/\-\:\@\#\$\%\&\*\(\)\+\=\[\]\{\}\<\>\_\|\~\`\'\"]'
       else:
           # åªä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼å’ŒåŸºæœ¬æ ‡ç‚¹
           pattern = r'[^a-zA-Z0-9\s\.\,\!\?\;\\/\-\:\@\#\$\%\&\*\(\)\+\=\[\]\{\}\<\>\_\|\~\`\'\"]'
       
       clean_text = re.sub(pattern, '', text)
       
       # å»é™¤å¤šä½™çš„ç©ºæ ¼
       clean_text = re.sub(r'\s+', ' ', clean_text).strip()
       
       return clean_text
   
   def clean_comments(df):
       """æ¸…æ´—è¯„è®ºæ–‡æœ¬ - å»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦"""
       print("\n" + "=" * 70)
       print("æ¸…æ´—è¯„è®ºæ–‡æœ¬:")
       print("=" * 70)
       
       # åˆ›å»ºå‰¯æœ¬
       df_clean = df.copy()
       
       # å»é™¤HTMLæ ‡ç­¾
       df_clean['comment_clean'] = df_clean['comment'].apply(remove_html_tags)
       
       # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ï¼‰
       df_clean['comment_clean'] = df_clean['comment_clean'].apply(
           lambda x: remove_special_characters(x, keep_chinese=True)
       )
       
       # æ˜¾ç¤ºæ¸…æ´—å‰åçš„å¯¹æ¯”
       comparison = pd.DataFrame({
           'åŸå§‹è¯„è®º': df_clean['comment'],
           'æ¸…æ´—åè¯„è®º': df_clean['comment_clean']
       })
       
       print("è¯„è®ºæ¸…æ´—å‰åå¯¹æ¯”:")
       for idx, row in comparison.iterrows():
           print(f"\nç”¨æˆ· {idx+1}:")
           print(f"  åŸå§‹: {row['åŸå§‹è¯„è®º']}")
           print(f"  æ¸…æ´—: {row['æ¸…æ´—åè¯„è®º']}")
       
       return df_clean
   
   def standardize_date_format(text):
       """ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DD"""
       if pd.isna(text):
           return text
       
       text = str(text).strip()
       
       # å®šä¹‰å¤šç§æ—¥æœŸæ ¼å¼æ¨¡å¼
       date_patterns = [
           (r'(\d{4})[/å¹´\.](\d{1,2})[/æœˆ\.](\d{1,2})æ—¥?', '{}-{:02d}-{:02d}'),  # 2023/01/15, 2023å¹´1æœˆ15æ—¥
           (r'(\d{1,2})[/æœˆ](\d{1,2})[/æ—¥å¹´](\d{4})', '{}-{:02d}-{:02d}'),       # 15/06/2023
           (r'([A-Za-z]+)\s*(\d{1,2}),\s*(\d{4})', '{}'),                        # July 1, 2023
       ]
       
       month_map = {
           'january': '01', 'february': '02', 'march': '03', 'april': '04',
           'may': '05', 'june': '06', 'july': '07', 'august': '08',
           'september': '09', 'october': '10', 'november': '11', 'december': '12',
           'jan': '01', 'feb': '02', 'mar': '03', 'apr': '04',
           'jun': '06', 'jul': '07', 'aug': '08', 'sep': '09',
           'oct': '10', 'nov': '11', 'dec': '12'
       }
       
       # å°è¯•åŒ¹é…å„ç§æ ¼å¼
       for pattern, format_str in date_patterns:
           match = re.search(pattern, text, re.IGNORECASE)
           if match:
               if 'month' in format_str:  # å¤„ç†è‹±æ–‡æœˆä»½
                   month_name = match.group(1).lower()
                   if month_name in month_map:
                       month = month_map[month_name]
                       day = match.group(2).zfill(2)
                       year = match.group(3)
                       return f"{year}-{month}-{day}"
               else:
                   groups = match.groups()
                   if len(groups) == 3:
                       if len(groups[0]) == 4:  # YYYY-MM-DD æ ¼å¼
                           year, month, day = groups
                       else:  # DD-MM-YYYY æ ¼å¼
                           day, month, year = groups
                       
                       month = month.zfill(2)
                       day = day.zfill(2)
                       return format_str.format(year, int(month), int(day))
       
       # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸå§‹å€¼å¹¶æ ‡è®°
       return f"æ— æ³•è§£æ: {text}"
   
   def clean_dates(df):
       """ç»Ÿä¸€æ—¥æœŸæ ¼å¼"""
       print("\n" + "=" * 70)
       print("ç»Ÿä¸€æ—¥æœŸæ ¼å¼:")
       print("=" * 70)
       
       df_clean = df.copy()
       
       # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
       df_clean['comment_date_standardized'] = df_clean['comment_date'].apply(standardize_date_format)
       
       # æ˜¾ç¤ºæ¸…æ´—å‰åçš„å¯¹æ¯”
       comparison = pd.DataFrame({
           'åŸå§‹æ—¥æœŸ': df_clean['comment_date'],
           'æ ‡å‡†åŒ–æ—¥æœŸ': df_clean['comment_date_standardized']
       })
       
       print("æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–å‰åå¯¹æ¯”:")
       for idx, row in comparison.iterrows():
           print(f"  ç”¨æˆ· {idx+1}: {row['åŸå§‹æ—¥æœŸ']} â†’ {row['æ ‡å‡†åŒ–æ—¥æœŸ']}")
       
       return df_clean
   
   def standardize_phone_number(phone):
       """æ ‡å‡†åŒ–æ‰‹æœºå·æ ¼å¼"""
       if pd.isna(phone):
           return phone
       
       phone = str(phone)
       
       # ç§»é™¤éæ•°å­—å­—ç¬¦
       digits = re.sub(r'\D', '', phone)
       
       # æ£€æŸ¥æ‰‹æœºå·æœ‰æ•ˆæ€§
       if len(digits) == 11:
           # ä¸­å›½æ‰‹æœºå·æ ¼å¼: XXX-XXXX-XXXX
           formatted = f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
           return formatted
       elif len(digits) == 10:
           # å¯èƒ½æ˜¯ç¼ºå°‘å›½å®¶ç ï¼Œå‡è®¾æ˜¯ä¸­å›½æ‰‹æœºå·
           formatted = f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
           return formatted
       else:
           return f"æ— æ•ˆå·ç : {phone}"
   
   def clean_phones(df):
       """æ ‡å‡†åŒ–æ‰‹æœºå·æ ¼å¼"""
       print("\n" + "=" * 70)
       print("æ ‡å‡†åŒ–æ‰‹æœºå·æ ¼å¼:")
       print("=" * 70)
       
       df_clean = df.copy()
       
       # æ ‡å‡†åŒ–æ‰‹æœºå·
       df_clean['phone_standardized'] = df_clean['phone'].apply(standardize_phone_number)
       
       # æ˜¾ç¤ºæ¸…æ´—å‰åçš„å¯¹æ¯”
       comparison = pd.DataFrame({
           'åŸå§‹æ‰‹æœºå·': df_clean['phone'],
           'æ ‡å‡†åŒ–æ‰‹æœºå·': df_clean['phone_standardized']
       })
       
       print("æ‰‹æœºå·æ ‡å‡†åŒ–å‰åå¯¹æ¯”:")
       for idx, row in comparison.iterrows():
           print(f"  ç”¨æˆ· {idx+1}: {row['åŸå§‹æ‰‹æœºå·']} â†’ {row['æ ‡å‡†åŒ–æ‰‹æœºå·']}")
       
       # ç»Ÿè®¡æœ‰æ•ˆæ‰‹æœºå·
       valid_phones = df_clean['phone_standardized'].apply(
           lambda x: not str(x).startswith('æ— æ•ˆå·ç ')
       ).sum()
       
       print(f"\næœ‰æ•ˆæ‰‹æœºå·æ•°é‡: {valid_phones}/{len(df_clean)}")
       
       return df_clean
   
   def extract_email_domain(email):
       """æå–ç”µå­é‚®ä»¶åŸŸå"""
       if pd.isna(email):
           return "æ— é‚®ç®±"
       
       email = str(email).strip()
       
       # éªŒè¯é‚®ç®±æ ¼å¼
       email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
       
       if re.match(email_pattern, email):
           # æå–åŸŸåéƒ¨åˆ†
           domain = email.split('@')[1]
           return domain
       else:
           return "æ— æ•ˆé‚®ç®±"
   
   def extract_email_domains(df):
       """æå–ç”µå­é‚®ä»¶åŸŸå"""
       print("\n" + "=" * 70)
       print("æå–ç”µå­é‚®ä»¶åŸŸå:")
       print("=" * 70)
       
       df_clean = df.copy()
       
       # æå–é‚®ç®±åŸŸå
       df_clean['email_domain'] = df_clean['email'].apply(extract_email_domain)
       
       # æ˜¾ç¤ºæå–ç»“æœ
       comparison = pd.DataFrame({
           'åŸå§‹é‚®ç®±': df_clean['email'],
           'åŸŸå': df_clean['email_domain']
       })
       
       print("é‚®ç®±åŸŸåæå–ç»“æœ:")
       for idx, row in comparison.iterrows():
           print(f"  ç”¨æˆ· {idx+1}: {row['åŸå§‹é‚®ç®±']} â†’ {row['åŸŸå']}")
       
       # ç»Ÿè®¡åŸŸååˆ†å¸ƒ
       domain_counts = df_clean['email_domain'].value_counts()
       print(f"\né‚®ç®±åŸŸååˆ†å¸ƒ:")
       for domain, count in domain_counts.items():
           print(f"  {domain}: {count}ä¸ª")
       
       return df_clean
   
   def create_final_dataframe(df):
       """åˆ›å»ºæœ€ç»ˆæ¸…æ´—åçš„æ•°æ®æ¡†"""
       print("\n" + "=" * 70)
       print("åˆ›å»ºæœ€ç»ˆæ¸…æ´—åçš„æ•°æ®:")
       print("=" * 70)
       
       # é€‰æ‹©éœ€è¦çš„åˆ—
       final_columns = {
           'user_id': 'ç”¨æˆ·ID',
           'comment_clean': 'æ¸…æ´—åè¯„è®º',
           'phone_standardized': 'æ ‡å‡†åŒ–æ‰‹æœºå·',
           'email_domain': 'é‚®ç®±åŸŸå',
           'comment_date_standardized': 'æ ‡å‡†åŒ–æ—¥æœŸ'
       }
       
       # åˆ›å»ºæœ€ç»ˆæ•°æ®æ¡†
       final_data = {}
       for old_col, new_col in final_columns.items():
           if old_col in df.columns:
               final_data[new_col] = df[old_col]
       
       final_df = pd.DataFrame(final_data)
       
       print("æœ€ç»ˆæ¸…æ´—åçš„æ•°æ®:")
       print(final_df.to_string(index=False))
       
       return final_df
   
   def generate_cleaning_report(df, final_df):
       """ç”Ÿæˆæ•°æ®æ¸…æ´—æŠ¥å‘Š"""
       print("\n" + "=" * 70)
       print("æ–‡æœ¬æ•°æ®æ¸…æ´—æŠ¥å‘Š:")
       print("=" * 70)
       
       # è®¡ç®—æ¸…æ´—ç»Ÿè®¡
       original_comments_length = df['comment'].str.len().sum()
       cleaned_comments_length = final_df['æ¸…æ´—åè¯„è®º'].str.len().sum()
       
       valid_phones = final_df['æ ‡å‡†åŒ–æ‰‹æœºå·'].apply(
           lambda x: not str(x).startswith('æ— æ•ˆå·ç ')
       ).sum()
       
       valid_emails = final_df['é‚®ç®±åŸŸå'].apply(
           lambda x: x not in ['æ— é‚®ç®±', 'æ— æ•ˆé‚®ç®±']
       ).sum()
       
       valid_dates = final_df['æ ‡å‡†åŒ–æ—¥æœŸ'].apply(
           lambda x: not str(x).startswith('æ— æ³•è§£æ')
       ).sum()
       
       print(f"è¯„è®ºæ–‡æœ¬é•¿åº¦å‡å°‘: {original_comments_length} â†’ {cleaned_comments_length} "
             f"(å‡å°‘ {((original_comments_length - cleaned_comments_length) / original_comments_length * 100):.1f}%)")
       print(f"æœ‰æ•ˆæ‰‹æœºå·: {valid_phones}/{len(final_df)}")
       print(f"æœ‰æ•ˆé‚®ç®±: {valid_emails}/{len(final_df)}")
       print(f"æœ‰æ•ˆæ—¥æœŸ: {valid_dates}/{len(final_df)}")
       
       # æ˜¾ç¤ºæ•°æ®è´¨é‡æå‡
       quality_metrics = {
           'è¯„è®ºæ¸…æ´åº¦': (cleaned_comments_length / original_comments_length) * 100,
           'æ‰‹æœºå·æœ‰æ•ˆæ€§': (valid_phones / len(final_df)) * 100,
           'é‚®ç®±æœ‰æ•ˆæ€§': (valid_emails / len(final_df)) * 100,
           'æ—¥æœŸæ ‡å‡†åŒ–': (valid_dates / len(final_df)) * 100
       }
       
       print("\næ•°æ®è´¨é‡æŒ‡æ ‡:")
       for metric, score in quality_metrics.items():
           print(f"  {metric}: {score:.1f}%")
   
   def save_cleaned_data(final_df):
       """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
       final_df.to_csv('user_comments_cleaned.csv', index=False, encoding='utf-8-sig')
       print(f"\næ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ° 'user_comments_cleaned.csv'")
   
   def main():
       """ä¸»å‡½æ•°"""
       print("æ–‡æœ¬æ•°æ®æ¸…æ´—è„šæœ¬")
       print("=" * 80)
       
       # åˆ›å»ºæµ‹è¯•æ•°æ®
       df = create_test_data()
       
       # åˆ†æåˆå§‹æ•°æ®
       analyze_text_data(df)
       
       # æ‰§è¡Œå„é¡¹æ¸…æ´—ä»»åŠ¡
       df = clean_comments(df)           # æ¸…æ´—è¯„è®ºæ–‡æœ¬
       df = clean_dates(df)              # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
       df = clean_phones(df)             # æ ‡å‡†åŒ–æ‰‹æœºå·
       df = extract_email_domains(df)    # æå–é‚®ç®±åŸŸå
       
       # åˆ›å»ºæœ€ç»ˆæ•°æ®æ¡†
       final_df = create_final_dataframe(df)
       
       # ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š
       generate_cleaning_report(df, final_df)
       
       # ä¿å­˜ç»“æœ
       save_cleaned_data(final_df)
       
       print("\n" + "=" * 80)
       print("æ–‡æœ¬æ•°æ®æ¸…æ´—å®Œæˆ!")
       print("=" * 80)
   
   if __name__ == "__main__":
       main()
   ```

3. ### æ•°æ®æ ‡å‡†åŒ–

   - ç¼–å†™è„šæœ¬å¯¹æ•°å€¼å‹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼š
     - æœ€å°-æœ€å¤§å½’ä¸€åŒ–
     - Z-scoreæ ‡å‡†åŒ–
     - å°æ•°å®šæ ‡æ ‡å‡†åŒ–
   
   ```python
   import pandas as pd
   import numpy as np
   import matplotlib.pyplot as plt
   import seaborn as sns
   from sklearn.preprocessing import MinMaxScaler, StandardScaler
   import warnings
   warnings.filterwarnings('ignore')
   
   def create_test_data():
       """åˆ›å»ºåŒ…å«ä¸åŒå°ºåº¦å’Œåˆ†å¸ƒçš„æµ‹è¯•æ•°æ®"""
       np.random.seed(42)
       
       # åˆ›å»ºä¸åŒåˆ†å¸ƒçš„æ•°æ®
       data = {
           'product_id': range(1, 101),
           'price': np.random.normal(500, 200, 100).clip(50, 1500),  # æ­£æ€åˆ†å¸ƒ
           'sales_volume': np.random.exponential(500, 100).clip(10, 3000),  # æŒ‡æ•°åˆ†å¸ƒ
           'rating': np.random.uniform(1, 5, 100),  # å‡åŒ€åˆ†å¸ƒ
           'profit': np.random.lognormal(5, 1, 100).clip(-100, 10000),  # å¯¹æ•°æ­£æ€åˆ†å¸ƒ
           'customer_count': np.random.poisson(500, 100)  # æ³Šæ¾åˆ†å¸ƒ
       }
       
       # æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼
       data['price'][0] = 5000  # é«˜å¼‚å¸¸å€¼
       data['sales_volume'][1] = -100  # è´Ÿå¼‚å¸¸å€¼
       data['rating'][2] = 10  # è¶…å‡ºèŒƒå›´çš„å¼‚å¸¸å€¼
       
       df = pd.DataFrame(data)
       df.to_csv('product_data.csv', index=False, encoding='utf-8-sig')
       print("æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ° 'product_data.csv'")
       return df
   
   def analyze_original_data(df):
       """åˆ†æåŸå§‹æ•°æ®çš„åˆ†å¸ƒå’Œç»Ÿè®¡ç‰¹æ€§"""
       print("=" * 70)
       print("åŸå§‹æ•°æ®åˆ†æ:")
       print("=" * 70)
       
       # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
       print("\nåŸå§‹æ•°æ®åŸºæœ¬ç»Ÿè®¡:")
       print(df.describe())
       
       # é€‰æ‹©æ•°å€¼åˆ—
       numeric_columns = df.select_dtypes(include=[np.number]).columns
       numeric_columns = [col for col in numeric_columns if col != 'product_id']
       
       print(f"\næ•°å€¼å‹åˆ—: {list(numeric_columns)}")
       
       # æ£€æŸ¥ç¼ºå¤±å€¼
       missing_values = df[numeric_columns].isnull().sum()
       print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
       for col, count in missing_values.items():
           print(f"  {col}: {count}")
       
       return numeric_columns
   
   def min_max_normalization(df, columns):
       """
       æœ€å°-æœ€å¤§å½’ä¸€åŒ–
       å°†æ•°æ®ç¼©æ”¾åˆ°[0, 1]èŒƒå›´
       """
       print("\n" + "=" * 70)
       print("æœ€å°-æœ€å¤§å½’ä¸€åŒ–:")
       print("=" * 70)
       
       df_normalized = df.copy()
       scaler = MinMaxScaler()
       
       normalized_data = {}
       
       for col in columns:
           # å¤„ç†å¼‚å¸¸å€¼ï¼ˆåœ¨å½’ä¸€åŒ–å‰å¤„ç†è´Ÿå€¼å’Œè¶…å‡ºèŒƒå›´çš„å€¼ï¼‰
           original_data = df[col].values.reshape(-1, 1)
           
           # ä½¿ç”¨sklearnçš„MinMaxScaler
           normalized_col = scaler.fit_transform(original_data)
           normalized_col_name = f"{col}_minmax"
           df_normalized[normalized_col_name] = normalized_col.flatten()
           
           # è®°å½•è½¬æ¢ä¿¡æ¯
           min_val = df[col].min()
           max_val = df[col].max()
           normalized_data[col] = {
               'min': min_val,
               'max': max_val,
               'normalized_min': normalized_col.min(),
               'normalized_max': normalized_col.max()
           }
           
           print(f"{col}: åŸå§‹èŒƒå›´ [{min_val:.2f}, {max_val:.2f}] â†’ å½’ä¸€åŒ–èŒƒå›´ [0, 1]")
       
       # æ˜¾ç¤ºéƒ¨åˆ†å½’ä¸€åŒ–ç»“æœ
       print(f"\nå½’ä¸€åŒ–ç»“æœç¤ºä¾‹ (å‰5è¡Œ):")
       normalized_columns = [f"{col}_minmax" for col in columns]
       print(df_normalized[['product_id'] + normalized_columns].head())
       
       return df_normalized, normalized_data
   
   def z_score_standardization(df, columns):
       """
       Z-scoreæ ‡å‡†åŒ–
       å°†æ•°æ®è½¬æ¢ä¸ºå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„åˆ†å¸ƒ
       """
       print("\n" + "=" * 70)
       print("Z-scoreæ ‡å‡†åŒ–:")
       print("=" * 70)
       
       df_standardized = df.copy()
       scaler = StandardScaler()
       
       standardization_data = {}
       
       for col in columns:
           original_data = df[col].values.reshape(-1, 1)
           
           # ä½¿ç”¨sklearnçš„StandardScaler
           standardized_col = scaler.fit_transform(original_data)
           standardized_col_name = f"{col}_zscore"
           df_standardized[standardized_col_name] = standardized_col.flatten()
           
           # è®°å½•è½¬æ¢ä¿¡æ¯
           mean_val = df[col].mean()
           std_val = df[col].std()
           standardization_data[col] = {
               'mean': mean_val,
               'std': std_val,
               'standardized_mean': standardized_col.mean(),
               'standardized_std': standardized_col.std()
           }
           
           print(f"{col}: åŸå§‹åˆ†å¸ƒ (å‡å€¼={mean_val:.2f}, æ ‡å‡†å·®={std_val:.2f}) â†’ æ ‡å‡†åŒ–åˆ†å¸ƒ (å‡å€¼â‰ˆ0, æ ‡å‡†å·®â‰ˆ1)")
       
       # æ˜¾ç¤ºéƒ¨åˆ†æ ‡å‡†åŒ–ç»“æœ
       print(f"\næ ‡å‡†åŒ–ç»“æœç¤ºä¾‹ (å‰5è¡Œ):")
       standardized_columns = [f"{col}_zscore" for col in columns]
       print(df_standardized[['product_id'] + standardized_columns].head())
       
       return df_standardized, standardization_data
   
   def decimal_scaling_normalization(df, columns):
       """
       å°æ•°å®šæ ‡æ ‡å‡†åŒ–
       é€šè¿‡ç§»åŠ¨å°æ•°ç‚¹ä½ç½®è¿›è¡Œæ ‡å‡†åŒ–
       """
       print("\n" + "=" * 70)
       print("å°æ•°å®šæ ‡æ ‡å‡†åŒ–:")
       print("=" * 70)
       
       df_decimal = df.copy()
       decimal_data = {}
       
       for col in columns:
           original_data = df[col].values
           
           # è®¡ç®—æ¯ä¸ªå€¼æ‰€éœ€çš„å°æ•°ç‚¹ç§»åŠ¨ä½æ•°
           max_abs_val = np.max(np.abs(original_data))
           if max_abs_val == 0:
               k = 1  # é¿å…é™¤ä»¥0
           else:
               k = np.ceil(np.log10(max_abs_val))
           
           # åº”ç”¨å°æ•°å®šæ ‡æ ‡å‡†åŒ–
           decimal_col = original_data / (10 ** k)
           decimal_col_name = f"{col}_decimal"
           df_decimal[decimal_col_name] = decimal_col
           
           # è®°å½•è½¬æ¢ä¿¡æ¯
           decimal_data[col] = {
               'max_absolute': max_abs_val,
               'k_value': k,
               'divisor': 10 ** k,
               'decimal_min': decimal_col.min(),
               'decimal_max': decimal_col.max()
           }
           
           print(f"{col}: æœ€å¤§ç»å¯¹å€¼={max_abs_val:.2f}, k={k}, é™¤æ•°={10**k:.0f}")
           print(f"      æ ‡å‡†åŒ–èŒƒå›´: [{decimal_col.min():.4f}, {decimal_col.max():.4f}]")
       
       # æ˜¾ç¤ºéƒ¨åˆ†å°æ•°å®šæ ‡æ ‡å‡†åŒ–ç»“æœ
       print(f"\nå°æ•°å®šæ ‡æ ‡å‡†åŒ–ç»“æœç¤ºä¾‹ (å‰5è¡Œ):")
       decimal_columns = [f"{col}_decimal" for col in columns]
       print(df_decimal[['product_id'] + decimal_columns].head())
       
       return df_decimal, decimal_data
   
   def compare_normalization_methods(df, numeric_columns):
       """æ¯”è¾ƒä¸åŒæ ‡å‡†åŒ–æ–¹æ³•çš„ç»“æœ"""
       print("\n" + "=" * 70)
       print("æ ‡å‡†åŒ–æ–¹æ³•æ¯”è¾ƒ:")
       print("=" * 70)
       
       comparison_data = []
       
       for col in numeric_columns:
           original_range = f"[{df[col].min():.2f}, {df[col].max():.2f}]"
           original_mean_std = f"Î¼={df[col].mean():.2f}, Ïƒ={df[col].std():.2f}"
           
           minmax_range = f"[{df[f'{col}_minmax'].min():.4f}, {df[f'{col}_minmax'].max():.4f}]"
           minmax_mean_std = f"Î¼={df[f'{col}_minmax'].mean():.4f}, Ïƒ={df[f'{col}_minmax'].std():.4f}"
           
           zscore_range = f"[{df[f'{col}_zscore'].min():.4f}, {df[f'{col}_zscore'].max():.4f}]"
           zscore_mean_std = f"Î¼={df[f'{col}_zscore'].mean():.4f}, Ïƒ={df[f'{col}_zscore'].std():.4f}"
           
           decimal_range = f"[{df[f'{col}_decimal'].min():.4f}, {df[f'{col}_decimal'].max():.4f}]"
           decimal_mean_std = f"Î¼={df[f'{col}_decimal'].mean():.4f}, Ïƒ={df[f'{col}_decimal'].std():.4f}"
           
           comparison_data.append({
               'Column': col,
               'Original': f"{original_range}\n{original_mean_std}",
               'Min-Max': f"{minmax_range}\n{minmax_mean_std}",
               'Z-Score': f"{zscore_range}\n{zscore_mean_std}",
               'Decimal': f"{decimal_range}\n{decimal_mean_std}"
           })
       
       comparison_df = pd.DataFrame(comparison_data)
       print(comparison_df.to_string(index=False, max_colwidth=30))
   
   def visualize_normalization(df, numeric_columns):
       """å¯è§†åŒ–æ ‡å‡†åŒ–å‰åçš„æ•°æ®åˆ†å¸ƒ"""
       print("\n" + "=" * 70)
       print("ç”Ÿæˆæ•°æ®åˆ†å¸ƒå¯è§†åŒ–...")
       print("=" * 70)
       
       # è®¾ç½®å›¾å½¢æ ·å¼
       plt.style.use('default')
       fig, axes = plt.subplots(len(numeric_columns), 4, figsize=(20, 5*len(numeric_columns)))
       
       if len(numeric_columns) == 1:
           axes = axes.reshape(1, -1)
       
       for i, col in enumerate(numeric_columns):
           # åŸå§‹æ•°æ®
           axes[i, 0].hist(df[col], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
           axes[i, 0].set_title(f'Original {col}')
           axes[i, 0].set_xlabel('Value')
           axes[i, 0].set_ylabel('Frequency')
           
           # æœ€å°-æœ€å¤§å½’ä¸€åŒ–
           axes[i, 1].hist(df[f'{col}_minmax'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
           axes[i, 1].set_title(f'Min-Max {col}')
           axes[i, 1].set_xlabel('Normalized Value')
           axes[i, 1].set_ylabel('Frequency')
           
           # Z-scoreæ ‡å‡†åŒ–
           axes[i, 2].hist(df[f'{col}_zscore'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
           axes[i, 2].set_title(f'Z-Score {col}')
           axes[i, 2].set_xlabel('Standardized Value')
           axes[i, 2].set_ylabel('Frequency')
           
           # å°æ•°å®šæ ‡æ ‡å‡†åŒ–
           axes[i, 3].hist(df[f'{col}_decimal'], bins=20, alpha=0.7, color='gold', edgecolor='black')
           axes[i, 3].set_title(f'Decimal Scaling {col}')
           axes[i, 3].set_xlabel('Decimal Scaled Value')
           axes[i, 3].set_ylabel('Frequency')
       
       plt.tight_layout()
       plt.savefig('normalization_comparison.png', dpi=300, bbox_inches='tight')
       print("å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° 'normalization_comparison.png'")
       
       # åˆ›å»ºç®±çº¿å›¾æ¯”è¾ƒ
       plt.figure(figsize=(15, 8))
       
       # ä¸ºæ¯ä¸ªæ•°å€¼åˆ—åˆ›å»ºå­å›¾
       for i, col in enumerate(numeric_columns):
           plt.subplot(2, 3, i+1)
           
           # å‡†å¤‡æ¯”è¾ƒæ•°æ®
           comparison_data = [
               df[col],
               df[f'{col}_minmax'],
               df[f'{col}_zscore'],
               df[f'{col}_decimal']
           ]
           
           plt.boxplot(comparison_data, labels=['Original', 'Min-Max', 'Z-Score', 'Decimal'])
           plt.title(f'{col} Distribution Comparison')
           plt.xticks(rotation=45)
       
       plt.tight_layout()
       plt.savefig('normalization_boxplots.png', dpi=300, bbox_inches='tight')
       print("ç®±çº¿å›¾æ¯”è¾ƒå·²ä¿å­˜åˆ° 'normalization_boxplots.png'")
   
   def evaluate_normalization_quality(df, numeric_columns):
       """è¯„ä¼°æ ‡å‡†åŒ–è´¨é‡"""
       print("\n" + "=" * 70)
       print("æ ‡å‡†åŒ–è´¨é‡è¯„ä¼°:")
       print("=" * 70)
       
       evaluation_data = []
       
       for col in numeric_columns:
           # åŸå§‹æ•°æ®çš„å˜å¼‚ç³»æ•°
           original_cv = df[col].std() / df[col].mean() if df[col].mean() != 0 else np.inf
           
           # è¯„ä¼°æœ€å°-æœ€å¤§å½’ä¸€åŒ–
           minmax_range = df[f'{col}_minmax'].max() - df[f'{col}_minmax'].min()
           minmax_cv = df[f'{col}_minmax'].std() / df[f'{col}_minmax'].mean() if df[f'{col}_minmax'].mean() != 0 else np.inf
           
           # è¯„ä¼°Z-scoreæ ‡å‡†åŒ–
           zscore_mean = abs(df[f'{col}_zscore'].mean())
           zscore_std = df[f'{col}_zscore'].std()
           
           # è¯„ä¼°å°æ•°å®šæ ‡æ ‡å‡†åŒ–
           decimal_cv = df[f'{col}_decimal'].std() / df[f'{col}_decimal'].mean() if df[f'{col}_decimal'].mean() != 0 else np.inf
           
           evaluation_data.append({
               'Column': col,
               'Original_CV': f"{original_cv:.4f}",
               'MinMax_Range': f"{minmax_range:.4f}",
               'MinMax_CV': f"{minmax_cv:.4f}",
               'ZScore_Mean_Abs': f"{zscore_mean:.4f}",
               'ZScore_Std': f"{zscore_std:.4f}",
               'Decimal_CV': f"{decimal_cv:.4f}"
           })
       
       evaluation_df = pd.DataFrame(evaluation_data)
       print(evaluation_df.to_string(index=False))
       
       # è´¨é‡æ€»ç»“
       print("\næ ‡å‡†åŒ–è´¨é‡æ€»ç»“:")
       print("- Min-Maxå½’ä¸€åŒ–: é€‚åˆéœ€è¦å›ºå®šèŒƒå›´[0,1]çš„åœºæ™¯")
       print("- Z-Scoreæ ‡å‡†åŒ–: é€‚åˆéœ€è¦æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„åœºæ™¯")
       print("- å°æ•°å®šæ ‡æ ‡å‡†åŒ–: é€‚åˆä¿æŒåŸå§‹æ•°æ®åˆ†å¸ƒå½¢çŠ¶çš„åœºæ™¯")
   
   def save_normalized_data(df):
       """ä¿å­˜æ ‡å‡†åŒ–åçš„æ•°æ®"""
       # ä¿å­˜å®Œæ•´çš„æ ‡å‡†åŒ–æ•°æ®
       df.to_csv('product_data_normalized.csv', index=False, encoding='utf-8-sig')
       print(f"\nå®Œæ•´æ ‡å‡†åŒ–æ•°æ®å·²ä¿å­˜åˆ° 'product_data_normalized.csv'")
       
       # ä¿å­˜ä»…åŒ…å«æ ‡å‡†åŒ–åˆ—çš„æ•°æ®
       normalized_columns = [col for col in df.columns if any(x in col for x in ['_minmax', '_zscore', '_decimal'])]
       normalized_df = df[['product_id'] + normalized_columns]
       normalized_df.to_csv('normalized_columns_only.csv', index=False, encoding='utf-8-sig')
       print(f"ä»…æ ‡å‡†åŒ–åˆ—æ•°æ®å·²ä¿å­˜åˆ° 'normalized_columns_only.csv'")
   
   def main():
       """ä¸»å‡½æ•°"""
       print("æ•°å€¼å‹æ•°æ®æ ‡å‡†åŒ–è„šæœ¬")
       print("=" * 80)
       
       # åˆ›å»ºæµ‹è¯•æ•°æ®
       df = create_test_data()
       
       # åˆ†æåŸå§‹æ•°æ®
       numeric_columns = analyze_original_data(df)
       
       # æ‰§è¡Œä¸‰ç§æ ‡å‡†åŒ–
       df, minmax_data = min_max_normalization(df, numeric_columns)
       df, zscore_data = z_score_standardization(df, numeric_columns)
       df, decimal_data = decimal_scaling_normalization(df, numeric_columns)
       
       # æ¯”è¾ƒä¸åŒæ–¹æ³•
       compare_normalization_methods(df, numeric_columns)
       
       # è¯„ä¼°æ ‡å‡†åŒ–è´¨é‡
       evaluate_normalization_quality(df, numeric_columns)
       
       # å¯è§†åŒ–ç»“æœ
       visualize_normalization(df, numeric_columns)
       
       # ä¿å­˜ç»“æœ
       save_normalized_data(df)
       
       print("\n" + "=" * 80)
       print("æ•°æ®æ ‡å‡†åŒ–å®Œæˆ!")
       print("=" * 80)
   
   if __name__ == "__main__":
       main()
   ```

## é«˜çº§é¢˜ç›®

1. ### å¤šæºæ•°æ®æ•´åˆ

   - ç¼–å†™è„šæœ¬æ•´åˆæ¥è‡ªCSVã€Excelå’ŒJSONä¸‰ä¸ªä¸åŒæ¥æºçš„æ•°æ®ï¼š
     - ç»Ÿä¸€åˆ—åå’Œæ•°æ®ç±»å‹
     - å¤„ç†æ—¶é—´çª—å£é‡å 
     - åˆå¹¶åå»é‡

   ```python
   import pandas as pd
   import numpy as np
   import json
   import os
   from datetime import datetime, timedelta
   import warnings
   warnings.filterwarnings('ignore')
   
   def create_test_data():
       """åˆ›å»ºCSVã€Excelå’ŒJSONä¸‰ä¸ªä¸åŒæ¥æºçš„æµ‹è¯•æ•°æ®"""
       
       # CSVæ•°æ® (ç¬¬ä¸€å­£åº¦)
       csv_data = {
           'order_id': ['Q1-1001', 'Q1-1002', 'Q1-1003', 'Q1-1004'],
           'customer_id': ['C001', 'C002', 'C003', 'C001'],
           'product_name': ['äº§å“A', 'äº§å“B', 'äº§å“C', 'äº§å“A'],
           'amount': [299.0, 899.0, 1599.0, 299.0],
           'order_date': ['2023-01-15', '2023-02-20', '2023-03-10', '2023-03-25'],
           'region': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'åŒ—äº¬']
       }
       csv_df = pd.DataFrame(csv_data)
       csv_df.to_csv('sales_q1.csv', index=False, encoding='utf-8-sig')
       
       # Excelæ•°æ® (ç¬¬äºŒå­£åº¦) - åˆ—åå’Œæ ¼å¼ä¸åŒ
       excel_data = {
           'è®¢å•ID': ['Q2-1001', 'Q2-1002', 'Q2-1003', 'Q2-1004'],
           'å®¢æˆ·ID': ['C003', 'C004', 'C001', 'C002'],
           'äº§å“åç§°': ['äº§å“C', 'äº§å“D', 'äº§å“A', 'äº§å“B'],
           'é‡‘é¢': [1599.0, 499.0, 299.0, 899.0],
           'ä¸‹å•æ—¥æœŸ': ['2023-04-10', '2023-05-15', '2023-06-20', '2023-06-25'],
           'åœ°åŒº': ['å¹¿å·', 'æ·±åœ³', 'åŒ—äº¬', 'ä¸Šæµ·']
       }
       excel_df = pd.DataFrame(excel_data)
       excel_df.to_excel('sales_q2.xlsx', index=False)
       
       # JSONæ•°æ® (ç¬¬ä¸‰å­£åº¦) - ç»“æ„å’Œæ ¼å¼ä¸åŒ
       json_data = [
           {
               "order_id": "Q3-1001",
               "customer_id": "C002",
               "product": "äº§å“B",
               "price": 899.0,
               "date": "2023-07-20",
               "location": "ä¸Šæµ·"
           },
           {
               "order_id": "Q3-1002",
               "customer_id": "C004",
               "product": "äº§å“D",
               "price": 499.0,
               "date": "2023-08-15",
               "location": "æ·±åœ³"
           },
           {
               "order_id": "Q3-1003",
               "customer_id": "C005",
               "product": "äº§å“E",
               "price": 1299.0,
               "date": "2023-09-10",
               "location": "æ­å·"
           },
           {
               "order_id": "Q3-1004",  # é‡å¤è®¢å•ï¼Œç”¨äºæµ‹è¯•å»é‡
               "customer_id": "C001",
               "product": "äº§å“A",
               "price": 299.0,
               "date": "2023-09-20",
               "location": "åŒ—äº¬"
           }
       ]
       
       with open('sales_q3.json', 'w', encoding='utf-8') as f:
           json.dump(json_data, f, ensure_ascii=False, indent=2)
       
       print("æµ‹è¯•æ•°æ®å·²åˆ›å»º:")
       print("  - sales_q1.csv (CSVæ ¼å¼)")
       print("  - sales_q2.xlsx (Excelæ ¼å¼)") 
       print("  - sales_q3.json (JSONæ ¼å¼)")
       
       return csv_df, excel_df, pd.DataFrame(json_data)
   
   def load_multi_source_data():
       """ä»CSVã€Excelã€JSONä¸‰ä¸ªæ¥æºåŠ è½½æ•°æ®"""
       print("=" * 70)
       print("åŠ è½½å¤šæºæ•°æ®:")
       print("=" * 70)
       
       data_sources = {}
       
       try:
           # åŠ è½½CSVæ•°æ®
           csv_df = pd.read_csv('sales_q1.csv')
           data_sources['csv'] = {
               'data': csv_df,
               'source': 'sales_q1.csv',
               'format': 'CSV'
           }
           print(f"CSVæ•°æ®åŠ è½½æˆåŠŸ: {len(csv_df)} æ¡è®°å½•")
           print(f"CSVåˆ—å: {list(csv_df.columns)}")
           
       except Exception as e:
           print(f"CSVæ•°æ®åŠ è½½å¤±è´¥: {e}")
           return None
       
       try:
           # åŠ è½½Excelæ•°æ®
           excel_df = pd.read_excel('sales_q2.xlsx')
           data_sources['excel'] = {
               'data': excel_df,
               'source': 'sales_q2.xlsx',
               'format': 'Excel'
           }
           print(f"Excelæ•°æ®åŠ è½½æˆåŠŸ: {len(excel_df)} æ¡è®°å½•")
           print(f"Excelåˆ—å: {list(excel_df.columns)}")
           
       except Exception as e:
           print(f"Excelæ•°æ®åŠ è½½å¤±è´¥: {e}")
           return None
       
       try:
           # åŠ è½½JSONæ•°æ®
           with open('sales_q3.json', 'r', encoding='utf-8') as f:
               json_data = json.load(f)
           json_df = pd.DataFrame(json_data)
           data_sources['json'] = {
               'data': json_df,
               'source': 'sales_q3.json',
               'format': 'JSON'
           }
           print(f"JSONæ•°æ®åŠ è½½æˆåŠŸ: {len(json_df)} æ¡è®°å½•")
           print(f"JSONåˆ—å: {list(json_df.columns)}")
           
       except Exception as e:
           print(f"JSONæ•°æ®åŠ è½½å¤±è´¥: {e}")
           return None
       
       return data_sources
   
   def analyze_source_data(data_sources):
       """åˆ†æå„æ•°æ®æºçš„ç»“æ„å’Œå†…å®¹"""
       print("\n" + "=" * 70)
       print("æ•°æ®æºåˆ†æ:")
       print("=" * 70)
       
       for source_name, source_info in data_sources.items():
           df = source_info['data']
           print(f"\n{source_info['format']}æ•°æ® ({source_info['source']}):")
           print(f"  è®°å½•æ•°: {len(df)}")
           print(f"  åˆ—æ•°: {len(df.columns)}")
           print(f"  æ•°æ®ç±»å‹:")
           for col, dtype in df.dtypes.items():
               print(f"    {col}: {dtype}")
           print(f"  å‰3è¡Œæ•°æ®:")
           print(df.head(3).to_string(index=False))
   
   def unify_column_names(data_sources):
       """ç»Ÿä¸€å„æ•°æ®æºçš„åˆ—å"""
       print("\n" + "=" * 70)
       print("ç»Ÿä¸€åˆ—å:")
       print("=" * 70)
       
       # å®šä¹‰æ ‡å‡†åˆ—åæ˜ å°„
       column_mapping = {
           # CSVåˆ—åæ˜ å°„ (å·²ç»æ˜¯æ ‡å‡†æ ¼å¼)
           'order_id': 'order_id',
           'customer_id': 'customer_id', 
           'product_name': 'product',
           'amount': 'amount',
           'order_date': 'order_date',
           'region': 'region',
           
           # Excelåˆ—åæ˜ å°„
           'è®¢å•ID': 'order_id',
           'å®¢æˆ·ID': 'customer_id',
           'äº§å“åç§°': 'product', 
           'é‡‘é¢': 'amount',
           'ä¸‹å•æ—¥æœŸ': 'order_date',
           'åœ°åŒº': 'region',
           
           # JSONåˆ—åæ˜ å°„
           'price': 'amount',
           'date': 'order_date',
           'location': 'region'
       }
       
       unified_data = {}
       
       for source_name, source_info in data_sources.items():
           df = source_info['data'].copy()
           original_columns = list(df.columns)
           
           # åº”ç”¨åˆ—åæ˜ å°„
           df.rename(columns=column_mapping, inplace=True)
           
           # ç¡®ä¿æ‰€æœ‰æ•°æ®æºéƒ½æœ‰ç›¸åŒçš„åˆ—
           expected_columns = ['order_id', 'customer_id', 'product', 'amount', 'order_date', 'region']
           for col in expected_columns:
               if col not in df.columns:
                   df[col] = None  # æ·»åŠ ç¼ºå¤±åˆ—
           
           unified_data[source_name] = {
               'data': df[expected_columns],  # æŒ‰æ ‡å‡†é¡ºåºæ’åˆ—åˆ—
               'source': source_info['source'],
               'format': source_info['format'],
               'original_columns': original_columns
           }
           
           print(f"\n{source_info['format']}æ•°æ®åˆ—åç»Ÿä¸€:")
           print(f"  åŸå§‹åˆ—å: {original_columns}")
           print(f"  ç»Ÿä¸€ååˆ—å: {list(df[expected_columns].columns)}")
       
       return unified_data
   
   def unify_data_types(unified_data):
       """ç»Ÿä¸€å„æ•°æ®æºçš„æ•°æ®ç±»å‹"""
       print("\n" + "=" * 70)
       print("ç»Ÿä¸€æ•°æ®ç±»å‹:")
       print("=" * 70)
       
       for source_name, source_info in unified_data.items():
           df = source_info['data'].copy()
           
           # è®°å½•åŸå§‹æ•°æ®ç±»å‹
           original_dtypes = df.dtypes.to_dict()
           
           # ç»Ÿä¸€æ•°æ®ç±»å‹
           df['order_id'] = df['order_id'].astype(str)
           df['customer_id'] = df['customer_id'].astype(str)
           df['product'] = df['product'].astype(str)
           df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
           df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')
           df['region'] = df['region'].astype(str)
           
           # æ›´æ–°æ•°æ®
           unified_data[source_name]['data'] = df
           
           print(f"\n{source_info['format']}æ•°æ®ç±»å‹ç»Ÿä¸€:")
           for col in df.columns:
               print(f"  {col}: {original_dtypes[col]} â†’ {df[col].dtype}")
       
       return unified_data
   
   def merge_data_sources(unified_data):
       """åˆå¹¶æ‰€æœ‰æ•°æ®æº"""
       print("\n" + "=" * 70)
       print("åˆå¹¶æ•°æ®æº:")
       print("=" * 70)
       
       # åˆå¹¶æ‰€æœ‰æ•°æ®
       all_data = []
       source_info_list = []
       
       for source_name, source_info in unified_data.items():
           df = source_info['data'].copy()
           df['data_source'] = source_info['format']
           df['original_source'] = source_info['source']
           all_data.append(df)
           
           print(f"æ·»åŠ  {source_info['format']} æ•°æ®: {len(df)} æ¡è®°å½•")
           source_info_list.append({
               'source': source_info['format'],
               'records': len(df)
           })
       
       # åˆå¹¶æ•°æ®æ¡†
       merged_df = pd.concat(all_data, ignore_index=True)
       
       print(f"\nåˆå¹¶åæ€»è®°å½•æ•°: {len(merged_df)}")
       print(f"åˆå¹¶ååˆ—å: {list(merged_df.columns)}")
       
       return merged_df, source_info_list
   
   def detect_time_overlaps(df, time_column='order_date', group_columns=['customer_id', 'product']):
       """æ£€æµ‹æ—¶é—´çª—å£é‡å """
       print("\n" + "=" * 70)
       print("æ£€æµ‹æ—¶é—´çª—å£é‡å :")
       print("=" * 70)
       
       # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
       df[time_column] = pd.to_datetime(df[time_column])
       
       # æŒ‰å®¢æˆ·å’Œäº§å“åˆ†ç»„ï¼Œæ£€æŸ¥æ—¶é—´é‡å 
       overlaps_detected = []
       
       for (customer, product), group in df.groupby(group_columns):
           if len(group) > 1:
               # æŒ‰æ—¶é—´æ’åº
               group = group.sort_values(time_column)
               
               # æ£€æŸ¥è¿ç»­è®¢å•ä¹‹é—´çš„æ—¶é—´é—´éš”
               time_diffs = group[time_column].diff().dt.days
               short_intervals = time_diffs[time_diffs < 30]  # 30å¤©å†…è§†ä¸ºå¯èƒ½é‡å 
               
               if len(short_intervals) > 0:
                   overlaps_detected.append({
                       'customer_id': customer,
                       'product': product,
                       'records': len(group),
                       'date_range': f"{group[time_column].min().strftime('%Y-%m-%d')} è‡³ {group[time_column].max().strftime('%Y-%m-%d')}",
                       'time_intervals': list(time_diffs[time_diffs.notna()])
                   })
       
       if overlaps_detected:
           print("å‘ç°æ—¶é—´çª—å£é‡å :")
           overlap_df = pd.DataFrame(overlaps_detected)
           print(overlap_df.to_string(index=False))
           
           # æ˜¾ç¤ºé‡å è¯¦æƒ…
           print("\né‡å è¯¦æƒ…:")
           for overlap in overlaps_detected:
               customer_data = df[(df['customer_id'] == overlap['customer_id']) & 
                                 (df['product'] == overlap['product'])]
               print(f"\nå®¢æˆ· {overlap['customer_id']} - äº§å“ {overlap['product']}:")
               print(customer_data[['order_id', 'order_date', 'amount', 'data_source']].to_string(index=False))
       else:
           print("æœªå‘ç°æ—¶é—´çª—å£é‡å ")
       
       return overlaps_detected
   
   def handle_duplicates(df):
       """å¤„ç†é‡å¤æ•°æ®"""
       print("\n" + "=" * 70)
       print("å¤„ç†é‡å¤æ•°æ®:")
       print("=" * 70)
       
       # æ£€æµ‹å®Œå…¨é‡å¤çš„è®°å½•
       duplicate_mask = df.duplicated(subset=['order_id', 'customer_id', 'product', 'order_date', 'amount'])
       complete_duplicates = df[duplicate_mask]
       
       if len(complete_duplicates) > 0:
           print(f"å‘ç° {len(complete_duplicates)} æ¡å®Œå…¨é‡å¤è®°å½•:")
           print(complete_duplicates[['order_id', 'customer_id', 'product', 'order_date', 'data_source']].to_string(index=False))
       else:
           print("æœªå‘ç°å®Œå…¨é‡å¤è®°å½•")
       
       # æ£€æµ‹åŸºäºè®¢å•IDçš„éƒ¨åˆ†é‡å¤ï¼ˆä¸åŒæ•°æ®æºå¯èƒ½æœ‰ç›¸åŒè®¢å•ï¼‰
       order_id_duplicates = df[df.duplicated(subset=['order_id'], keep=False)]
       
       if len(order_id_duplicates) > 0:
           print(f"\nå‘ç° {len(order_id_duplicates)} æ¡åŸºäºè®¢å•IDçš„é‡å¤è®°å½•:")
           
           # æŒ‰è®¢å•IDåˆ†ç»„æ˜¾ç¤ºé‡å¤
           for order_id, group in order_id_duplicates.groupby('order_id'):
               if len(group) > 1:
                   print(f"\nè®¢å• {order_id} çš„é‡å¤è®°å½•:")
                   print(group[['customer_id', 'product', 'order_date', 'amount', 'data_source']].to_string(index=False))
                   
                   # ä¿ç•™æœ€æ–°æ•°æ®æºçš„è®°å½•ï¼ˆå‡è®¾JSONæ˜¯æœ€æ–°çš„ï¼‰
                   source_priority = {'JSON': 1, 'Excel': 2, 'CSV': 3}
                   group['source_priority'] = group['data_source'].map(source_priority)
                   keep_record = group.loc[group['source_priority'].idxmin()]
                   
                   print(f"ä¿ç•™è®°å½•: {keep_record['data_source']} æ•°æ®æº")
       
       # ç§»é™¤å®Œå…¨é‡å¤çš„è®°å½•
       df_cleaned = df.drop_duplicates(subset=['order_id', 'customer_id', 'product', 'order_date', 'amount'])
       
       # å¯¹äºè®¢å•IDé‡å¤çš„è®°å½•ï¼Œä¿ç•™ä¼˜å…ˆçº§æœ€é«˜çš„æ•°æ®æº
       df_cleaned = df_cleaned.sort_values('data_source', key=lambda x: x.map({'JSON': 1, 'Excel': 2, 'CSV': 3}))
       df_cleaned = df_cleaned.drop_duplicates(subset=['order_id'], keep='first')
       
       print(f"\nå»é‡å‰è®°å½•æ•°: {len(df)}")
       print(f"å»é‡åè®°å½•æ•°: {len(df_cleaned)}")
       print(f"ç§»é™¤é‡å¤è®°å½•: {len(df) - len(df_cleaned)}")
       
       return df_cleaned
   
   def analyze_merged_data(df):
       """åˆ†æåˆå¹¶åçš„æ•°æ®"""
       print("\n" + "=" * 70)
       print("åˆå¹¶æ•°æ®ç»Ÿè®¡åˆ†æ:")
       print("=" * 70)
       
       # åŸºæœ¬ç»Ÿè®¡
       print(f"æ€»è®°å½•æ•°: {len(df)}")
       print(f"å”¯ä¸€å®¢æˆ·æ•°: {df['customer_id'].nunique()}")
       print(f"äº§å“ç§ç±»: {df['product'].nunique()}")
       print(f"åœ°åŒºæ•°é‡: {df['region'].nunique()}")
       
       # æ•°æ®æºåˆ†å¸ƒ
       print(f"\næ•°æ®æºåˆ†å¸ƒ:")
       source_dist = df['data_source'].value_counts()
       for source, count in source_dist.items():
           print(f"  {source}: {count} æ¡è®°å½• ({count/len(df)*100:.1f}%)")
       
       # æ—¶é—´èŒƒå›´
       print(f"\næ—¶é—´èŒƒå›´:")
       print(f"  æœ€æ—©è®¢å•: {df['order_date'].min().strftime('%Y-%m-%d')}")
       print(f"  æœ€æ™šè®¢å•: {df['order_date'].max().strftime('%Y-%m-%d')}")
       print(f"  æ—¶é—´è·¨åº¦: {(df['order_date'].max() - df['order_date'].min()).days} å¤©")
       
       # é‡‘é¢ç»Ÿè®¡
       print(f"\né‡‘é¢ç»Ÿè®¡:")
       print(f"  æ€»é‡‘é¢: {df['amount'].sum():.2f}")
       print(f"  å¹³å‡é‡‘é¢: {df['amount'].mean():.2f}")
       print(f"  æœ€å°é‡‘é¢: {df['amount'].min():.2f}")
       print(f"  æœ€å¤§é‡‘é¢: {df['amount'].max():.2f}")
       
       # çƒ­é—¨äº§å“
       print(f"\nçƒ­é—¨äº§å“ (æŒ‰é”€å”®æ¬¡æ•°):")
       product_counts = df['product'].value_counts()
       for product, count in product_counts.items():
           product_amount = df[df['product'] == product]['amount'].sum()
           print(f"  {product}: {count} æ¬¡, æ€»é‡‘é¢ {product_amount:.2f}")
       
       # åœ°åŒºåˆ†å¸ƒ
       print(f"\nåœ°åŒºåˆ†å¸ƒ:")
       region_counts = df['region'].value_counts()
       for region, count in region_counts.items():
           print(f"  {region}: {count} æ¡è®°å½•")
   
   def save_integrated_data(df, output_file='integrated_sales_data.csv'):
       """ä¿å­˜æ•´åˆåçš„æ•°æ®"""
       print("\n" + "=" * 70)
       print("ä¿å­˜æ•´åˆæ•°æ®:")
       print("=" * 70)
       
       # ä¿å­˜ä¸ºCSV
       df.to_csv(output_file, index=False, encoding='utf-8-sig')
       print(f"æ•´åˆæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
       
       # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
       report_file = 'integration_report.txt'
       with open(report_file, 'w', encoding='utf-8') as f:
           f.write("å¤šæºæ•°æ®æ•´åˆæŠ¥å‘Š\n")
           f.write("=" * 50 + "\n")
           f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
           f.write(f"æ€»è®°å½•æ•°: {len(df)}\n")
           f.write(f"æ•°æ®æº: CSV, Excel, JSON\n")
           f.write(f"è¾“å‡ºæ–‡ä»¶: {output_file}\n\n")
           
           f.write("æ•°æ®æºç»Ÿè®¡:\n")
           for source, count in df['data_source'].value_counts().items():
               f.write(f"  {source}: {count} æ¡è®°å½•\n")
       
       print(f"æ•´åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
   
   def main():
       """ä¸»å‡½æ•°"""
       print("å¤šæºæ•°æ®æ•´åˆè„šæœ¬")
       print("=" * 80)
       
       # åˆ›å»ºæµ‹è¯•æ•°æ®
       print("åˆ›å»ºæµ‹è¯•æ•°æ®...")
       create_test_data()
       
       # åŠ è½½å¤šæºæ•°æ®
       data_sources = load_multi_source_data()
       if not data_sources:
           print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼")
           return
       
       # åˆ†ææºæ•°æ®
       analyze_source_data(data_sources)
       
       # ç»Ÿä¸€åˆ—å
       unified_data = unify_column_names(data_sources)
       
       # ç»Ÿä¸€æ•°æ®ç±»å‹
       unified_data = unify_data_types(unified_data)
       
       # åˆå¹¶æ•°æ®
       merged_df, source_info = merge_data_sources(unified_data)
       
       # æ£€æµ‹æ—¶é—´é‡å 
       overlaps = detect_time_overlaps(merged_df)
       
       # å¤„ç†é‡å¤æ•°æ®
       final_df = handle_duplicates(merged_df)
       
       # åˆ†ææ•´åˆåçš„æ•°æ®
       analyze_merged_data(final_df)
       
       # ä¿å­˜ç»“æœ
       save_integrated_data(final_df)
       
       print("\n" + "=" * 80)
       print("å¤šæºæ•°æ®æ•´åˆå®Œæˆ!")
       print("=" * 80)
   
   if __name__ == "__main__":
       main()
   ```

2. ### å¤æ‚è§„åˆ™æ¸…æ´—

   - ç¼–å†™è„šæœ¬å®ç°ä¸šåŠ¡è§„åˆ™éªŒè¯ï¼š
     - éªŒè¯èº«ä»½è¯å·æ ¼å¼å’Œæœ‰æ•ˆæ€§
     - æ£€æŸ¥æ•°æ®é€»è¾‘å…³ç³»ï¼ˆå¦‚ç»“æŸæ—¥æœŸ>å¼€å§‹æ—¥æœŸï¼‰
     - éªŒè¯æ•°æ®å®Œæ•´æ€§çº¦æŸ

   ```python
   import pandas as pd
   import numpy as np
   import re
   from datetime import datetime, timedelta
   import warnings
   warnings.filterwarnings('ignore')
   
   def create_test_data():
       """åˆ›å»ºåŒ…å«å„ç§ä¸šåŠ¡è§„åˆ™é—®é¢˜çš„æµ‹è¯•æ•°æ®"""
       
       data = {
           'user_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
           'name': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ', 'å­™å…«', 'å‘¨ä¹', 'å´å', 'éƒ‘åä¸€', None],
           'id_card': [
               '110101199001011234',  # æœ‰æ•ˆ
               '11010119900101123X',  # æœ‰æ•ˆï¼Œæœ€åä¸€ä½X
               '123456789012345678',  # æ ¼å¼æ­£ç¡®ä½†æ ¡éªŒç é”™è¯¯
               '110101900101123',     # ä½æ•°ä¸è¶³
               '110101199013011234',  # æœˆä»½é”™è¯¯
               '110101199001321234',  # æ—¥æœŸé”™è¯¯
               '110101189001011234',  # å¹´ä»½å¤ªæ—©
               '110101210001011234',  # å¹´ä»½æœªæ¥
               'ABCDEFGHIJKLMNOPQR',  # åŒ…å«å­—æ¯
               '110101199001011235'   # æ ¡éªŒç é”™è¯¯
           ],
           'start_date': [
               '2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01', '2023-05-01',
               '2023-06-01', '2023-07-01', '2023-08-01', '2023-09-01', '2023-10-01'
           ],
           'end_date': [
               '2023-12-31', '2023-01-15',  # ç»“æŸæ—©äºå¼€å§‹
               '2023-06-30', '2023-03-15',  # ç»“æŸæ—©äºå¼€å§‹
               '2024-05-31', '2023-05-30',  # ç»“æŸæ—©äºå¼€å§‹
               '2024-07-31', '2023-07-15',  # ç»“æŸæ—©äºå¼€å§‹
               '2024-09-30', '2023-08-31'   # ç»“æŸæ—©äºå¼€å§‹
           ],
           'age': [25, 30, -5, 150, 35, 28, 42, 29, 31, 26],  # åŒ…å«å¼‚å¸¸å¹´é¾„
           'salary': [5000, 8000, 12000, -1000, 7500, 6000, 9500, 11000, 6800, 7200],  # åŒ…å«è´Ÿå·¥èµ„
           'department': ['æŠ€æœ¯éƒ¨', 'å¸‚åœºéƒ¨', 'è´¢åŠ¡éƒ¨', None, 'äººåŠ›èµ„æºéƒ¨', 'æŠ€æœ¯éƒ¨', 'å¸‚åœºéƒ¨', 'è´¢åŠ¡éƒ¨', 'è¿è¥éƒ¨', 'æŠ€æœ¯éƒ¨'],
           'manager_id': [None, 1, 2, 3, 4, 1, 2, 3, 4, 1],  # åŒ…å«å¾ªç¯å¼•ç”¨
           'email': [
               'zhangsan@company.com',
               'lisi@company.com',
               'invalid-email',
               'zhaoliu@company.com',
               'qianqi@',
               'sunba@company.com',
               'zhoujiu@company.com',
               'wushi@company.com',
               'zhengshiyi@company.com',
               'missing@company.com'
           ],
           'phone': [
               '13812345678',
               '13512345678',
               '12345678901',  # æ— æ•ˆå·æ®µ
               '13612345678',
               '1391234567',   # ä½æ•°ä¸è¶³
               '13712345678',
               '15012345678',
               '15112345678',
               '15212345678',
               '15312345678'
           ]
       }
       
       df = pd.DataFrame(data)
       df.to_csv('business_data.csv', index=False, encoding='utf-8-sig')
       print("æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ° 'business_data.csv'")
       return df
   
   def analyze_initial_data(df):
       """åˆ†æåˆå§‹æ•°æ®çŠ¶æ€"""
       print("=" * 80)
       print("åˆå§‹æ•°æ®åˆ†æ:")
       print("=" * 80)
       
       print(f"æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
       print("\næ•°æ®ç±»å‹:")
       print(df.dtypes)
       
       print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
       missing_data = df.isnull().sum()
       for col, count in missing_data.items():
           if count > 0:
               print(f"  {col}: {count} ä¸ªç¼ºå¤±å€¼")
       
       print("\næ•°æ®ç¤ºä¾‹:")
       print(df.head())
   
   def validate_id_card(id_card):
       """
       éªŒè¯èº«ä»½è¯å·æ ¼å¼å’Œæœ‰æ•ˆæ€§
       æ”¯æŒ18ä½èº«ä»½è¯å·ç éªŒè¯
       """
       if pd.isna(id_card):
           return False, "èº«ä»½è¯å·ä¸ºç©º"
       
       id_card = str(id_card).strip()
       
       # åŸºæœ¬æ ¼å¼éªŒè¯
       if len(id_card) != 18:
           return False, f"èº«ä»½è¯å·é•¿åº¦åº”ä¸º18ä½ï¼Œå½“å‰ä¸º{len(id_card)}ä½"
       
       if not re.match(r'^\d{17}[\dXx]$', id_card):
           return False, "èº«ä»½è¯å·æ ¼å¼ä¸æ­£ç¡®ï¼Œå‰17ä½åº”ä¸ºæ•°å­—ï¼Œæœ€åä¸€ä½ä¸ºæ•°å­—æˆ–X"
       
       # åœ°åŒºç éªŒè¯ï¼ˆç®€å•éªŒè¯å‰6ä½ï¼‰
       area_codes = ['11', '12', '13', '14', '15', '21', '22', '23', '31', '32', '33', 
                     '34', '35', '36', '37', '41', '42', '43', '44', '45', '46', '50', 
                     '51', '52', '53', '54', '61', '62', '63', '64', '65']
       if id_card[:2] not in area_codes:
           return False, "èº«ä»½è¯åœ°åŒºç æ— æ•ˆ"
       
       # å‡ºç”Ÿæ—¥æœŸéªŒè¯
       try:
           year = int(id_card[6:10])
           month = int(id_card[10:12])
           day = int(id_card[12:14])
           
           # æ£€æŸ¥å¹´ä»½èŒƒå›´ï¼ˆ1900-å½“å‰å¹´ä»½ï¼‰
           current_year = datetime.now().year
           if year < 1900 or year > current_year:
               return False, f"å‡ºç”Ÿå¹´ä»½{year}è¶…å‡ºæœ‰æ•ˆèŒƒå›´"
           
           # æ£€æŸ¥æœˆä»½
           if month < 1 or month > 12:
               return False, f"å‡ºç”Ÿæœˆä»½{month}æ— æ•ˆ"
           
           # æ£€æŸ¥æ—¥æœŸ
           if day < 1 or day > 31:
               return False, f"å‡ºç”Ÿæ—¥æœŸ{day}æ— æ•ˆ"
           
           # éªŒè¯å…·ä½“æ—¥æœŸæœ‰æ•ˆæ€§
           try:
               birth_date = datetime(year, month, day)
               if birth_date > datetime.now():
                   return False, "å‡ºç”Ÿæ—¥æœŸä¸èƒ½æ™šäºå½“å‰æ—¥æœŸ"
           except ValueError:
               return False, "å‡ºç”Ÿæ—¥æœŸæ— æ•ˆï¼ˆå¦‚2æœˆ30æ—¥ï¼‰"
           
       except ValueError:
           return False, "å‡ºç”Ÿæ—¥æœŸéƒ¨åˆ†åŒ…å«æ— æ•ˆæ•°å­—"
       
       # æ ¡éªŒç éªŒè¯
       factors = [7, 9, 10, 5, 8, 4, 2, 1, 6, 3, 7, 9, 10, 5, 8, 4, 2]
       check_codes = ['1', '0', 'X', '9', '8', '7', '6', '5', '4', '3', '2']
       
       total = 0
       for i in range(17):
           total += int(id_card[i]) * factors[i]
       
       check_index = total % 11
       expected_check_code = check_codes[check_index]
       
       if id_card[17].upper() != expected_check_code:
           return False, f"æ ¡éªŒç é”™è¯¯ï¼Œåº”ä¸º{expected_check_code}"
       
       return True, "èº«ä»½è¯å·æœ‰æ•ˆ"
   
   def validate_id_cards(df):
       """æ‰¹é‡éªŒè¯èº«ä»½è¯å·"""
       print("\n" + "=" * 80)
       print("èº«ä»½è¯å·éªŒè¯:")
       print("=" * 80)
       
       validation_results = []
       
       for idx, row in df.iterrows():
           is_valid, message = validate_id_card(row['id_card'])
           validation_results.append({
               'user_id': row['user_id'],
               'id_card': row['id_card'],
               'is_valid': is_valid,
               'message': message
           })
           
           if not is_valid:
               print(f"ç”¨æˆ· {row['user_id']}: {message}")
       
       # ç»Ÿè®¡éªŒè¯ç»“æœ
       valid_count = sum(1 for result in validation_results if result['is_valid'])
       invalid_count = len(validation_results) - valid_count
       
       print(f"\nèº«ä»½è¯éªŒè¯ç»Ÿè®¡:")
       print(f"  æœ‰æ•ˆ: {valid_count}")
       print(f"  æ— æ•ˆ: {invalid_count}")
       print(f"  æœ‰æ•ˆç‡: {valid_count/len(validation_results)*100:.1f}%")
       
       return validation_results
   
   def check_logical_relationships(df):
       """æ£€æŸ¥æ•°æ®é€»è¾‘å…³ç³»"""
       print("\n" + "=" * 80)
       print("æ•°æ®é€»è¾‘å…³ç³»æ£€æŸ¥:")
       print("=" * 80)
       
       logical_issues = []
       
       # 1. æ£€æŸ¥æ—¥æœŸé€»è¾‘ï¼šç»“æŸæ—¥æœŸ > å¼€å§‹æ—¥æœŸ
       df['start_date'] = pd.to_datetime(df['start_date'])
       df['end_date'] = pd.to_datetime(df['end_date'])
       
       date_issues = df[df['end_date'] <= df['start_date']]
       if not date_issues.empty:
           print("å‘ç°æ—¥æœŸé€»è¾‘é—®é¢˜ (ç»“æŸæ—¥æœŸ <= å¼€å§‹æ—¥æœŸ):")
           for idx, row in date_issues.iterrows():
               issue = {
                   'user_id': row['user_id'],
                   'issue_type': 'æ—¥æœŸé€»è¾‘',
                   'description': f"ç»“æŸæ—¥æœŸ({row['end_date'].strftime('%Y-%m-%d')}) <= å¼€å§‹æ—¥æœŸ({row['start_date'].strftime('%Y-%m-%d')})",
                   'severity': 'é«˜'
               }
               logical_issues.append(issue)
               print(f"  ç”¨æˆ· {row['user_id']}: {issue['description']}")
       
       # 2. æ£€æŸ¥å¹´é¾„åˆç†æ€§ (0-150)
       age_issues = df[(df['age'] < 0) | (df['age'] > 150)]
       if not age_issues.empty:
           print("\nå‘ç°å¹´é¾„é€»è¾‘é—®é¢˜ (å¹´é¾„ < 0 æˆ– > 150):")
           for idx, row in age_issues.iterrows():
               issue = {
                   'user_id': row['user_id'],
                   'issue_type': 'å¹´é¾„é€»è¾‘',
                   'description': f"å¹´é¾„å€¼å¼‚å¸¸: {row['age']}",
                   'severity': 'é«˜'
               }
               logical_issues.append(issue)
               print(f"  ç”¨æˆ· {row['user_id']}: {issue['description']}")
       
       # 3. æ£€æŸ¥å·¥èµ„åˆç†æ€§ (>= 0)
       salary_issues = df[df['salary'] < 0]
       if not salary_issues.empty:
           print("\nå‘ç°å·¥èµ„é€»è¾‘é—®é¢˜ (å·¥èµ„ < 0):")
           for idx, row in salary_issues.iterrows():
               issue = {
                   'user_id': row['user_id'],
                   'issue_type': 'å·¥èµ„é€»è¾‘',
                   'description': f"å·¥èµ„å€¼å¼‚å¸¸: {row['salary']}",
                   'severity': 'é«˜'
               }
               logical_issues.append(issue)
               print(f"  ç”¨æˆ· {row['user_id']}: {issue['description']}")
       
       # 4. æ£€æŸ¥é‚®ç®±æ ¼å¼
       email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
       email_issues = df[~df['email'].str.match(email_pattern, na=False)]
       if not email_issues.empty:
           print("\nå‘ç°é‚®ç®±æ ¼å¼é—®é¢˜:")
           for idx, row in email_issues.iterrows():
               issue = {
                   'user_id': row['user_id'],
                   'issue_type': 'é‚®ç®±æ ¼å¼',
                   'description': f"é‚®ç®±æ ¼å¼æ— æ•ˆ: {row['email']}",
                   'severity': 'ä¸­'
               }
               logical_issues.append(issue)
               print(f"  ç”¨æˆ· {row['user_id']}: {issue['description']}")
       
       # 5. æ£€æŸ¥æ‰‹æœºå·æ ¼å¼
       phone_pattern = r'^1[3-9]\d{9}$'
       phone_issues = df[~df['phone'].str.match(phone_pattern, na=False)]
       if not phone_issues.empty:
           print("\nå‘ç°æ‰‹æœºå·æ ¼å¼é—®é¢˜:")
           for idx, row in phone_issues.iterrows():
               issue = {
                   'user_id': row['user_id'],
                   'issue_type': 'æ‰‹æœºå·æ ¼å¼',
                   'description': f"æ‰‹æœºå·æ ¼å¼æ— æ•ˆ: {row['phone']}",
                   'severity': 'ä¸­'
               }
               logical_issues.append(issue)
               print(f"  ç”¨æˆ· {row['user_id']}: {issue['description']}")
       
       # ç»Ÿè®¡é€»è¾‘é—®é¢˜
       if logical_issues:
           issue_df = pd.DataFrame(logical_issues)
           severity_counts = issue_df['severity'].value_counts()
           print(f"\né€»è¾‘é—®é¢˜ä¸¥é‡æ€§ç»Ÿè®¡:")
           for severity, count in severity_counts.items():
               print(f"  {severity}: {count} ä¸ªé—®é¢˜")
       else:
           print("æœªå‘ç°é€»è¾‘å…³ç³»é—®é¢˜")
       
       return logical_issues
   
   def validate_integrity_constraints(df):
       """éªŒè¯æ•°æ®å®Œæ•´æ€§çº¦æŸ"""
       print("\n" + "=" * 80)
       print("æ•°æ®å®Œæ•´æ€§çº¦æŸéªŒè¯:")
       print("=" * 80)
       
       integrity_issues = []
       
       # 1. éç©ºçº¦æŸæ£€æŸ¥
       required_columns = ['user_id', 'name', 'id_card', 'department']
       for col in required_columns:
           null_count = df[col].isnull().sum()
           if null_count > 0:
               print(f"éç©ºçº¦æŸè¿å - {col}: {null_count} ä¸ªç©ºå€¼")
               null_rows = df[df[col].isnull()]
               for idx, row in null_rows.iterrows():
                   issue = {
                       'user_id': row['user_id'],
                       'issue_type': 'éç©ºçº¦æŸ',
                       'description': f"å­—æ®µ '{col}' ä¸èƒ½ä¸ºç©º",
                       'severity': 'é«˜'
                   }
                   integrity_issues.append(issue)
       
       # 2. å”¯ä¸€æ€§çº¦æŸæ£€æŸ¥
       # ç”¨æˆ·IDå”¯ä¸€æ€§
       duplicate_users = df[df.duplicated(subset=['user_id'], keep=False)]
       if not duplicate_users.empty:
           print(f"å”¯ä¸€æ€§çº¦æŸè¿å - user_id: {len(duplicate_users)} ä¸ªé‡å¤å€¼")
           for user_id, group in duplicate_users.groupby('user_id'):
               issue = {
                   'user_id': user_id,
                   'issue_type': 'å”¯ä¸€æ€§çº¦æŸ',
                   'description': f"ç”¨æˆ·ID '{user_id}' é‡å¤å‡ºç°",
                   'severity': 'é«˜'
               }
               integrity_issues.append(issue)
       
       # èº«ä»½è¯å·å”¯ä¸€æ€§
       duplicate_id_cards = df[df.duplicated(subset=['id_card'], keep=False)]
       if not duplicate_id_cards.empty:
           print(f"å”¯ä¸€æ€§çº¦æŸè¿å - id_card: {len(duplicate_id_cards)} ä¸ªé‡å¤å€¼")
           for id_card, group in duplicate_id_cards.groupby('id_card'):
               issue = {
                   'user_id': group.iloc[0]['user_id'],
                   'issue_type': 'å”¯ä¸€æ€§çº¦æŸ',
                   'description': f"èº«ä»½è¯å· '{id_card}' é‡å¤å‡ºç°",
                   'severity': 'é«˜'
               }
               integrity_issues.append(issue)
       
       # 3. å¤–é”®çº¦æŸæ£€æŸ¥ (manager_id å¿…é¡»å­˜åœ¨äº user_id ä¸­)
       valid_manager_ids = set(df['user_id'].dropna())
       invalid_managers = df[~df['manager_id'].isin(valid_manager_ids) & df['manager_id'].notna()]
       if not invalid_managers.empty:
           print(f"å¤–é”®çº¦æŸè¿å - manager_id: {len(invalid_managers)} ä¸ªæ— æ•ˆç»ç†ID")
           for idx, row in invalid_managers.iterrows():
               issue = {
                   'user_id': row['user_id'],
                   'issue_type': 'å¤–é”®çº¦æŸ',
                   'description': f"ç»ç†ID '{row['manager_id']}' ä¸å­˜åœ¨",
                   'severity': 'ä¸­'
               }
               integrity_issues.append(issue)
       
       # 4. æ£€æŸ¥å¾ªç¯å¼•ç”¨ (manager_id ä¸èƒ½æŒ‡å‘è‡ªå·±)
       self_reference = df[df['user_id'] == df['manager_id']]
       if not self_reference.empty:
           print(f"å¾ªç¯å¼•ç”¨é—®é¢˜: {len(self_reference)} ä¸ªè‡ªå¼•ç”¨")
           for idx, row in self_reference.iterrows():
               issue = {
                   'user_id': row['user_id'],
                   'issue_type': 'å¾ªç¯å¼•ç”¨',
                   'description': f"ç»ç†IDæŒ‡å‘è‡ªå·±",
                   'severity': 'ä¸­'
               }
               integrity_issues.append(issue)
       
       # 5. ä¸šåŠ¡è§„åˆ™ï¼šéƒ¨é—¨ä¸€è‡´æ€§æ£€æŸ¥
       # å‡è®¾åŒä¸€éƒ¨é—¨çš„å‘˜å·¥åº”è¯¥æœ‰ç›¸ä¼¼çš„å·¥èµ„èŒƒå›´
       dept_salary_stats = df.groupby('department')['salary'].agg(['mean', 'std']).fillna(0)
       for dept, stats in dept_salary_stats.iterrows():
           if stats['std'] > 0:  # æœ‰æ ‡å‡†å·®æ‰æ£€æŸ¥
               lower_bound = stats['mean'] - 2 * stats['std']
               upper_bound = stats['mean'] + 2 * stats['std']
               outliers = df[(df['department'] == dept) & 
                            ((df['salary'] < lower_bound) | (df['salary'] > upper_bound))]
               
               if not outliers.empty:
                   print(f"ä¸šåŠ¡è§„åˆ™è¿å - {dept}éƒ¨é—¨å·¥èµ„å¼‚å¸¸: {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
                   for idx, row in outliers.iterrows():
                       issue = {
                           'user_id': row['user_id'],
                           'issue_type': 'ä¸šåŠ¡è§„åˆ™',
                           'description': f"{dept}éƒ¨é—¨å·¥èµ„å¼‚å¸¸: {row['salary']} (èŒƒå›´: {lower_bound:.0f}-{upper_bound:.0f})",
                           'severity': 'ä½'
                       }
                       integrity_issues.append(issue)
       
       # ç»Ÿè®¡å®Œæ•´æ€§é—®é¢˜
       if integrity_issues:
           issue_df = pd.DataFrame(integrity_issues)
           issue_type_counts = issue_df['issue_type'].value_counts()
           print(f"\nå®Œæ•´æ€§çº¦æŸé—®é¢˜ç»Ÿè®¡:")
           for issue_type, count in issue_type_counts.items():
               print(f"  {issue_type}: {count} ä¸ªé—®é¢˜")
       else:
           print("æœªå‘ç°å®Œæ•´æ€§çº¦æŸé—®é¢˜")
       
       return integrity_issues
   
   def generate_cleaning_suggestions(df, id_validation_results, logical_issues, integrity_issues):
       """ç”Ÿæˆæ•°æ®æ¸…æ´—å»ºè®®"""
       print("\n" + "=" * 80)
       print("æ•°æ®æ¸…æ´—å»ºè®®:")
       print("=" * 80)
       
       cleaning_plan = []
       
       # èº«ä»½è¯å·æ¸…æ´—å»ºè®®
       invalid_id_cards = [result for result in id_validation_results if not result['is_valid']]
       if invalid_id_cards:
           print("\nèº«ä»½è¯å·æ¸…æ´—å»ºè®®:")
           for result in invalid_id_cards:
               suggestion = {
                   'user_id': result['user_id'],
                   'field': 'id_card',
                   'issue': result['message'],
                   'suggestion': 'éœ€è¦äººå·¥æ ¸å®å¹¶ä¿®æ­£èº«ä»½è¯å·',
                   'priority': 'é«˜'
               }
               cleaning_plan.append(suggestion)
               print(f"  ç”¨æˆ· {result['user_id']}: {result['message']} â†’ éœ€è¦äººå·¥æ ¸å®")
       
       # é€»è¾‘é—®é¢˜æ¸…æ´—å»ºè®®
       if logical_issues:
           print("\né€»è¾‘é—®é¢˜æ¸…æ´—å»ºè®®:")
           for issue in logical_issues:
               if issue['issue_type'] == 'æ—¥æœŸé€»è¾‘':
                   suggestion = {
                       'user_id': issue['user_id'],
                       'field': 'start_date/end_date',
                       'issue': issue['description'],
                       'suggestion': 'æ£€æŸ¥å¹¶ä¿®æ­£æ—¥æœŸé¡ºåºï¼Œç¡®ä¿ç»“æŸæ—¥æœŸæ™šäºå¼€å§‹æ—¥æœŸ',
                       'priority': issue['severity']
                   }
               elif issue['issue_type'] == 'å¹´é¾„é€»è¾‘':
                   suggestion = {
                       'user_id': issue['user_id'],
                       'field': 'age',
                       'issue': issue['description'],
                       'suggestion': 'å°†å¹´é¾„ä¿®æ­£ä¸ºåˆç†èŒƒå›´ (0-150)',
                       'priority': issue['severity']
                   }
               elif issue['issue_type'] == 'å·¥èµ„é€»è¾‘':
                   suggestion = {
                       'user_id': issue['user_id'],
                       'field': 'salary',
                       'issue': issue['description'],
                       'suggestion': 'å°†å·¥èµ„ä¿®æ­£ä¸ºéè´Ÿå€¼',
                       'priority': issue['severity']
                   }
               else:
                   suggestion = {
                       'user_id': issue['user_id'],
                       'field': issue['issue_type'].replace('æ ¼å¼', ''),
                       'issue': issue['description'],
                       'suggestion': 'ä¿®æ­£æ ¼å¼ä»¥ç¬¦åˆè§„èŒƒ',
                       'priority': issue['severity']
                   }
               
               cleaning_plan.append(suggestion)
               print(f"  ç”¨æˆ· {issue['user_id']}: {issue['description']} â†’ {suggestion['suggestion']}")
       
       # å®Œæ•´æ€§çº¦æŸæ¸…æ´—å»ºè®®
       if integrity_issues:
           print("\nå®Œæ•´æ€§çº¦æŸæ¸…æ´—å»ºè®®:")
           for issue in integrity_issues:
               if issue['issue_type'] == 'éç©ºçº¦æŸ':
                   suggestion = {
                       'user_id': issue['user_id'],
                       'field': 'ç›¸å…³å­—æ®µ',
                       'issue': issue['description'],
                       'suggestion': 'è¡¥å……ç¼ºå¤±çš„å¿…è¦å­—æ®µæ•°æ®',
                       'priority': issue['severity']
                   }
               elif issue['issue_type'] == 'å”¯ä¸€æ€§çº¦æŸ':
                   suggestion = {
                       'user_id': issue['user_id'],
                       'field': 'ç›¸å…³å­—æ®µ',
                       'issue': issue['description'],
                       'suggestion': 'ç§»é™¤é‡å¤è®°å½•æˆ–ä¿®æ­£é‡å¤å€¼',
                       'priority': issue['severity']
                   }
               elif issue['issue_type'] == 'å¤–é”®çº¦æŸ':
                   suggestion = {
                       'user_id': issue['user_id'],
                       'field': 'manager_id',
                       'issue': issue['description'],
                       'suggestion': 'ä¿®æ­£ä¸ºæœ‰æ•ˆçš„ç»ç†IDæˆ–è®¾ç½®ä¸ºç©º',
                       'priority': issue['severity']
                   }
               else:
                   suggestion = {
                       'user_id': issue['user_id'],
                       'field': 'ç›¸å…³å­—æ®µ',
                       'issue': issue['description'],
                       'suggestion': 'æ ¹æ®ä¸šåŠ¡è§„åˆ™è¿›è¡Œä¿®æ­£',
                       'priority': issue['severity']
                   }
               
               cleaning_plan.append(suggestion)
               print(f"  ç”¨æˆ· {issue['user_id']}: {issue['description']} â†’ {suggestion['suggestion']}")
       
       return cleaning_plan
   
   def apply_data_cleaning(df, cleaning_plan):
       """åº”ç”¨æ•°æ®æ¸…æ´—"""
       print("\n" + "=" * 80)
       print("åº”ç”¨æ•°æ®æ¸…æ´—:")
       print("=" * 80)
       
       df_cleaned = df.copy()
       applied_fixes = []
       
       # æŒ‰ä¼˜å…ˆçº§æ’åºæ¸…æ´—å»ºè®®ï¼ˆé«˜ä¼˜å…ˆçº§å…ˆå¤„ç†ï¼‰
       high_priority = [item for item in cleaning_plan if item['priority'] == 'é«˜']
       medium_priority = [item for item in cleaning_plan if item['priority'] == 'ä¸­']
       low_priority = [item for item in cleaning_plan if item['priority'] == 'ä½']
       
       sorted_plan = high_priority + medium_priority + low_priority
       
       for suggestion in sorted_plan:
           user_id = suggestion['user_id']
           field = suggestion['field']
           
           # æ ¹æ®é—®é¢˜ç±»å‹åº”ç”¨ä¸åŒçš„æ¸…æ´—ç­–ç•¥
           if 'èº«ä»½è¯' in suggestion['issue']:
               # å¯¹äºèº«ä»½è¯é—®é¢˜ï¼Œæ ‡è®°ä¸ºéœ€è¦äººå·¥æ£€æŸ¥
               df_cleaned.loc[df_cleaned['user_id'] == user_id, 'id_card_status'] = 'éœ€äººå·¥æ ¸æŸ¥'
               applied_fixes.append(f"ç”¨æˆ· {user_id}: æ ‡è®°èº«ä»½è¯éœ€äººå·¥æ ¸æŸ¥")
           
           elif 'å¹´é¾„' in suggestion['issue']:
               # ä¿®æ­£å¼‚å¸¸å¹´é¾„ä¸ºå¹³å‡å€¼
               avg_age = df_cleaned[df_cleaned['age'].between(0, 150)]['age'].mean()
               df_cleaned.loc[df_cleaned['user_id'] == user_id, 'age'] = round(avg_age)
               applied_fixes.append(f"ç”¨æˆ· {user_id}: å¹´é¾„ä¿®æ­£ä¸º {round(avg_age)}")
           
           elif 'å·¥èµ„' in suggestion['issue'] and 'è´Ÿ' in suggestion['issue']:
               # ä¿®æ­£è´Ÿå·¥èµ„ä¸ºéƒ¨é—¨å¹³å‡å·¥èµ„
               user_dept = df_cleaned.loc[df_cleaned['user_id'] == user_id, 'department'].iloc[0]
               if user_dept:
                   dept_avg_salary = df_cleaned[
                       (df_cleaned['department'] == user_dept) & 
                       (df_cleaned['salary'] >= 0)
                   ]['salary'].mean()
                   df_cleaned.loc[df_cleaned['user_id'] == user_id, 'salary'] = dept_avg_salary
                   applied_fixes.append(f"ç”¨æˆ· {user_id}: å·¥èµ„ä¿®æ­£ä¸ºéƒ¨é—¨å¹³å‡å€¼ {dept_avg_salary:.0f}")
           
           elif 'æ—¥æœŸé€»è¾‘' in suggestion['issue']:
               # äº¤æ¢å¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸ
               user_data = df_cleaned[df_cleaned['user_id'] == user_id]
               start_date = user_data['start_date'].iloc[0]
               end_date = user_data['end_date'].iloc[0]
               
               if start_date > end_date:
                   df_cleaned.loc[df_cleaned['user_id'] == user_id, 'start_date'] = end_date
                   df_cleaned.loc[df_cleaned['user_id'] == user_id, 'end_date'] = start_date
                   applied_fixes.append(f"ç”¨æˆ· {user_id}: äº¤æ¢å¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸ")
           
           elif 'éç©ºçº¦æŸ' in suggestion['issue'] and 'name' in suggestion['issue']:
               # ä¸ºç¼ºå¤±å§“åç”Ÿæˆå ä½ç¬¦
               df_cleaned.loc[df_cleaned['user_id'] == user_id, 'name'] = 'æœªçŸ¥ç”¨æˆ·'
               applied_fixes.append(f"ç”¨æˆ· {user_id}: ç¼ºå¤±å§“åè®¾ç½®ä¸º'æœªçŸ¥ç”¨æˆ·'")
           
           elif 'éç©ºçº¦æŸ' in suggestion['issue'] and 'department' in suggestion['issue']:
               # ä¸ºç¼ºå¤±éƒ¨é—¨è®¾ç½®é»˜è®¤å€¼
               df_cleaned.loc[df_cleaned['user_id'] == user_id, 'department'] = 'æœªåˆ†é…éƒ¨é—¨'
               applied_fixes.append(f"ç”¨æˆ· {user_id}: ç¼ºå¤±éƒ¨é—¨è®¾ç½®ä¸º'æœªåˆ†é…éƒ¨é—¨'")
       
       # æ˜¾ç¤ºåº”ç”¨çš„ä¿®å¤
       print("å·²åº”ç”¨çš„è‡ªåŠ¨ä¿®å¤:")
       for fix in applied_fixes:
           print(f"  âœ“ {fix}")
       
       return df_cleaned
   
   def generate_quality_report(df, df_cleaned, id_validation_results, logical_issues, integrity_issues):
       """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
       print("\n" + "=" * 80)
       print("æ•°æ®è´¨é‡æŠ¥å‘Š:")
       print("=" * 80)
       
       # è®¡ç®—è´¨é‡æŒ‡æ ‡
       original_records = len(df)
       cleaned_records = len(df_cleaned)
       
       # èº«ä»½è¯æœ‰æ•ˆæ€§
       id_valid_rate = sum(1 for result in id_validation_results if result['is_valid']) / len(id_validation_results) * 100
       
       # é€»è¾‘é—®é¢˜æ•°é‡
       logical_issue_count = len(logical_issues)
       
       # å®Œæ•´æ€§é—®é¢˜æ•°é‡
       integrity_issue_count = len(integrity_issues)
       
       # ç¼ºå¤±å€¼æ”¹å–„
       original_missing = df.isnull().sum().sum()
       cleaned_missing = df_cleaned.isnull().sum().sum()
       missing_improvement = original_missing - cleaned_missing
       
       print(f"æ•°æ®è´¨é‡æŒ‡æ ‡:")
       print(f"  â€¢ æ€»è®°å½•æ•°: {original_records} â†’ {cleaned_records}")
       print(f"  â€¢ èº«ä»½è¯æœ‰æ•ˆç‡: {id_valid_rate:.1f}%")
       print(f"  â€¢ é€»è¾‘é—®é¢˜æ•°: {logical_issue_count}")
       print(f"  â€¢ å®Œæ•´æ€§é—®é¢˜æ•°: {integrity_issue_count}")
       print(f"  â€¢ ç¼ºå¤±å€¼å‡å°‘: {missing_improvement}")
       
       # è´¨é‡è¯„åˆ† (ç®€åŒ–è®¡ç®—)
       quality_score = 100
       quality_score -= (100 - id_valid_rate) * 0.3  # èº«ä»½è¯æœ‰æ•ˆæ€§æƒé‡30%
       quality_score -= min(logical_issue_count * 5, 30)  # é€»è¾‘é—®é¢˜æƒé‡
       quality_score -= min(integrity_issue_count * 3, 20)  # å®Œæ•´æ€§é—®é¢˜æƒé‡
       quality_score = max(quality_score, 0)
       
       print(f"  â€¢ æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.1f}/100")
       
       # è´¨é‡ç­‰çº§
       if quality_score >= 90:
           grade = "ä¼˜ç§€"
       elif quality_score >= 80:
           grade = "è‰¯å¥½"
       elif quality_score >= 70:
           grade = "ä¸€èˆ¬"
       elif quality_score >= 60:
           grade = "åŠæ ¼"
       else:
           grade = "ä¸åŠæ ¼"
       
       print(f"  â€¢ è´¨é‡ç­‰çº§: {grade}")
       
       return quality_score, grade
   
   def save_cleaned_data(df_cleaned, quality_score, grade):
       """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
       output_file = 'business_data_cleaned.csv'
       df_cleaned.to_csv(output_file, index=False, encoding='utf-8-sig')
       print(f"\næ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
       
       # ä¿å­˜è´¨é‡æŠ¥å‘Š
       report_file = 'data_quality_report.txt'
       with open(report_file, 'w', encoding='utf-8') as f:
           f.write("ä¸šåŠ¡æ•°æ®è´¨é‡æŠ¥å‘Š\n")
           f.write("=" * 50 + "\n")
           f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
           f.write(f"æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.1f}/100\n")
           f.write(f"è´¨é‡ç­‰çº§: {grade}\n")
           f.write(f"è¾“å‡ºæ–‡ä»¶: {output_file}\n\n")
           
           f.write("æ¸…æ´—æ€»ç»“:\n")
           f.write("  â€¢ éªŒè¯äº†èº«ä»½è¯å·æ ¼å¼å’Œæœ‰æ•ˆæ€§\n")
           f.write("  â€¢ æ£€æŸ¥äº†æ•°æ®é€»è¾‘å…³ç³»ï¼ˆæ—¥æœŸã€å¹´é¾„ã€å·¥èµ„ç­‰ï¼‰\n")
           f.write("  â€¢ éªŒè¯äº†æ•°æ®å®Œæ•´æ€§çº¦æŸï¼ˆéç©ºã€å”¯ä¸€æ€§ã€å¤–é”®ç­‰ï¼‰\n")
           f.write("  â€¢ åº”ç”¨äº†è‡ªåŠ¨æ•°æ®æ¸…æ´—è§„åˆ™\n")
       
       print(f"è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
   
   def main():
       """ä¸»å‡½æ•°"""
       print("å¤æ‚è§„åˆ™æ¸…æ´—è„šæœ¬")
       print("=" * 100)
       
       # åˆ›å»ºæµ‹è¯•æ•°æ®
       print("åˆ›å»ºæµ‹è¯•æ•°æ®...")
       df = create_test_data()
       
       # åˆ†æåˆå§‹æ•°æ®
       analyze_initial_data(df)
       
       # éªŒè¯èº«ä»½è¯å·
       id_validation_results = validate_id_cards(df)
       
       # æ£€æŸ¥é€»è¾‘å…³ç³»
       logical_issues = check_logical_relationships(df)
       
       # éªŒè¯å®Œæ•´æ€§çº¦æŸ
       integrity_issues = validate_integrity_constraints(df)
       
       # ç”Ÿæˆæ¸…æ´—å»ºè®®
       cleaning_plan = generate_cleaning_suggestions(df, id_validation_results, logical_issues, integrity_issues)
       
       # åº”ç”¨æ•°æ®æ¸…æ´—
       df_cleaned = apply_data_cleaning(df, cleaning_plan)
       
       # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
       quality_score, grade = generate_quality_report(df, df_cleaned, id_validation_results, logical_issues, integrity_issues)
       
       # ä¿å­˜ç»“æœ
       save_cleaned_data(df_cleaned, quality_score, grade)
       
       print("\n" + "=" * 100)
       print("å¤æ‚è§„åˆ™æ¸…æ´—å®Œæˆ!")
       print("=" * 100)
   
   if __name__ == "__main__":
       main()
   ```

3. ### éç»“æ„åŒ–æ•°æ®æ¸…æ´—

   - ç¼–å†™è„šæœ¬å¤„ç†åŠç»“æ„åŒ–æ—¥å¿—æ•°æ®ï¼š
   - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®ä¿¡æ¯
   - è§£æåµŒå¥—JSONç»“æ„
   - å¤„ç†ä¸å®Œæ•´è®°å½•

   ```python
   import pandas as pd
   import numpy as np
   import re
   import json
   import os
   from datetime import datetime, timedelta
   import warnings
   from typing import Dict, List, Any, Optional
   warnings.filterwarnings('ignore')
   
   def create_test_log_data():
       """åˆ›å»ºåŒ…å«å„ç§åŠç»“æ„åŒ–æ—¥å¿—æ•°æ®çš„æµ‹è¯•æ–‡ä»¶"""
       
       log_entries = [
           # æ ‡å‡†æ ¼å¼æ—¥å¿—
           '2023-10-15 08:30:15 [INFO] User login successful. {"user_id": "U1001", "ip": "192.168.1.100", "session_id": "SESS001", "user_agent": "Mozilla/5.0"}',
           
           # åµŒå¥—JSONç»“æ„
           '2023-10-15 08:31:22 [ERROR] Database connection failed. {"error": {"code": 500, "message": "Connection timeout"}, "context": {"database": "users_db", "retry_count": 3, "last_success": "2023-10-15 08:25:00"}}',
           
           # ä¸å®Œæ•´è®°å½• - ç¼ºå°‘æ—¶é—´æˆ³
           '[WARN] High memory usage detected. {"memory_usage": 85, "process": "web_server", "recommendation": "restart service"}',
           
           # æ··åˆæ ¼å¼æ—¥å¿—
           '2023-10-15 08:32:45 [INFO] Order created. User: U1002, OrderID: ORD001, Amount: $150.75, Items: [{"product_id": "P001", "quantity": 2}, {"product_id": "P005", "quantity": 1}]',
           
           # æŸåçš„JSON
           '2023-10-15 08:33:10 [ERROR] Payment processing failed. {"transaction_id": "TXN1001", "status": "failed", "reason": "insufficient funds", "amount": 75.50, invalid json here',
           
           # å¤šè¡Œæ—¥å¿—
           '2023-10-15 08:34:20 [DEBUG] Stack trace: \n  File "app.py", line 150\n  File "utils.py", line 45\nException: Division by zero\n{"trace_id": "TRACE001", "component": "calculator"}',
           
           # æ— JSONçš„çº¯æ–‡æœ¬æ—¥å¿—
           '2023-10-15 08:35:30 [INFO] Server started successfully on port 8080',
           
           # å¤æ‚çš„åµŒå¥—ç»“æ„
           '2023-10-15 08:36:40 [INFO] API response. {"request": {"method": "POST", "endpoint": "/api/users", "headers": {"Content-Type": "application/json"}}, "response": {"status": 201, "body": {"user": {"id": "U1003", "profile": {"name": "John Doe", "preferences": {"theme": "dark", "language": "en"}}}}}}',
           
           # ä¸å®Œæ•´è®°å½• - ç¼ºå°‘æ—¥å¿—çº§åˆ«
           '2023-10-15 08:37:55 User logout. {"user_id": "U1001", "session_duration": 1800}',
           
           # æ ¼å¼æ··ä¹±çš„æ—¥å¿—
           'ERROR 2023-10-15 08:38:12 Cache miss for key: user_profile_U1004 TTL: 300',
           
           # åŒ…å«æ•°ç»„çš„JSON
           '2023-10-15 08:39:25 [INFO] Batch processing completed. {"batch_id": "BATCH001", "processed_items": 1500, "results": [{"item_id": "I001", "status": "success"}, {"item_id": "I002", "status": "failed", "error": "timeout"}]}',
           
           # ä¸å®Œæ•´è®°å½• - åªæœ‰JSON
           '{"timestamp": "2023-10-15 08:40:00", "event": "heartbeat", "service": "auth_service", "status": "healthy"}',
           
           # ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç é—®é¢˜
           '2023-10-15 08:41:15 [INFO] User input received. {"user_id": "U1005", "input": "Hello & Welcome! <script>alert(\'test\')</script>", "encoding": "UTF-8"}'
       ]
       
       # å†™å…¥æ—¥å¿—æ–‡ä»¶
       with open('application.log', 'w', encoding='utf-8') as f:
           for entry in log_entries:
               f.write(entry + '\n')
       
       print("æµ‹è¯•æ—¥å¿—æ•°æ®å·²åˆ›å»ºåˆ° 'application.log'")
       return log_entries
   
   def analyze_log_structure(log_entries):
       """åˆ†ææ—¥å¿—æ•°æ®çš„ç»“æ„å’Œæ¨¡å¼"""
       print("=" * 80)
       print("æ—¥å¿—ç»“æ„åˆ†æ:")
       print("=" * 80)
       
       patterns = {
           'timestamp': r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}',
           'log_level': r'\[(INFO|ERROR|WARN|DEBUG)\]',
           'json_object': r'\{.*\}',
           'json_array': r'\[.*\]'
       }
       
       stats = {
           'total_entries': len(log_entries),
           'has_timestamp': 0,
           'has_log_level': 0,
           'has_json': 0,
           'has_nested_json': 0,
           'incomplete_records': 0
       }
       
       for i, entry in enumerate(log_entries):
           print(f"\næ¡ç›® {i+1}:")
           print(f"  å†…å®¹: {entry[:100]}{'...' if len(entry) > 100 else ''}")
           
           has_timestamp = bool(re.search(patterns['timestamp'], entry))
           has_log_level = bool(re.search(patterns['log_level'], entry))
           has_json = bool(re.search(patterns['json_object'], entry))
           
           stats['has_timestamp'] += 1 if has_timestamp else 0
           stats['has_log_level'] += 1 if has_log_level else 0
           stats['has_json'] += 1 if has_json else 0
           
           # æ£€æŸ¥åµŒå¥—JSON
           if has_json:
               try:
                   json_match = re.search(r'(\{.*\})', entry, re.DOTALL)
                   if json_match:
                       json_str = json_match.group(1)
                       parsed = json.loads(json_str)
                       if has_nested_structure(parsed):
                           stats['has_nested_json'] += 1
               except:
                   pass
           
           # æ£€æŸ¥å®Œæ•´æ€§
           if not has_timestamp or not has_log_level:
               stats['incomplete_records'] += 1
           
           print(f"  æ—¶é—´æˆ³: {'âœ“' if has_timestamp else 'âœ—'}")
           print(f"  æ—¥å¿—çº§åˆ«: {'âœ“' if has_log_level else 'âœ—'}")
           print(f"  JSONæ•°æ®: {'âœ“' if has_json else 'âœ—'}")
       
       print(f"\nç»Ÿè®¡æ‘˜è¦:")
       print(f"  æ€»æ—¥å¿—æ¡ç›®: {stats['total_entries']}")
       print(f"  åŒ…å«æ—¶é—´æˆ³: {stats['has_timestamp']} ({stats['has_timestamp']/stats['total_entries']*100:.1f}%)")
       print(f"  åŒ…å«æ—¥å¿—çº§åˆ«: {stats['has_log_level']} ({stats['has_log_level']/stats['total_entries']*100:.1f}%)")
       print(f"  åŒ…å«JSON: {stats['has_json']} ({stats['has_json']/stats['total_entries']*100:.1f}%)")
       print(f"  åŒ…å«åµŒå¥—JSON: {stats['has_nested_json']}")
       print(f"  ä¸å®Œæ•´è®°å½•: {stats['incomplete_records']} ({stats['incomplete_records']/stats['total_entries']*100:.1f}%)")
       
       return stats
   
   def has_nested_structure(obj, depth=0):
       """æ£€æŸ¥JSONå¯¹è±¡æ˜¯å¦æœ‰åµŒå¥—ç»“æ„"""
       if depth > 3:  # é˜²æ­¢æ— é™é€’å½’
           return False
       
       if isinstance(obj, dict):
           for value in obj.values():
               if isinstance(value, (dict, list)):
                   return True
               elif has_nested_structure(value, depth + 1):
                   return True
       elif isinstance(obj, list):
           for item in obj:
               if isinstance(item, (dict, list)):
                   return True
               elif has_nested_structure(item, depth + 1):
                   return True
       return False
   
   def extract_with_regex(log_entries):
       """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®ä¿¡æ¯"""
       print("\n" + "=" * 80)
       print("æ­£åˆ™è¡¨è¾¾å¼æå–:")
       print("=" * 80)
       
       # å®šä¹‰æå–æ¨¡å¼
       patterns = {
           'timestamp': r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})',
           'log_level': r'\[(INFO|ERROR|WARN|DEBUG|WARNING)\]',
           'json_data': r'(\{.*\})',
           'user_id': r'user[_\s]?id[:\s"]*([^\s,"}]+)',
           'ip_address': r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})',
           'error_code': r'error[_\s]?code[:\s"]*(\d+)',
           'amount': r'amount[:\s\$]*([\d.]+)',
           'order_id': r'order[_\s]?id[:\s"]*([^\s,"}]+)',
       }
       
       extracted_data = []
       
       for i, entry in enumerate(log_entries):
           entry_data = {
               'line_number': i + 1,
               'original_entry': entry,
               'extracted_fields': {}
           }
           
           # æå–å„ä¸ªå­—æ®µ
           for field, pattern in patterns.items():
               matches = re.findall(pattern, entry, re.IGNORECASE)
               if matches:
                   entry_data['extracted_fields'][field] = matches[0] if len(matches) == 1 else matches
           
           extracted_data.append(entry_data)
       
       # æ˜¾ç¤ºæå–ç»“æœ
       print("æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœ:")
       for data in extracted_data[:5]:  # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
           print(f"\nè¡Œå· {data['line_number']}:")
           print(f"  åŸå§‹: {data['original_entry'][:80]}...")
           if data['extracted_fields']:
               for field, value in data['extracted_fields'].items():
                   print(f"  {field}: {value}")
           else:
               print("  æ— æå–å­—æ®µ")
       
       return extracted_data
   
   def parse_json_structures(log_entries):
       """è§£æåµŒå¥—JSONç»“æ„"""
       print("\n" + "=" * 80)
       print("JSONç»“æ„è§£æ:")
       print("=" * 80)
       
       parsed_data = []
       json_parse_stats = {
           'successful_parses': 0,
           'failed_parses': 0,
           'nested_structures': 0,
           'array_structures': 0
       }
       
       for i, entry in enumerate(log_entries):
           entry_data = {
               'line_number': i + 1,
               'original_entry': entry,
               'json_data': None,
               'parse_success': False,
               'parse_error': None,
               'flattened_fields': {}
           }
           
           # å°è¯•æå–å’Œè§£æJSON
           json_match = re.search(r'(\{.*\})', entry, re.DOTALL)
           if json_match:
               json_str = json_match.group(1)
               
               try:
                   # å°è¯•è§£æJSON
                   parsed_json = json.loads(json_str)
                   entry_data['json_data'] = parsed_json
                   entry_data['parse_success'] = True
                   json_parse_stats['successful_parses'] += 1
                   
                   # æ£€æŸ¥åµŒå¥—ç»“æ„
                   if has_nested_structure(parsed_json):
                       json_parse_stats['nested_structures'] += 1
                   
                   # å±•å¹³åµŒå¥—ç»“æ„
                   flattened = flatten_json(parsed_json)
                   entry_data['flattened_fields'] = flattened
                   
                   # æ£€æŸ¥æ•°ç»„ç»“æ„
                   if has_array_structure(parsed_json):
                       json_parse_stats['array_structures'] += 1
                   
               except json.JSONDecodeError as e:
                   entry_data['parse_error'] = str(e)
                   json_parse_stats['failed_parses'] += 1
                   
                   # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é”™è¯¯
                   fixed_json = try_fix_json(json_str)
                   if fixed_json:
                       try:
                           parsed_json = json.loads(fixed_json)
                           entry_data['json_data'] = parsed_json
                           entry_data['parse_success'] = True
                           entry_data['flattened_fields'] = flatten_json(parsed_json)
                           json_parse_stats['successful_parses'] += 1
                           json_parse_stats['failed_parses'] -= 1
                           print(f"è¡Œ {i+1}: JSONä¿®å¤æˆåŠŸ")
                       except:
                           pass
           
           parsed_data.append(entry_data)
       
       # æ˜¾ç¤ºè§£æç»Ÿè®¡
       print(f"JSONè§£æç»Ÿè®¡:")
       print(f"  æˆåŠŸè§£æ: {json_parse_stats['successful_parses']}")
       print(f"  è§£æå¤±è´¥: {json_parse_stats['failed_parses']}")
       print(f"  åµŒå¥—ç»“æ„: {json_parse_stats['nested_structures']}")
       print(f"  æ•°ç»„ç»“æ„: {json_parse_stats['array_structures']}")
       
       # æ˜¾ç¤ºè§£æç¤ºä¾‹
       print(f"\nJSONè§£æç¤ºä¾‹:")
       for data in parsed_data[:3]:
           if data['parse_success'] and data['flattened_fields']:
               print(f"\nè¡Œå· {data['line_number']}:")
               for key, value in list(data['flattened_fields'].items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå­—æ®µ
                   print(f"  {key}: {str(value)[:50]}{'...' if len(str(value)) > 50 else ''}")
       
       return parsed_data, json_parse_stats
   
   def flatten_json(obj, parent_key='', sep='_'):
       """å±•å¹³åµŒå¥—JSONç»“æ„"""
       items = {}
       
       if isinstance(obj, dict):
           for k, v in obj.items():
               new_key = f"{parent_key}{sep}{k}" if parent_key else k
               if isinstance(v, dict):
                   items.update(flatten_json(v, new_key, sep=sep))
               elif isinstance(v, list):
                   # å¤„ç†æ•°ç»„ - è½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–è¿›ä¸€æ­¥å¤„ç†
                   if all(isinstance(item, (str, int, float)) for item in v):
                       items[new_key] = ', '.join(map(str, v))
                   else:
                       # å¯¹äºå¤æ‚æ•°ç»„ï¼Œåªå–ç¬¬ä¸€ä¸ªå…ƒç´ è¿›è¡Œå±•å¹³ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                       if v and isinstance(v[0], dict):
                           items.update(flatten_json(v[0], f"{new_key}_item0", sep=sep))
                       else:
                           items[new_key] = str(v)
               else:
                   items[new_key] = v
       else:
           items[parent_key] = obj
       
       return items
   
   def has_array_structure(obj):
       """æ£€æŸ¥å¯¹è±¡æ˜¯å¦åŒ…å«æ•°ç»„ç»“æ„"""
       if isinstance(obj, list) and len(obj) > 0:
           return True
       
       if isinstance(obj, dict):
           for value in obj.values():
               if has_array_structure(value):
                   return True
       
       return False
   
   def try_fix_json(json_str):
       """å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é”™è¯¯"""
       fixes = [
           # ä¿®å¤ç¼ºå°‘å¼•å·çš„é”®
           (r'(\w+):', r'"\1":'),
           # ä¿®å¤å•å¼•å·
           (r"'", '"'),
           # ä¿®å¤å°¾éšé€—å·
           (r',\s*}', '}'),
           (r',\s*]', ']'),
           # ä¿®å¤æœªè½¬ä¹‰çš„æ§åˆ¶å­—ç¬¦
           (r'\n', '\\n'),
           (r'\t', '\\t'),
       ]
       
       fixed = json_str
       for pattern, replacement in fixes:
           fixed = re.sub(pattern, replacement, fixed)
       
       return fixed
   
   def handle_incomplete_records(parsed_data):
       """å¤„ç†ä¸å®Œæ•´è®°å½•"""
       print("\n" + "=" * 80)
       print("ä¸å®Œæ•´è®°å½•å¤„ç†:")
       print("=" * 80)
       
       incomplete_records = []
       completion_strategies = {
           'missing_timestamp': 'ä»ä¸Šä¸‹æ–‡æ¨æ–­æˆ–ä½¿ç”¨å½“å‰æ—¶é—´',
           'missing_log_level': 'æ ¹æ®å†…å®¹æ¨æ–­çº§åˆ«',
           'malformed_json': 'å°è¯•ä¿®å¤æˆ–æå–å¯ç”¨éƒ¨åˆ†',
           'partial_data': 'æ ‡è®°ä¸ºä¸å®Œæ•´ä½†ä¿ç•™å¯ç”¨ä¿¡æ¯'
       }
       
       for data in parsed_data:
           entry = data['original_entry']
           issues = []
           
           # æ£€æµ‹é—®é¢˜
           if not re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', entry):
               issues.append('missing_timestamp')
           
           if not re.search(r'\[(INFO|ERROR|WARN|DEBUG)\]', entry):
               issues.append('missing_log_level')
           
           if not data['parse_success'] and re.search(r'\{.*\}', entry):
               issues.append('malformed_json')
           
           if len(entry.strip()) < 10:  # éå¸¸çŸ­çš„è®°å½•
               issues.append('partial_data')
           
           if issues:
               record_info = {
                   'line_number': data['line_number'],
                   'original_entry': entry,
                   'issues': issues,
                   'suggested_fixes': [completion_strategies[issue] for issue in issues],
                   'repaired_entry': repair_incomplete_record(data, issues)
               }
               incomplete_records.append(record_info)
       
       # æ˜¾ç¤ºä¸å®Œæ•´è®°å½•
       print(f"å‘ç° {len(incomplete_records)} ä¸ªä¸å®Œæ•´è®°å½•:")
       for record in incomplete_records[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
           print(f"\nè¡Œå· {record['line_number']}:")
           print(f"  é—®é¢˜: {', '.join(record['issues'])}")
           print(f"  åŸå§‹: {record['original_entry'][:80]}...")
           print(f"  ä¿®å¤å»ºè®®: {', '.join(record['suggested_fixes'])}")
           if record['repaired_entry']:
               print(f"  ä¿®å¤å: {record['repaired_entry'][:80]}...")
       
       return incomplete_records
   
   def repair_incomplete_record(data, issues):
       """ä¿®å¤ä¸å®Œæ•´è®°å½•"""
       entry = data['original_entry']
       repaired = entry
       
       # ä¿®å¤ç¼ºå¤±çš„æ—¶é—´æˆ³
       if 'missing_timestamp' in issues:
           # å°è¯•ä»JSONä¸­æå–æ—¶é—´æˆ³
           if data.get('json_data') and 'timestamp' in data['json_data']:
               timestamp = data['json_data']['timestamp']
               repaired = f"{timestamp} {repaired}"
           else:
               # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºå ä½ç¬¦
               current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
               repaired = f"{current_time} {repaired}"
       
       # ä¿®å¤ç¼ºå¤±çš„æ—¥å¿—çº§åˆ«
       if 'missing_log_level' in issues:
           # æ ¹æ®å†…å®¹æ¨æ–­æ—¥å¿—çº§åˆ«
           entry_lower = entry.lower()
           if any(word in entry_lower for word in ['error', 'failed', 'exception']):
               log_level = '[ERROR]'
           elif any(word in entry_lower for word in ['warn', 'warning']):
               log_level = '[WARN]'
           elif any(word in entry_lower for word in ['debug', 'trace']):
               log_level = '[DEBUG]'
           else:
               log_level = '[INFO]'
           
           # æ’å…¥æ—¥å¿—çº§åˆ«
           if re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', repaired):
               repaired = re.sub(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', r'\1 ' + log_level, repaired)
           else:
               repaired = f"{log_level} {repaired}"
       
       # ä¿®å¤æ ¼å¼é”™è¯¯çš„JSON
       if 'malformed_json' in issues and data.get('parse_error'):
           json_match = re.search(r'(\{.*\})', entry, re.DOTALL)
           if json_match:
               original_json = json_match.group(1)
               fixed_json = try_fix_json(original_json)
               repaired = repaired.replace(original_json, fixed_json)
       
       return repaired
   
   def create_structured_dataframe(parsed_data, incomplete_records):
       """åˆ›å»ºç»“æ„åŒ–çš„DataFrame"""
       print("\n" + "=" * 80)
       print("åˆ›å»ºç»“æ„åŒ–æ•°æ®:")
       print("=" * 80)
       
       structured_data = []
       
       for data in parsed_data:
           record = {
               'line_number': data['line_number'],
               'timestamp': None,
               'log_level': None,
               'message': None,
               'has_json': data['parse_success'],
               'is_complete': data['line_number'] not in [r['line_number'] for r in incomplete_records]
           }
           
           # æå–åŸºç¡€å­—æ®µ
           timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', data['original_entry'])
           if timestamp_match:
               record['timestamp'] = timestamp_match.group(1)
           
           log_level_match = re.search(r'\[(INFO|ERROR|WARN|DEBUG)\]', data['original_entry'])
           if log_level_match:
               record['log_level'] = log_level_match.group(1)
           
           # æå–æ¶ˆæ¯éƒ¨åˆ†ï¼ˆå»é™¤æ—¶é—´æˆ³å’Œæ—¥å¿—çº§åˆ«ï¼‰
           message = data['original_entry']
           if timestamp_match:
               message = message.replace(timestamp_match.group(0), '').strip()
           if log_level_match:
               message = message.replace(f'[{log_level_match.group(1)}]', '').strip()
           record['message'] = message
           
           # æ·»åŠ å±•å¹³çš„JSONå­—æ®µ
           if data['flattened_fields']:
               for key, value in data['flattened_fields'].items():
                   # ç®€åŒ–é”®åï¼Œé¿å…å¤ªé•¿çš„åˆ—å
                   simple_key = re.sub(r'[^a-zA-Z0-9_]', '_', key)
                   simple_key = re.sub(r'_+', '_', simple_key).strip('_')
                   record[f'json_{simple_key}'] = value
           
           structured_data.append(record)
       
       # åˆ›å»ºDataFrame
       df = pd.DataFrame(structured_data)
       
       print(f"ç»“æ„åŒ–æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
       print(f"åŒ…å«JSONçš„è®°å½•: {df['has_json'].sum()}")
       print(f"å®Œæ•´è®°å½•: {df['is_complete'].sum()}")
       
       # æ˜¾ç¤ºDataFrameçš„å‰å‡ è¡Œ
       print(f"\nç»“æ„åŒ–æ•°æ®ç¤ºä¾‹:")
       display_columns = ['line_number', 'timestamp', 'log_level', 'message', 'has_json', 'is_complete']
       json_columns = [col for col in df.columns if col.startswith('json_')]
       if json_columns:
           display_columns.extend(json_columns[:3])  # æ˜¾ç¤ºå‰3ä¸ªJSONå­—æ®µ
       
       print(df[display_columns].head().to_string(index=False))
       
       return df
   
   def generate_cleaning_report(stats, json_parse_stats, incomplete_records, df):
       """ç”Ÿæˆæ•°æ®æ¸…æ´—æŠ¥å‘Š"""
       print("\n" + "=" * 80)
       print("éç»“æ„åŒ–æ•°æ®æ¸…æ´—æŠ¥å‘Š:")
       print("=" * 80)
       
       total_records = stats['total_entries']
       complete_records = df['is_complete'].sum()
       records_with_json = df['has_json'].sum()
       
       print(f"æ¸…æ´—ç»Ÿè®¡:")
       print(f"  â€¢ æ€»æ—¥å¿—è®°å½•: {total_records}")
       print(f"  â€¢ æˆåŠŸè§£æJSON: {json_parse_stats['successful_parses']} ({json_parse_stats['successful_parses']/total_records*100:.1f}%)")
       print(f"  â€¢ å¤„ç†ä¸å®Œæ•´è®°å½•: {len(incomplete_records)} ({len(incomplete_records)/total_records*100:.1f}%)")
       print(f"  â€¢ æœ€ç»ˆå®Œæ•´è®°å½•: {complete_records} ({complete_records/total_records*100:.1f}%)")
       print(f"  â€¢ åŒ…å«ç»“æ„åŒ–æ•°æ®: {records_with_json} ({records_with_json/total_records*100:.1f}%)")
       
       # æ•°æ®è´¨é‡è¯„åˆ†
       quality_score = 0
       quality_score += (json_parse_stats['successful_parses'] / total_records) * 40  # JSONè§£ææƒé‡
       quality_score += (complete_records / total_records) * 40  # å®Œæ•´æ€§æƒé‡
       quality_score += (stats['has_timestamp'] / total_records) * 10  # æ—¶é—´æˆ³æƒé‡
       quality_score += (stats['has_log_level'] / total_records) * 10  # æ—¥å¿—çº§åˆ«æƒé‡
       
       print(f"  â€¢ æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.1f}/100")
       
       if quality_score >= 80:
           grade = "ä¼˜ç§€"
       elif quality_score >= 70:
           grade = "è‰¯å¥½"
       elif quality_score >= 60:
           grade = "ä¸€èˆ¬"
       else:
           grade = "éœ€è¦æ”¹è¿›"
       
       print(f"  â€¢ è´¨é‡ç­‰çº§: {grade}")
       
       return quality_score, grade
   
   def save_cleaned_data(df, quality_score, grade):
       """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
       output_file = 'cleaned_log_data.csv'
       df.to_csv(output_file, index=False, encoding='utf-8-sig')
       print(f"\næ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
       
       # ä¿å­˜JSONå­—æ®µå•ç‹¬çš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
       json_columns = [col for col in df.columns if col.startswith('json_')]
       if json_columns:
           json_df = df[['line_number'] + json_columns]
           json_output_file = 'extracted_json_fields.csv'
           json_df.to_csv(json_output_file, index=False, encoding='utf-8-sig')
           print(f"æå–çš„JSONå­—æ®µå·²ä¿å­˜åˆ°: {json_output_file}")
       
       # ä¿å­˜æ¸…æ´—æŠ¥å‘Š
       report_file = 'log_cleaning_report.txt'
       with open(report_file, 'w', encoding='utf-8') as f:
           f.write("éç»“æ„åŒ–æ—¥å¿—æ•°æ®æ¸…æ´—æŠ¥å‘Š\n")
           f.write("=" * 50 + "\n")
           f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
           f.write(f"æ•°æ®è´¨é‡è¯„åˆ†: {quality_score:.1f}/100\n")
           f.write(f"è´¨é‡ç­‰çº§: {grade}\n")
           f.write(f"è¾“å‡ºæ–‡ä»¶: {output_file}\n\n")
           
           f.write("æ¸…æ´—æ€»ç»“:\n")
           f.write("  â€¢ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–äº†å…³é”®ä¿¡æ¯\n")
           f.write("  â€¢ è§£æäº†åµŒå¥—JSONç»“æ„å¹¶å±•å¹³\n")
           f.write("  â€¢ å¤„ç†äº†ä¸å®Œæ•´è®°å½•å¹¶å°è¯•ä¿®å¤\n")
           f.write("  â€¢ åˆ›å»ºäº†ç»“æ„åŒ–çš„æ•°æ®è¡¨\n")
       
       print(f"æ¸…æ´—æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
   
   def main():
       """ä¸»å‡½æ•°"""
       print("éç»“æ„åŒ–æ•°æ®æ¸…æ´—è„šæœ¬")
       print("=" * 100)
       
       # åˆ›å»ºæµ‹è¯•æ•°æ®
       print("åˆ›å»ºæµ‹è¯•æ—¥å¿—æ•°æ®...")
       log_entries = create_test_log_data()
       
       # åˆ†ææ—¥å¿—ç»“æ„
       stats = analyze_log_structure(log_entries)
       
       # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¿¡æ¯
       extracted_data = extract_with_regex(log_entries)
       
       # è§£æJSONç»“æ„
       parsed_data, json_parse_stats = parse_json_structures(log_entries)
       
       # å¤„ç†ä¸å®Œæ•´è®°å½•
       incomplete_records = handle_incomplete_records(parsed_data)
       
       # åˆ›å»ºç»“æ„åŒ–æ•°æ®
       df = create_structured_dataframe(parsed_data, incomplete_records)
       
       # ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š
       quality_score, grade = generate_cleaning_report(stats, json_parse_stats, incomplete_records, df)
       
       # ä¿å­˜ç»“æœ
       save_cleaned_data(df, quality_score, grade)
       
       print("\n" + "=" * 100)
       print("éç»“æ„åŒ–æ•°æ®æ¸…æ´—å®Œæˆ!")
       print("=" * 100)
   
   if __name__ == "__main__":
       main()
   ```

4. ### æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š

   - ç¼–å†™è„šæœ¬ç”Ÿæˆæ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Šï¼š
     - è®¡ç®—å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€ä¸€è‡´æ€§æŒ‡æ ‡
     - å¯è§†åŒ–æ•°æ®åˆ†å¸ƒå’Œå¼‚å¸¸
     - è¾“å‡ºæ¸…æ´—å»ºè®®
   
   ```python
   import pandas as pd
   import numpy as np
   import matplotlib.pyplot as plt
   import seaborn as sns
   import warnings
   from datetime import datetime, timedelta
   import json
   from scipy import stats
   import missingno as msno
   from sklearn.ensemble import IsolationForest
   import plotly.express as px
   import plotly.graph_objects as go
   from plotly.subplots import make_subplots
   import io
   import base64
   
   warnings.filterwarnings('ignore')
   plt.style.use('default')
   
   class DataQualityAnalyzer:
       """æ•°æ®è´¨é‡åˆ†æå™¨"""
       
       def __init__(self, df, name="æ•°æ®é›†"):
           self.df = df.copy()
           self.name = name
           self.quality_metrics = {}
           self.visualizations = {}
           self.cleaning_recommendations = []
           
       def create_sample_data(self):
           """åˆ›å»ºåŒ…å«å„ç§æ•°æ®è´¨é‡é—®é¢˜çš„æµ‹è¯•æ•°æ®"""
           np.random.seed(42)
           
           data = {
               'customer_id': range(1, 1001),
               'name': [f'Customer_{i}' for i in range(1, 1001)],
               'age': np.random.normal(35, 10, 1000).clip(18, 80).astype(int),
               'email': [f'user{i}@example.com' for i in range(1, 1001)],
               'phone': [f'138{str(i).zfill(8)}' for i in range(1, 1001)],
               'registration_date': pd.date_range('2020-01-01', periods=1000, freq='D'),
               'last_login': pd.date_range('2023-01-01', periods=1000, freq='H'),
               'total_spent': np.random.exponential(100, 1000),
               'order_count': np.random.poisson(5, 1000),
               'city': np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'æˆéƒ½'], 1000),
               'segment': np.random.choice(['VIP', 'æ™®é€š', 'æ–°ç”¨æˆ·'], 1000, p=[0.1, 0.7, 0.2])
           }
           
           # æ·»åŠ æ•°æ®è´¨é‡é—®é¢˜
           df_sample = pd.DataFrame(data)
           
           # 1. ç¼ºå¤±å€¼
           df_sample.loc[df_sample.sample(50).index, 'age'] = np.nan
           df_sample.loc[df_sample.sample(30).index, 'email'] = None
           df_sample.loc[df_sample.sample(20).index, 'phone'] = ''
           df_sample.loc[df_sample.sample(40).index, 'last_login'] = pd.NaT
           
           # 2. å¼‚å¸¸å€¼
           df_sample.loc[df_sample.sample(10).index, 'age'] = [150, -5, 200, 0, 999, 18, 19, 20, 21, 22]
           df_sample.loc[df_sample.sample(15).index, 'total_spent'] = [-1000, 50000, 100000, 250000, 500000] * 3
           df_sample.loc[df_sample.sample(8).index, 'order_count'] = [-10, 1000, 500, 300, 200, 150, 120, 80]
           
           # 3. ä¸ä¸€è‡´æ•°æ®
           df_sample.loc[df_sample.sample(25).index, 'city'] = ['Unknown', 'å…¶ä»–', 'N/A', 'Beijing', 'Shanghai']
           df_sample.loc[df_sample.sample(20).index, 'segment'] = ['VIP', 'vip', 'æ™®é€šå®¢æˆ·', 'æ–°ç”¨æˆ·', 'New']
           
           # 4. æ ¼å¼é—®é¢˜
           df_sample.loc[df_sample.sample(15).index, 'email'] = ['invalid', 'user@', '@domain.com', 'no.at.sign']
           df_sample.loc[df_sample.sample(12).index, 'phone'] = ['123', '123456789012345', 'abc12345678']
           
           # 5. é€»è¾‘é”™è¯¯
           df_sample.loc[df_sample.sample(5).index, 'registration_date'] = df_sample['last_login'] + timedelta(days=30)
           
           return df_sample
       
       def calculate_completeness_metrics(self):
           """è®¡ç®—å®Œæ•´æ€§æŒ‡æ ‡"""
           print("è®¡ç®—å®Œæ•´æ€§æŒ‡æ ‡...")
           
           completeness = {}
           
           # 1. æ€»ä½“ç¼ºå¤±ç‡
           total_cells = self.df.size
           missing_cells = self.df.isnull().sum().sum() + (self.df == '').sum().sum()
           completeness['overall_missing_rate'] = (missing_cells / total_cells) * 100
           
           # 2. å„åˆ—ç¼ºå¤±ç‡
           column_missing_rates = {}
           for column in self.df.columns:
               null_count = self.df[column].isnull().sum()
               empty_count = (self.df[column] == '').sum()
               total_count = len(self.df)
               column_missing_rates[column] = ((null_count + empty_count) / total_count) * 100
           
           completeness['column_missing_rates'] = column_missing_rates
           
           # 3. å®Œæ•´è®°å½•æ¯”ä¾‹
           complete_records = self.df.notnull().all(axis=1).sum()
           completeness['complete_record_rate'] = (complete_records / len(self.df)) * 100
           
           # 4. æ•°æ®è¦†ç›–åº¦ï¼ˆåŸºäºå…³é”®å­—æ®µï¼‰
           key_columns = ['customer_id', 'name', 'email']  # å‡è®¾è¿™äº›æ˜¯å…³é”®å­—æ®µ
           key_coverage = {}
           for col in key_columns:
               if col in self.df.columns:
                   coverage = (self.df[col].notnull().sum() / len(self.df)) * 100
                   key_coverage[col] = coverage
           
           completeness['key_field_coverage'] = key_coverage
           
           self.quality_metrics['completeness'] = completeness
           return completeness
       
       def calculate_accuracy_metrics(self):
           """è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡"""
           print("è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡...")
           
           accuracy = {}
           
           # 1. æ ¼å¼å‡†ç¡®æ€§
           format_accuracy = {}
           
           # é‚®ç®±æ ¼å¼éªŒè¯
           if 'email' in self.df.columns:
               email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
               valid_emails = self.df['email'].str.match(email_pattern, na=False).sum()
               format_accuracy['email'] = (valid_emails / self.df['email'].notnull().sum()) * 100
           
           # æ‰‹æœºå·æ ¼å¼éªŒè¯
           if 'phone' in self.df.columns:
               phone_pattern = r'^1[3-9]\d{9}$'
               valid_phones = self.df['phone'].str.match(phone_pattern, na=False).sum()
               format_accuracy['phone'] = (valid_phones / self.df['phone'].notnull().sum()) * 100
           
           accuracy['format_accuracy'] = format_accuracy
           
           # 2. èŒƒå›´å‡†ç¡®æ€§
           range_accuracy = {}
           
           if 'age' in self.df.columns:
               valid_ages = self.df['age'].between(0, 120).sum()
               range_accuracy['age'] = (valid_ages / self.df['age'].notnull().sum()) * 100
           
           if 'total_spent' in self.df.columns:
               valid_spent = (self.df['total_spent'] >= 0).sum()
               range_accuracy['total_spent'] = (valid_spent / self.df['total_spent'].notnull().sum()) * 100
           
           accuracy['range_accuracy'] = range_accuracy
           
           # 3. ä¸šåŠ¡è§„åˆ™å‡†ç¡®æ€§
           business_accuracy = {}
           
           # æ£€æŸ¥æ³¨å†Œæ—¥æœŸæ—©äºæœ€åç™»å½•æ—¥æœŸ
           if all(col in self.df.columns for col in ['registration_date', 'last_login']):
               date_logic_valid = (self.df['registration_date'] <= self.df['last_login']).sum()
               business_accuracy['date_logic'] = (date_logic_valid / len(self.df)) * 100
           
           # æ£€æŸ¥è®¢å•æ•°é‡ä¸æ¶ˆè´¹é‡‘é¢çš„é€»è¾‘å…³ç³»
           if all(col in self.df.columns for col in ['order_count', 'total_spent']):
               # ç®€å•é€»è¾‘ï¼šæœ‰è®¢å•åº”è¯¥æœ‰æ¶ˆè´¹ï¼Œæœ‰æ¶ˆè´¹åº”è¯¥æœ‰è®¢å•
               order_spent_logic = ((self.df['order_count'] > 0) == (self.df['total_spent'] > 0)).sum()
               business_accuracy['order_spent_logic'] = (order_spent_logic / len(self.df)) * 100
           
           accuracy['business_accuracy'] = business_accuracy
           
           self.quality_metrics['accuracy'] = accuracy
           return accuracy
       
       def calculate_consistency_metrics(self):
           """è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡"""
           print("è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡...")
           
           consistency = {}
           
           # 1. æ•°æ®ç±»å‹ä¸€è‡´æ€§
           type_consistency = {}
           for column in self.df.columns:
               # æ£€æŸ¥åˆ—ä¸­æ•°æ®ç±»å‹æ˜¯å¦ä¸€è‡´
               unique_dtypes = self.df[column].apply(type).nunique()
               type_consistency[column] = unique_dtypes == 1
           
           consistency['type_consistency'] = type_consistency
           
           # 2. å€¼åŸŸä¸€è‡´æ€§
           domain_consistency = {}
           
           # åˆ†ç±»å­—æ®µçš„å€¼åŸŸæ£€æŸ¥
           categorical_columns = self.df.select_dtypes(include=['object']).columns
           for col in categorical_columns:
               if col in ['city', 'segment']:  # å‡è®¾è¿™äº›æ˜¯å·²çŸ¥çš„åˆ†ç±»å­—æ®µ
                   expected_values = {
                       'city': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'æˆéƒ½'],
                       'segment': ['VIP', 'æ™®é€š', 'æ–°ç”¨æˆ·']
                   }
                   if col in expected_values:
                       valid_values = self.df[col].isin(expected_values[col]).sum()
                       domain_consistency[col] = (valid_values / self.df[col].notnull().sum()) * 100
           
           consistency['domain_consistency'] = domain_consistency
           
           # 3. é‡å¤æ•°æ®æ£€æŸ¥
           duplicate_metrics = {}
           
           # å®Œå…¨é‡å¤è®°å½•
           complete_duplicates = self.df.duplicated().sum()
           duplicate_metrics['complete_duplicates'] = (complete_duplicates / len(self.df)) * 100
           
           # åŸºäºå…³é”®å­—æ®µçš„éƒ¨åˆ†é‡å¤
           if 'customer_id' in self.df.columns:
               key_duplicates = self.df.duplicated(subset=['customer_id']).sum()
               duplicate_metrics['key_duplicates'] = (key_duplicates / len(self.df)) * 100
           
           consistency['duplicate_metrics'] = duplicate_metrics
           
           # 4. è·¨å­—æ®µä¸€è‡´æ€§
           cross_field_consistency = {}
           
           # æ£€æŸ¥åŸå¸‚å’Œå®¢æˆ·åˆ†æ®µçš„ç»„åˆæ˜¯å¦åˆç†
           if all(col in self.df.columns for col in ['city', 'segment']):
               # è¿™é‡Œå¯ä»¥å®šä¹‰ä¸šåŠ¡è§„åˆ™ï¼Œæ¯”å¦‚æŸäº›åŸå¸‚ä¸åº”è¯¥æœ‰æŸäº›åˆ†æ®µ
               # ç®€åŒ–ç¤ºä¾‹ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å¼‚å¸¸ç»„åˆ
               valid_combinations = len(self.df)  # å‡è®¾æ‰€æœ‰ç»„åˆéƒ½æœ‰æ•ˆ
               cross_field_consistency['city_segment'] = (valid_combinations / len(self.df)) * 100
           
           consistency['cross_field_consistency'] = cross_field_consistency
           
           self.quality_metrics['consistency'] = consistency
           return consistency
       
       def detect_anomalies(self):
           """æ£€æµ‹æ•°æ®å¼‚å¸¸"""
           print("æ£€æµ‹æ•°æ®å¼‚å¸¸...")
           
           anomalies = {}
           
           # 1. æ•°å€¼å­—æ®µçš„ç»Ÿè®¡å¼‚å¸¸
           numerical_columns = self.df.select_dtypes(include=[np.number]).columns
           
           for col in numerical_columns:
               col_data = self.df[col].dropna()
               if len(col_data) > 0:
                   # ä½¿ç”¨Z-scoreæ£€æµ‹å¼‚å¸¸å€¼
                   z_scores = np.abs(stats.zscore(col_data))
                   outliers_z = (z_scores > 3).sum()
                   
                   # ä½¿ç”¨IQRæ£€æµ‹å¼‚å¸¸å€¼
                   Q1 = col_data.quantile(0.25)
                   Q3 = col_data.quantile(0.75)
                   IQR = Q3 - Q1
                   lower_bound = Q1 - 1.5 * IQR
                   upper_bound = Q3 + 1.5 * IQR
                   outliers_iqr = ((col_data < lower_bound) | (col_data > upper_bound)).sum()
                   
                   anomalies[col] = {
                       'z_score_outliers': outliers_z,
                       'z_score_outlier_rate': (outliers_z / len(col_data)) * 100,
                       'iqr_outliers': outliers_iqr,
                       'iqr_outlier_rate': (outliers_iqr / len(col_data)) * 100
                   }
           
           # 2. ä½¿ç”¨éš”ç¦»æ£®æ—æ£€æµ‹å¤šå˜é‡å¼‚å¸¸
           if len(numerical_columns) >= 2:
               try:
                   numerical_data = self.df[numerical_columns].dropna()
                   if len(numerical_data) > 10:
                       iso_forest = IsolationForest(contamination=0.1, random_state=42)
                       predictions = iso_forest.fit_predict(numerical_data)
                       multivariate_outliers = (predictions == -1).sum()
                       anomalies['multivariate_outliers'] = {
                           'count': multivariate_outliers,
                           'rate': (multivariate_outliers / len(numerical_data)) * 100
                       }
               except:
                   anomalies['multivariate_outliers'] = {'count': 0, 'rate': 0}
           
           self.quality_metrics['anomalies'] = anomalies
           return anomalies
       
       def create_visualizations(self):
           """åˆ›å»ºæ•°æ®è´¨é‡å¯è§†åŒ–"""
           print("åˆ›å»ºæ•°æ®å¯è§†åŒ–...")
           
           # 1. ç¼ºå¤±å€¼çŸ©é˜µ
           plt.figure(figsize=(12, 8))
           msno.matrix(self.df)
           plt.title(f'{self.name} - ç¼ºå¤±å€¼çŸ©é˜µ', fontsize=16, fontweight='bold')
           plt.tight_layout()
           self.visualizations['missing_matrix'] = self._fig_to_base64(plt)
           plt.close()
           
           # 2. æ•°æ®å®Œæ•´æ€§çƒ­åŠ›å›¾
           plt.figure(figsize=(10, 6))
           completeness_data = []
           for col in self.df.columns:
               complete_rate = (self.df[col].notnull().sum() / len(self.df)) * 100
               completeness_data.append(complete_rate)
           
           plt.barh(self.df.columns, completeness_data, color='skyblue')
           plt.xlabel('å®Œæ•´æ€§æ¯”ä¾‹ (%)')
           plt.title('å„å­—æ®µæ•°æ®å®Œæ•´æ€§', fontsize=14, fontweight='bold')
           plt.xlim(0, 100)
           plt.grid(axis='x', alpha=0.3)
           plt.tight_layout()
           self.visualizations['completeness_bar'] = self._fig_to_base64(plt)
           plt.close()
           
           # 3. æ•°å€¼å­—æ®µåˆ†å¸ƒå’Œå¼‚å¸¸å€¼
           numerical_columns = self.df.select_dtypes(include=[np.number]).columns
           
           if len(numerical_columns) > 0:
               fig, axes = plt.subplots(2, 2, figsize=(15, 10))
               axes = axes.flatten()
               
               for i, col in enumerate(numerical_columns[:4]):  # æœ€å¤šæ˜¾ç¤º4ä¸ªæ•°å€¼å­—æ®µ
                   if i < len(axes):
                       data = self.df[col].dropna()
                       axes[i].hist(data, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')
                       axes[i].set_title(f'{col} åˆ†å¸ƒ', fontweight='bold')
                       axes[i].set_xlabel(col)
                       axes[i].set_ylabel('é¢‘æ•°')
                       
                       # æ ‡è®°å¼‚å¸¸å€¼èŒƒå›´
                       Q1 = data.quantile(0.25)
                       Q3 = data.quantile(0.75)
                       IQR = Q3 - Q1
                       lower_bound = Q1 - 1.5 * IQR
                       upper_bound = Q3 + 1.5 * IQR
                       
                       outliers = data[(data < lower_bound) | (data > upper_bound)]
                       if len(outliers) > 0:
                           axes[i].axvspan(lower_bound, upper_bound, alpha=0.2, color='green', label='æ­£å¸¸èŒƒå›´')
                           axes[i].legend()
               
               plt.suptitle('æ•°å€¼å­—æ®µåˆ†å¸ƒä¸å¼‚å¸¸å€¼æ£€æµ‹', fontsize=16, fontweight='bold')
               plt.tight_layout()
               self.visualizations['numerical_distributions'] = self._fig_to_base64(plt)
               plt.close()
           
           # 4. åˆ†ç±»å­—æ®µåˆ†å¸ƒ
           categorical_columns = self.df.select_dtypes(include=['object']).columns
           
           if len(categorical_columns) > 0:
               fig, axes = plt.subplots(2, 2, figsize=(15, 10))
               axes = axes.flatten()
               
               for i, col in enumerate(categorical_columns[:4]):  # æœ€å¤šæ˜¾ç¤º4ä¸ªåˆ†ç±»å­—æ®µ
                   if i < len(axes):
                       value_counts = self.df[col].value_counts().head(10)  # åªæ˜¾ç¤ºå‰10ä¸ª
                       axes[i].bar(value_counts.index, value_counts.values, color='lightgreen')
                       axes[i].set_title(f'{col} åˆ†å¸ƒ', fontweight='bold')
                       axes[i].set_xlabel(col)
                       axes[i].set_ylabel('è®¡æ•°')
                       axes[i].tick_params(axis='x', rotation=45)
               
               plt.suptitle('åˆ†ç±»å­—æ®µåˆ†å¸ƒ', fontsize=16, fontweight='bold')
               plt.tight_layout()
               self.visualizations['categorical_distributions'] = self._fig_to_base64(plt)
               plt.close()
           
           # 5. æ•°æ®è´¨é‡è¯„åˆ†é›·è¾¾å›¾
           categories = ['å®Œæ•´æ€§', 'å‡†ç¡®æ€§', 'ä¸€è‡´æ€§']
           scores = [
               self.quality_metrics['completeness']['complete_record_rate'],
               np.mean([v for k, v in self.quality_metrics['accuracy']['format_accuracy'].items()]) if self.quality_metrics['accuracy']['format_accuracy'] else 0,
               np.mean([v for k, v in self.quality_metrics['consistency']['domain_consistency'].items()]) if self.quality_metrics['consistency']['domain_consistency'] else 0
           ]
           
           # å®Œæˆé›·è¾¾å›¾
           angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
           scores += scores[:1]  # é—­åˆé›·è¾¾å›¾
           angles += angles[:1]
           
           fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
           ax.plot(angles, scores, 'o-', linewidth=2, label='æ•°æ®è´¨é‡')
           ax.fill(angles, scores, alpha=0.25)
           ax.set_thetagrids(np.degrees(angles[:-1]), categories)
           ax.set_ylim(0, 100)
           ax.set_title('æ•°æ®è´¨é‡è¯„ä¼°é›·è¾¾å›¾', size=16, fontweight='bold')
           ax.grid(True)
           plt.tight_layout()
           self.visualizations['quality_radar'] = self._fig_to_base64(plt)
           plt.close()
       
       def _fig_to_base64(self, fig):
           """å°†matplotlibå›¾å½¢è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
           buf = io.BytesIO()
           fig.savefig(buf, format='png', dpi=100, bbox_inches='tight')
           buf.seek(0)
           img_str = base64.b64encode(buf.read()).decode('utf-8')
           buf.close()
           return img_str
       
       def generate_cleaning_recommendations(self):
           """ç”Ÿæˆæ•°æ®æ¸…æ´—å»ºè®®"""
           print("ç”Ÿæˆæ¸…æ´—å»ºè®®...")
           
           recommendations = []
           
           # åŸºäºå®Œæ•´æ€§é—®é¢˜çš„å»ºè®®
           completeness = self.quality_metrics['completeness']
           if completeness['overall_missing_rate'] > 5:
               recommendations.append({
                   'category': 'å®Œæ•´æ€§',
                   'issue': f"æ€»ä½“ç¼ºå¤±ç‡è¾ƒé«˜ ({completeness['overall_missing_rate']:.1f}%)",
                   'recommendation': 'å®æ–½æ•°æ®å¡«å……ç­–ç•¥ï¼Œå¦‚å‡å€¼/ä¸­ä½æ•°å¡«å……ã€å‘å‰/å‘åå¡«å……æˆ–ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹ç¼ºå¤±å€¼',
                   'priority': 'é«˜' if completeness['overall_missing_rate'] > 10 else 'ä¸­'
               })
           
           # åŸºäºé«˜ç¼ºå¤±ç‡çš„åˆ—æå‡ºå…·ä½“å»ºè®®
           for col, rate in completeness['column_missing_rates'].items():
               if rate > 20:
                   recommendations.append({
                       'category': 'å®Œæ•´æ€§',
                       'issue': f"å­—æ®µ '{col}' ç¼ºå¤±ä¸¥é‡ ({rate:.1f}%)",
                       'recommendation': f'è€ƒè™‘åˆ é™¤è¯¥å­—æ®µæˆ–ä¸ä¸šåŠ¡éƒ¨é—¨æ²Ÿé€šç¡®å®šå¡«å……ç­–ç•¥',
                       'priority': 'é«˜'
                   })
           
           # åŸºäºå‡†ç¡®æ€§é—®é¢˜çš„å»ºè®®
           accuracy = self.quality_metrics['accuracy']
           for field, acc_rate in accuracy['format_accuracy'].items():
               if acc_rate < 95:
                   recommendations.append({
                       'category': 'å‡†ç¡®æ€§',
                       'issue': f"å­—æ®µ '{field}' æ ¼å¼å‡†ç¡®ç‡è¾ƒä½ ({acc_rate:.1f}%)",
                       'recommendation': f'å®æ–½æ•°æ®éªŒè¯è§„åˆ™ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼éªŒè¯{field}æ ¼å¼ï¼Œå¹¶ä¿®æ­£æˆ–åˆ é™¤æ— æ•ˆè®°å½•',
                       'priority': 'ä¸­' if acc_rate > 80 else 'é«˜'
                   })
           
           # åŸºäºä¸€è‡´æ€§é—®é¢˜çš„å»ºè®®
           consistency = self.quality_metrics['consistency']
           if consistency['duplicate_metrics']['complete_duplicates'] > 1:
               recommendations.append({
                   'category': 'ä¸€è‡´æ€§',
                   'issue': f"å‘ç°å®Œå…¨é‡å¤è®°å½• ({consistency['duplicate_metrics']['complete_duplicates']:.1f}%)",
                   'recommendation': 'å®æ–½å»é‡æµç¨‹ï¼ŒåŸºäºä¸šåŠ¡é”®åˆ é™¤é‡å¤è®°å½•',
                   'priority': 'é«˜'
               })
           
           # åŸºäºå¼‚å¸¸å€¼çš„å»ºè®®
           anomalies = self.quality_metrics['anomalies']
           for col, anomaly_info in anomalies.items():
               if col != 'multivariate_outliers':
                   if anomaly_info['z_score_outlier_rate'] > 5:
                       recommendations.append({
                           'category': 'å‡†ç¡®æ€§',
                           'issue': f"å­—æ®µ '{col}' å¼‚å¸¸å€¼è¾ƒå¤š (Z-score: {anomaly_info['z_score_outlier_rate']:.1f}%)",
                           'recommendation': f'åˆ†æå¼‚å¸¸å€¼åŸå› ï¼Œæ ¹æ®ä¸šåŠ¡è§„åˆ™è¿›è¡Œä¿®æ­£ã€åˆ é™¤æˆ–ä¿ç•™',
                           'priority': 'ä¸­'
                       })
           
           self.cleaning_recommendations = recommendations
           return recommendations
       
       def calculate_overall_quality_score(self):
           """è®¡ç®—æ€»ä½“æ•°æ®è´¨é‡è¯„åˆ†"""
           print("è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†...")
           
           completeness = self.quality_metrics['completeness']
           accuracy = self.quality_metrics['accuracy']
           consistency = self.quality_metrics['consistency']
           
           # æƒé‡åˆ†é…
           weights = {
               'completeness': 0.4,  # å®Œæ•´æ€§æœ€é‡è¦
               'accuracy': 0.35,     # å‡†ç¡®æ€§æ¬¡é‡è¦
               'consistency': 0.25   # ä¸€è‡´æ€§ç›¸å¯¹æ¬¡è¦
           }
           
           # å®Œæ•´æ€§å¾—åˆ† (åŸºäºå®Œæ•´è®°å½•æ¯”ä¾‹)
           completeness_score = completeness['complete_record_rate']
           
           # å‡†ç¡®æ€§å¾—åˆ† (å¹³å‡å„ç§å‡†ç¡®æ€§æŒ‡æ ‡)
           accuracy_components = []
           if accuracy['format_accuracy']:
               accuracy_components.append(np.mean(list(accuracy['format_accuracy'].values())))
           if accuracy['range_accuracy']:
               accuracy_components.append(np.mean(list(accuracy['range_accuracy'].values())))
           if accuracy['business_accuracy']:
               accuracy_components.append(np.mean(list(accuracy['business_accuracy'].values())))
           
           accuracy_score = np.mean(accuracy_components) if accuracy_components else 100
           
           # ä¸€è‡´æ€§å¾—åˆ†
           consistency_components = []
           if consistency['domain_consistency']:
               consistency_components.append(np.mean(list(consistency['domain_consistency'].values())))
           
           # è€ƒè™‘é‡å¤æ•°æ®çš„å½±å“
           duplicate_penalty = consistency['duplicate_metrics']['complete_duplicates']
           consistency_score = (np.mean(consistency_components) if consistency_components else 100) - duplicate_penalty
           
           # è®¡ç®—åŠ æƒæ€»åˆ†
           overall_score = (
               completeness_score * weights['completeness'] +
               accuracy_score * weights['accuracy'] +
               max(consistency_score, 0) * weights['consistency']
           )
           
           # ç¡®å®šè´¨é‡ç­‰çº§
           if overall_score >= 90:
               quality_grade = "ä¼˜ç§€"
           elif overall_score >= 80:
               quality_grade = "è‰¯å¥½"
           elif overall_score >= 70:
               quality_grade = "ä¸€èˆ¬"
           elif overall_score >= 60:
               quality_grade = "åŠæ ¼"
           else:
               quality_grade = "ä¸åŠæ ¼"
           
           self.quality_metrics['overall_score'] = overall_score
           self.quality_metrics['quality_grade'] = quality_grade
           
           return overall_score, quality_grade
       
       def generate_html_report(self):
           """ç”ŸæˆHTMLæ ¼å¼çš„æ•°æ®è´¨é‡æŠ¥å‘Š"""
           print("ç”ŸæˆHTMLæŠ¥å‘Š...")
           
           overall_score = self.quality_metrics.get('overall_score', 0)
           quality_grade = self.quality_metrics.get('quality_grade', 'æœªçŸ¥')
           
           html_content = f"""
           <!DOCTYPE html>
           <html lang="zh-CN">
           <head>
               <meta charset="UTF-8">
               <meta name="viewport" content="width=device-width, initial-scale=1.0">
               <title>æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š - {self.name}</title>
               <style>
                   body {{ font-family: Arial, sans-serif; margin: 20px; color: #333; }}
                   .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; }}
                   .score-card {{ background: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin-bottom: 20px; }}
                   .metric-card {{ background: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 4px solid #007bff; }}
                   .recommendation {{ background: #fff3cd; padding: 15px; border-radius: 8px; margin-bottom: 10px; border-left: 4px solid #ffc107; }}
                   .high-priority {{ border-left-color: #dc3545 !important; background: #f8d7da; }}
                   .medium-priority {{ border-left-color: #ffc107 !important; background: #fff3cd; }}
                   .low-priority {{ border-left-color: #28a745 !important; background: #d4edda; }}
                   .visualization {{ text-align: center; margin: 20px 0; }}
                   img {{ max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 5px; }}
                   table {{ width: 100%; border-collapse: collapse; margin: 15px 0; }}
                   th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                   th {{ background-color: #f2f2f2; }}
                   .quality-badge {{ display: inline-block; padding: 5px 15px; border-radius: 20px; color: white; font-weight: bold; }}
                   .excellent {{ background: #28a745; }}
                   .good {{ background: #17a2b8; }}
                   .average {{ background: #ffc107; color: #333; }}
                   .poor {{ background: #dc3545; }}
               </style>
           </head>
           <body>
               <div class="header">
                   <h1>æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š</h1>
                   <h2>{self.name}</h2>
                   <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
               </div>
               
               <div class="score-card">
                   <h2>æ€»ä½“è´¨é‡è¯„ä¼°</h2>
                   <div style="display: flex; align-items: center; gap: 20px;">
                       <div style="flex: 1;">
                           <h3>è´¨é‡è¯„åˆ†: <span class="quality-badge {quality_grade.lower()}">{overall_score:.1f}/100 - {quality_grade}</span></h3>
                           <p>æ•°æ®é›†åŒ…å« {len(self.df)} æ¡è®°å½•ï¼Œ{len(self.df.columns)} ä¸ªå­—æ®µ</p>
                       </div>
                       <div style="flex: 1;">
                           <img src="data:image/png;base64,{self.visualizations.get('quality_radar', '')}" alt="è´¨é‡é›·è¾¾å›¾">
                       </div>
                   </div>
               </div>
               
               <div class="score-card">
                   <h2>è¯¦ç»†è´¨é‡æŒ‡æ ‡</h2>
                   
                   <h3>å®Œæ•´æ€§æŒ‡æ ‡</h3>
                   <div class="metric-card">
                       <p><strong>æ€»ä½“ç¼ºå¤±ç‡:</strong> {self.quality_metrics['completeness']['overall_missing_rate']:.2f}%</p>
                       <p><strong>å®Œæ•´è®°å½•æ¯”ä¾‹:</strong> {self.quality_metrics['completeness']['complete_record_rate']:.2f}%</p>
                   </div>
                   
                   <h3>å‡†ç¡®æ€§æŒ‡æ ‡</h3>
                   <div class="metric-card">
           """
           
           # æ·»åŠ å‡†ç¡®æ€§æŒ‡æ ‡
           for field, accuracy in self.quality_metrics['accuracy']['format_accuracy'].items():
               html_content += f'<p><strong>{field}æ ¼å¼å‡†ç¡®ç‡:</strong> {accuracy:.2f}%</p>'
           
           for field, accuracy in self.quality_metrics['accuracy']['range_accuracy'].items():
               html_content += f'<p><strong>{field}èŒƒå›´å‡†ç¡®ç‡:</strong> {accuracy:.2f}%</p>'
           
           html_content += """
                   </div>
                   
                   <h3>ä¸€è‡´æ€§æŒ‡æ ‡</h3>
                   <div class="metric-card">
           """
           
           # æ·»åŠ ä¸€è‡´æ€§æŒ‡æ ‡
           html_content += f'<p><strong>å®Œå…¨é‡å¤è®°å½•:</strong> {self.quality_metrics["consistency"]["duplicate_metrics"]["complete_duplicates"]:.2f}%</p>'
           
           if 'key_duplicates' in self.quality_metrics["consistency"]["duplicate_metrics"]:
               html_content += f'<p><strong>å…³é”®å­—æ®µé‡å¤:</strong> {self.quality_metrics["consistency"]["duplicate_metrics"]["key_duplicates"]:.2f}%</p>'
           
           html_content += """
                   </div>
               </div>
               
               <div class="score-card">
                   <h2>æ•°æ®å¯è§†åŒ–</h2>
                   
                   <div class="visualization">
                       <h3>ç¼ºå¤±å€¼åˆ†æ</h3>
                       <img src="data:image/png;base64,""" + self.visualizations.get('missing_matrix', '') + """" alt="ç¼ºå¤±å€¼çŸ©é˜µ">
                   </div>
                   
                   <div class="visualization">
                       <h3>å­—æ®µå®Œæ•´æ€§</h3>
                       <img src="data:image/png;base64,""" + self.visualizations.get('completeness_bar', '') + """" alt="å­—æ®µå®Œæ•´æ€§">
                   </div>
           """
           
           # æ·»åŠ æ•°å€¼å­—æ®µåˆ†å¸ƒ
           if 'numerical_distributions' in self.visualizations:
               html_content += """
                   <div class="visualization">
                       <h3>æ•°å€¼å­—æ®µåˆ†å¸ƒä¸å¼‚å¸¸å€¼</h3>
                       <img src="data:image/png;base64,""" + self.visualizations['numerical_distributions'] + """" alt="æ•°å€¼å­—æ®µåˆ†å¸ƒ">
                   </div>
               """
           
           # æ·»åŠ åˆ†ç±»å­—æ®µåˆ†å¸ƒ
           if 'categorical_distributions' in self.visualizations:
               html_content += """
                   <div class="visualization">
                       <h3>åˆ†ç±»å­—æ®µåˆ†å¸ƒ</h3>
                       <img src="data:image/png;base64,""" + self.visualizations['categorical_distributions'] + """" alt="åˆ†ç±»å­—æ®µåˆ†å¸ƒ">
                   </div>
               """
           
           html_content += """
               </div>
               
               <div class="score-card">
                   <h2>æ•°æ®æ¸…æ´—å»ºè®®</h2>
           """
           
           # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„å»ºè®®
           high_priority = [r for r in self.cleaning_recommendations if r['priority'] == 'é«˜']
           medium_priority = [r for r in self.cleaning_recommendations if r['priority'] == 'ä¸­']
           low_priority = [r for r in self.cleaning_recommendations if r['priority'] == 'ä½']
           
           if high_priority:
               html_content += '<h3 style="color: #dc3545;">é«˜ä¼˜å…ˆçº§å»ºè®®</h3>'
               for rec in high_priority:
                   html_content += f"""
                   <div class="recommendation high-priority">
                       <h4>{rec['category']}: {rec['issue']}</h4>
                       <p>{rec['recommendation']}</p>
                   </div>
                   """
           
           if medium_priority:
               html_content += '<h3 style="color: #ffc107;">ä¸­ä¼˜å…ˆçº§å»ºè®®</h3>'
               for rec in medium_priority:
                   html_content += f"""
                   <div class="recommendation medium-priority">
                       <h4>{rec['category']}: {rec['issue']}</h4>
                       <p>{rec['recommendation']}</p>
                   </div>
                   """
           
           if low_priority:
               html_content += '<h3 style="color: #28a745;">ä½ä¼˜å…ˆçº§å»ºè®®</h3>'
               for rec in low_priority:
                   html_content += f"""
                   <div class="recommendation low-priority">
                       <h4>{rec['category']}: {rec['issue']}</h4>
                       <p>{rec['recommendation']}</p>
                   </div>
                   """
           
           if not self.cleaning_recommendations:
               html_content += '<p>æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€é‡å¤§æ¸…æ´—æ“ä½œã€‚</p>'
           
           html_content += """
               </div>
               
               <div class="score-card">
                   <h2>è¯¦ç»†è´¨é‡æŒ‡æ ‡è¡¨</h2>
                   <table>
                       <tr>
                           <th>æŒ‡æ ‡ç±»åˆ«</th>
                           <th>å…·ä½“æŒ‡æ ‡</th>
                           <th>æ•°å€¼</th>
                           <th>çŠ¶æ€</th>
                       </tr>
           """
           
           # æ·»åŠ å®Œæ•´æ€§æŒ‡æ ‡è¡¨æ ¼è¡Œ
           html_content += f"""
                       <tr>
                           <td rowspan="2">å®Œæ•´æ€§</td>
                           <td>æ€»ä½“ç¼ºå¤±ç‡</td>
                           <td>{self.quality_metrics['completeness']['overall_missing_rate']:.2f}%</td>
                           <td>{'è‰¯å¥½' if self.quality_metrics['completeness']['overall_missing_rate'] < 5 else 'éœ€æ”¹è¿›'}</td>
                       </tr>
                       <tr>
                           <td>å®Œæ•´è®°å½•æ¯”ä¾‹</td>
                           <td>{self.quality_metrics['completeness']['complete_record_rate']:.2f}%</td>
                           <td>{'è‰¯å¥½' if self.quality_metrics['completeness']['complete_record_rate'] > 90 else 'éœ€æ”¹è¿›'}</td>
                       </tr>
           """
           
           # æ·»åŠ å‡†ç¡®æ€§æŒ‡æ ‡è¡¨æ ¼è¡Œ
           row_count = 0
           for field, accuracy in self.quality_metrics['accuracy']['format_accuracy'].items():
               status = 'è‰¯å¥½' if accuracy > 95 else 'éœ€æ”¹è¿›'
               html_content += f"""
                       <tr>
                           <td>{'å‡†ç¡®æ€§' if row_count == 0 else ''}</td>
                           <td>{field}æ ¼å¼å‡†ç¡®ç‡</td>
                           <td>{accuracy:.2f}%</td>
                           <td>{status}</td>
                       </tr>
               """
               row_count += 1
           
           html_content += """
                   </table>
               </div>
               
               <footer style="text-align: center; margin-top: 50px; padding: 20px; border-top: 1px solid #ddd;">
                   <p>æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š - ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d')}</p>
               </footer>
           </body>
           </html>
           """
           
           return html_content
       
       def run_full_analysis(self):
           """è¿è¡Œå®Œæ•´çš„æ•°æ®è´¨é‡åˆ†æ"""
           print(f"å¼€å§‹åˆ†ææ•°æ®é›†: {self.name}")
           print("=" * 60)
           
           # è®¡ç®—å„é¡¹æŒ‡æ ‡
           self.calculate_completeness_metrics()
           self.calculate_accuracy_metrics()
           self.calculate_consistency_metrics()
           self.detect_anomalies()
           
           # åˆ›å»ºå¯è§†åŒ–
           self.create_visualizations()
           
           # ç”Ÿæˆå»ºè®®å’Œæ€»ä½“è¯„åˆ†
           self.generate_cleaning_recommendations()
           overall_score, quality_grade = self.calculate_overall_quality_score()
           
           # ç”ŸæˆæŠ¥å‘Š
           html_report = self.generate_html_report()
           
           # ä¿å­˜æŠ¥å‘Š
           report_filename = f"data_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
           with open(report_filename, 'w', encoding='utf-8') as f:
               f.write(html_report)
           
           print(f"\næ•°æ®è´¨é‡åˆ†æå®Œæˆ!")
           print(f"æ€»ä½“è´¨é‡è¯„åˆ†: {overall_score:.1f}/100 - {quality_grade}")
           print(f"HTMLæŠ¥å‘Šå·²ä¿å­˜: {report_filename}")
           
           return self.quality_metrics, self.cleaning_recommendations, report_filename
   
   def main():
       """ä¸»å‡½æ•°"""
       print("æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨")
       print("=" * 80)
       
       # åˆ›å»ºåˆ†æå™¨å®ä¾‹
       analyzer = DataQualityAnalyzer(pd.DataFrame(), "å®¢æˆ·æ•°æ®é›†")
       
       # åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥åŠ è½½çœŸå®æ•°æ®ï¼‰
       sample_data = analyzer.create_sample_data()
       analyzer.df = sample_data
       
       # è¿è¡Œå®Œæ•´åˆ†æ
       metrics, recommendations, report_file = analyzer.run_full_analysis()
       
       # æ‰“å°å…³é”®å‘ç°
       print("\n" + "=" * 80)
       print("å…³é”®å‘ç°æ‘˜è¦:")
       print("=" * 80)
       
       print(f"1. æ€»ä½“è´¨é‡: {metrics['overall_score']:.1f}/100 ({metrics['quality_grade']})")
       print(f"2. æ•°æ®å®Œæ•´æ€§: {metrics['completeness']['complete_record_rate']:.1f}%")
       print(f"3. å‘ç°æ¸…æ´—å»ºè®®: {len(recommendations)} æ¡")
       
       # æ˜¾ç¤ºé«˜ä¼˜å…ˆçº§å»ºè®®
       high_priority = [r for r in recommendations if r['priority'] == 'é«˜']
       if high_priority:
           print(f"\né«˜ä¼˜å…ˆçº§å»ºè®® ({len(high_priority)} æ¡):")
           for i, rec in enumerate(high_priority, 1):
               print(f"  {i}. {rec['issue']}")
       
       print(f"\nè¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹: {report_file}")
       print("=" * 80)
   
   if __name__ == "__main__":
       main()
   ```
   
   

## 20é“SQLè¯­æ³•å­¦ä¹ 

**åŸºç¡€æŸ¥è¯¢ï¼ˆ1-5ï¼‰**

1. ### ç®€å•æŸ¥è¯¢

   ```sql
   -- ä»employeesè¡¨ä¸­é€‰æ‹©æ‰€æœ‰å­—æ®µï¼Œåªæ˜¾ç¤ºå‰10æ¡è®°å½•
   ```

   ```sql
   -- ä»employeesè¡¨ä¸­é€‰æ‹©æ‰€æœ‰å­—æ®µï¼Œåªæ˜¾ç¤ºå‰10æ¡è®°å½•
   SELECT * FROM employees LIMIT 10;
   ```

2. ### æ¡ä»¶æŸ¥è¯¢

   ```sql
   -- æŸ¥è¯¢å·¥èµ„å¤§äº50000çš„å‘˜å·¥å§“åå’Œéƒ¨é—¨
   ```

   ```sql
   -- æŸ¥è¯¢å·¥èµ„å¤§äº50000çš„å‘˜å·¥å§“åå’Œéƒ¨é—¨
   SELECT name, department 
   FROM employees 
   WHERE salary > 50000;
   ```

3. ### å¤šæ¡ä»¶æŸ¥è¯¢

   ```sql
   -- æŸ¥è¯¢å§“åä»¥'å¼ 'å¼€å¤´ï¼Œä¸”åŒ…å«'ä¼Ÿ'å­—çš„å‘˜å·¥
   ```

   ```sql
   -- æŸ¥è¯¢åœ¨'æŠ€æœ¯éƒ¨'æˆ–'å¸‚åœºéƒ¨'å·¥ä½œï¼Œä¸”å·¥é¾„è¶…è¿‡3å¹´çš„å‘˜å·¥
   SELECT name, department, hire_date
   FROM employees 
   WHERE department IN ('æŠ€æœ¯éƒ¨', 'å¸‚åœºéƒ¨')
     AND DATEDIFF(CURDATE(), hire_date) > 3*365;
   ```

4. ### æ¨¡ç³ŠæŸ¥è¯¢

   ```sql
   -- æŸ¥è¯¢å§“åä»¥'å¼ 'å¼€å¤´ï¼Œä¸”åŒ…å«'ä¼Ÿ'å­—çš„å‘˜å·¥
   ```

   ```sql
   -- æŸ¥è¯¢å§“åä»¥'å¼ 'å¼€å¤´ï¼Œä¸”åŒ…å«'ä¼Ÿ'å­—çš„å‘˜å·¥
   SELECT name
   FROM employees
   WHERE name LIKE 'å¼ %' AND name LIKE '%ä¼Ÿ%';
   ```

   

5. ### èŒƒå›´æŸ¥è¯¢

   ```sql
   -- æŸ¥è¯¢å¹´é¾„åœ¨25åˆ°35å²ä¹‹é—´ï¼Œæˆ–è€…å·¥èµ„åœ¨30000åˆ°50000ä¹‹é—´çš„å‘˜å·¥
   ```
   
   ```sql
   -- æŸ¥è¯¢å¹´é¾„åœ¨25åˆ°35å²ä¹‹é—´ï¼Œæˆ–è€…å·¥èµ„åœ¨30000åˆ°50000ä¹‹é—´çš„å‘˜å·¥
   SELECT name, age, salary
   FROM employees
   WHERE (age BETWEEN 25 AND 35) 
      OR (salary BETWEEN 30000 AND 50000);
   ```

**èšåˆå‡½æ•°ï¼ˆ6-10ï¼‰**

6. ### åŸºæœ¬èšåˆ

   ```sql
   -- è®¡ç®—å„éƒ¨é—¨å‘˜å·¥æ•°é‡ã€å¹³å‡å·¥èµ„ã€æœ€é«˜å’Œæœ€ä½å·¥èµ„
   ```

   ```sql
   -- è®¡ç®—å„éƒ¨é—¨å‘˜å·¥æ•°é‡ã€å¹³å‡å·¥èµ„ã€æœ€é«˜å’Œæœ€ä½å·¥èµ„
   SELECT 
       department,
       COUNT(*) as å‘˜å·¥æ•°é‡,
       AVG(salary) as å¹³å‡å·¥èµ„,
       MAX(salary) as æœ€é«˜å·¥èµ„,
       MIN(salary) as æœ€ä½å·¥èµ„
   FROM employees
   GROUP BY department;
   ```

7. ### åˆ†ç»„ç»Ÿè®¡

   ```sql
   -- ç»Ÿè®¡æ¯ä¸ªéƒ¨é—¨æ¯ä¸ªèŒçº§çš„å‘˜å·¥æ•°é‡å’Œå¹³å‡å·¥èµ„
   ```

   ```sql
   -- ç»Ÿè®¡æ¯ä¸ªéƒ¨é—¨æ¯ä¸ªèŒçº§çš„å‘˜å·¥æ•°é‡å’Œå¹³å‡å·¥èµ„
   SELECT 
       department,
       position,
       COUNT(*) as å‘˜å·¥æ•°é‡,
       AVG(salary) as å¹³å‡å·¥èµ„
   FROM employees
   GROUP BY department, position
   ORDER BY department, position;
   ```

8. ### åˆ†ç»„è¿‡æ»¤

   ```sql
   -- æŸ¥è¯¢å‘˜å·¥æ•°é‡è¶…è¿‡10äººçš„éƒ¨é—¨åŠå…¶å¹³å‡å·¥èµ„
   ```

   ```sql
   -- æŸ¥è¯¢å‘˜å·¥æ•°é‡è¶…è¿‡10äººçš„éƒ¨é—¨åŠå…¶å¹³å‡å·¥èµ„
   SELECT 
       department,
       COUNT(*) as å‘˜å·¥æ•°é‡,
       AVG(salary) as å¹³å‡å·¥èµ„
   FROM employees
   GROUP BY department
   HAVING COUNT(*) > 10;
   ```

9. ### å¤šç»´åº¦èšåˆ

   ```sql
   -- æŒ‰éƒ¨é—¨å’Œæ€§åˆ«ç»Ÿè®¡å‘˜å·¥æ•°é‡ï¼Œå¹¶è®¡ç®—å„éƒ¨é—¨çš„ç”·å¥³æ¯”ä¾‹
   ```

   ```sql
   -- æŒ‰éƒ¨é—¨å’Œæ€§åˆ«ç»Ÿè®¡å‘˜å·¥æ•°é‡ï¼Œå¹¶è®¡ç®—å„éƒ¨é—¨çš„ç”·å¥³æ¯”ä¾‹
   WITH dept_gender_stats AS (
       SELECT 
           department,
           gender,
           COUNT(*) as count
       FROM employees
       GROUP BY department, gender
   ),
   dept_totals AS (
       SELECT 
           department,
           SUM(count) as total
       FROM dept_gender_stats
       GROUP BY department
   )
   SELECT 
       dgs.department,
       dgs.gender,
       dgs.count,
       ROUND(dgs.count / dt.total * 100, 2) as æ¯”ä¾‹
   FROM dept_gender_stats dgs
   JOIN dept_totals dt ON dgs.department = dt.department
   ORDER BY dgs.department, dgs.gender;
   ```

   

10. ### å¤æ‚èšåˆ

    ```sql
    -- è®¡ç®—æ¯ä¸ªéƒ¨é—¨çš„å·¥èµ„å·®å¼‚ç³»æ•°ï¼ˆæ ‡å‡†å·®/å¹³å‡å€¼ï¼‰
    ```
    
    ```sql
    -- è®¡ç®—æ¯ä¸ªéƒ¨é—¨çš„å·¥èµ„å·®å¼‚ç³»æ•°ï¼ˆæ ‡å‡†å·®/å¹³å‡å€¼ï¼‰
    SELECT 
        department,
        COUNT(*) as å‘˜å·¥æ•°é‡,
        AVG(salary) as å¹³å‡å·¥èµ„,
        STDDEV(salary) as å·¥èµ„æ ‡å‡†å·®,
        ROUND(STDDEV(salary) / AVG(salary), 4) as å·®å¼‚ç³»æ•°
    FROM employees
    GROUP BY department
    HAVING COUNT(*) > 1;
    ```

**å¤šè¡¨è¿æ¥ï¼ˆ11-15ï¼‰**

11. ### å†…è¿æ¥

    ```sql
    -- æŸ¥è¯¢å‘˜å·¥åŠå…¶æ‰€å±éƒ¨é—¨åç§°ï¼ˆemployeeså’Œdepartmentsè¡¨è¿æ¥ï¼‰
    ```

    ```sql
    -- æŸ¥è¯¢å‘˜å·¥åŠå…¶æ‰€å±éƒ¨é—¨åç§°ï¼ˆemployeeså’Œdepartmentsè¡¨è¿æ¥ï¼‰
    SELECT 
        e.emp_id,
        e.name,
        e.department,
        d.dept_name,
        d.location
    FROM employees e
    JOIN departments d ON e.department = d.dept_name;
    ```

12. ### å·¦è¿æ¥

    ```sql
    -- æŸ¥è¯¢æ‰€æœ‰å‘˜å·¥ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ²¡æœ‰åˆ†é…éƒ¨é—¨çš„å‘˜å·¥
    ```

    ```sql
    -- æŸ¥è¯¢æ‰€æœ‰å‘˜å·¥ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ²¡æœ‰åˆ†é…éƒ¨é—¨çš„å‘˜å·¥
    SELECT 
        e.emp_id,
        e.name,
        e.department,
        d.dept_name,
        d.location
    FROM employees e
    LEFT JOIN departments d ON e.department = d.dept_name;
    ```

13. ### å¤šè¡¨è¿æ¥

    ```sql
    -- æŸ¥è¯¢å‘˜å·¥å§“åã€éƒ¨é—¨åç§°å’Œé¡¹ç›®åç§°ï¼ˆä¸‰ä¸ªè¡¨è¿æ¥ï¼‰
    ```

    ```sql
    -- æŸ¥è¯¢å‘˜å·¥å§“åã€éƒ¨é—¨åç§°å’Œé¡¹ç›®åç§°ï¼ˆä¸‰ä¸ªè¡¨è¿æ¥ï¼‰
    SELECT 
        e.name as å‘˜å·¥å§“å,
        d.dept_name as éƒ¨é—¨åç§°,
        p.project_name as é¡¹ç›®åç§°,
        ep.role as è§’è‰²
    FROM employees e
    JOIN departments d ON e.department = d.dept_name
    JOIN employee_projects ep ON e.emp_id = ep.emp_id
    JOIN projects p ON ep.project_id = p.project_id;
    ```

14. ### è‡ªè¿æ¥

    ```sql
    -- æŸ¥è¯¢æ¯ä¸ªå‘˜å·¥åŠå…¶ç›´æ¥ä¸Šçº§çš„å§“å
    ```

    ```sql
    -- æŸ¥è¯¢æ¯ä¸ªå‘˜å·¥åŠå…¶ç›´æ¥ä¸Šçº§çš„å§“å
    SELECT 
        e.emp_id,
        e.name as å‘˜å·¥å§“å,
        e.manager_id,
        m.name as ç»ç†å§“å
    FROM employees e
    LEFT JOIN employees m ON e.manager_id = m.emp_id;
    ```

15. ### å¤æ‚è¿æ¥

    ```sql
    -- æŸ¥è¯¢å‚ä¸äº†å¤šä¸ªé¡¹ç›®çš„å‘˜å·¥åŠå…¶å‚ä¸çš„é¡¹ç›®æ•°é‡
    ```
    
    ```sql
    -- æŸ¥è¯¢å‚ä¸äº†å¤šä¸ªé¡¹ç›®çš„å‘˜å·¥åŠå…¶å‚ä¸çš„é¡¹ç›®æ•°é‡
    SELECT 
        e.emp_id,
        e.name,
        COUNT(ep.project_id) as é¡¹ç›®æ•°é‡,
        GROUP_CONCAT(p.project_name) as å‚ä¸é¡¹ç›®
    FROM employees e
    JOIN employee_projects ep ON e.emp_id = ep.emp_id
    JOIN projects p ON ep.project_id = p.project_id
    GROUP BY e.emp_id, e.name
    HAVING COUNT(ep.project_id) > 1;
    ```

**å­æŸ¥è¯¢å’Œé«˜çº§åŠŸèƒ½ï¼ˆ16-20ï¼‰**

16. ### å­æŸ¥è¯¢

    ```sql
    -- æŸ¥è¯¢å·¥èµ„é«˜äºæœ¬éƒ¨é—¨å¹³å‡å·¥èµ„çš„å‘˜å·¥
    ```

    ```sql
    -- æŸ¥è¯¢å·¥èµ„é«˜äºæœ¬éƒ¨é—¨å¹³å‡å·¥èµ„çš„å‘˜å·¥
    SELECT 
        e.emp_id,
        e.name,
        e.department,
        e.salary,
        (SELECT AVG(salary) FROM employees WHERE department = e.department) as éƒ¨é—¨å¹³å‡å·¥èµ„
    FROM employees e
    WHERE e.salary > (SELECT AVG(salary) FROM employees WHERE department = e.department);
    ```

    

17. ### ç›¸å…³å­æŸ¥è¯¢

    ```sql
    -- æŸ¥è¯¢æ¯ä¸ªéƒ¨é—¨å·¥èµ„æœ€é«˜çš„å‘˜å·¥ä¿¡æ¯
    ```

    ```sql
    -- æŸ¥è¯¢æ¯ä¸ªéƒ¨é—¨å·¥èµ„æœ€é«˜çš„å‘˜å·¥ä¿¡æ¯
    SELECT 
        e.emp_id,
        e.name,
        e.department,
        e.salary
    FROM employees e
    WHERE e.salary = (
        SELECT MAX(salary) 
        FROM employees 
        WHERE department = e.department
    )
    ```

18. ### çª—å£å‡½æ•°

    ```sql
    -- ä¸ºæ¯ä¸ªéƒ¨é—¨çš„å‘˜å·¥æŒ‰å·¥èµ„æ’åï¼Œæ˜¾ç¤ºæ’åå’Œå·¥èµ„
    ```

    ```sql
    -- ä¸ºæ¯ä¸ªéƒ¨é—¨çš„å‘˜å·¥æŒ‰å·¥èµ„æ’åï¼Œæ˜¾ç¤ºæ’åå’Œå·¥èµ„
    SELECT 
        emp_id,
        name,
        department,
        salary,
        RANK() OVER (PARTITION BY department ORDER BY salary DESC) as éƒ¨é—¨æ’å,
        DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as éƒ¨é—¨å¯†é›†æ’å,
        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as éƒ¨é—¨è¡Œå·
    FROM employees;
    ```

19. ### CASEè¡¨è¾¾å¼

    ```sql
    -- å°†å‘˜å·¥æŒ‰å·¥èµ„æ°´å¹³åˆ†ç±»ï¼šé«˜(>80000)ã€ä¸­(40000-80000)ã€ä½(<40000)
    ```

    ```sql
    -- å°†å‘˜å·¥æŒ‰å·¥èµ„æ°´å¹³åˆ†ç±»ï¼šé«˜(>80000)ã€ä¸­(40000-80000)ã€ä½(<40000)
    SELECT 
        emp_id,
        name,
        department,
        salary,
        CASE 
            WHEN salary > 80000 THEN 'é«˜'
            WHEN salary BETWEEN 40000 AND 80000 THEN 'ä¸­'
            ELSE 'ä½'
        END as å·¥èµ„æ°´å¹³
    FROM employees
    ORDER BY salary DESC;
    ```

    

20. ### é€’å½’æŸ¥è¯¢

    ```sql
    -- æŸ¥è¯¢å‘˜å·¥çš„æ‰€æœ‰ä¸‹å±ï¼ˆåŒ…æ‹¬é—´æ¥ä¸‹å±ï¼‰
    ```
    
    ```sql
    -- æŸ¥è¯¢å‘˜å·¥çš„æ‰€æœ‰ä¸‹å±ï¼ˆåŒ…æ‹¬é—´æ¥ä¸‹å±ï¼‰
    WITH RECURSIVE employee_hierarchy AS (
        -- åŸºç¡€æƒ…å†µï¼šç›´æ¥ä¸‹å±
        SELECT 
            emp_id,
            name,
            manager_id,
            1 as level,
            CAST(name AS CHAR(1000)) as hierarchy_path
        FROM employees
        WHERE manager_id IS NULL
        
        UNION ALL
        
        -- é€’å½’æƒ…å†µï¼šé—´æ¥ä¸‹å±
        SELECT 
            e.emp_id,
            e.name,
            e.manager_id,
            eh.level + 1,
            CONCAT(eh.hierarchy_path, ' -> ', e.name)
        FROM employees e
        JOIN employee_hierarchy eh ON e.manager_id = eh.emp_id
    )
    SELECT 
        emp_id,
        name,
        manager_id,
        level,
        hierarchy_path
    FROM employee_hierarchy
    ORDER BY level, emp_id;
    ```

### æ•°æ®æ¸…æ´—SQLå®ç°

```sql
-- ä½¿ç”¨SQLè¯­å¥æ¸…æ´—æ•°æ®ï¼š
-- 1. å¡«å……ç¼ºå¤±çš„éƒ¨é—¨ä¿¡æ¯ï¼ˆç”¨ä¸Šä¸€æœ‰æ•ˆå€¼å¡«å……ï¼‰
-- 2. åˆ é™¤é‡å¤çš„å‘˜å·¥è®°å½•ï¼ˆä¿ç•™æœ€æ–°è®°å½•ï¼‰
-- 3. æ ‡å‡†åŒ–ç”µè¯å·ç æ ¼å¼
-- 4. æ ‡è®°å¼‚å¸¸å·¥èµ„å€¼ï¼ˆè¶…å‡º3ä¸ªæ ‡å‡†å·®ï¼‰
```

```sql
-- ä½¿ç”¨SQLè¯­å¥æ¸…æ´—æ•°æ®ï¼š
-- 1. å¡«å……ç¼ºå¤±çš„éƒ¨é—¨ä¿¡æ¯ï¼ˆç”¨ä¸Šä¸€æœ‰æ•ˆå€¼å¡«å……ï¼‰
-- 2. åˆ é™¤é‡å¤çš„å‘˜å·¥è®°å½•ï¼ˆä¿ç•™æœ€æ–°è®°å½•ï¼‰
-- 3. æ ‡å‡†åŒ–ç”µè¯å·ç æ ¼å¼
-- 4. æ ‡è®°å¼‚å¸¸å·¥èµ„å€¼ï¼ˆè¶…å‡º3ä¸ªæ ‡å‡†å·®ï¼‰

-- åˆ›å»ºéœ€è¦æ¸…æ´—çš„æ•°æ®è¡¨
CREATE TABLE employee_cleanup (
    emp_id INT,
    name VARCHAR(50),
    department VARCHAR(50),
    phone VARCHAR(20),
    salary DECIMAL(10,2),
    record_date DATE
);

INSERT INTO employee_cleanup VALUES
(1, 'å¼ ä¸‰', 'æŠ€æœ¯éƒ¨', '13812345678', 60000, '2023-01-15'),
(1, 'å¼ ä¸‰', NULL, '138-1234-5678', 60000, '2023-01-16'),
(2, 'æå››', 'å¸‚åœºéƒ¨', '1351234567', 80000, '2023-01-15'),
(3, 'ç‹äº”', NULL, '13612345678', 55000, '2023-01-17'),
(4, 'èµµå…­', 'æŠ€æœ¯éƒ¨', '13912345678', 250000, '2023-01-15'),
(5, 'é’±ä¸ƒ', 'è´¢åŠ¡éƒ¨', '1371234abcd', 90000, '2023-01-18'),
(2, 'æå››', 'å¸‚åœºéƒ¨', '1351234567', 80000, '2023-01-19');

-- æ•°æ®æ¸…æ´—è§£å†³æ–¹æ¡ˆ
WITH cleaned_data AS (
    -- 1. å¡«å……ç¼ºå¤±çš„éƒ¨é—¨ä¿¡æ¯ï¼ˆç”¨ä¸Šä¸€æœ‰æ•ˆå€¼å¡«å……ï¼‰
    SELECT 
        emp_id,
        name,
        CASE 
            WHEN department IS NULL THEN 
                LAG(department) OVER (PARTITION BY emp_id ORDER BY record_date)
            ELSE department
        END as department_filled,
        
        -- 2. æ ‡å‡†åŒ–ç”µè¯å·ç æ ¼å¼
        REGEXP_REPLACE(phone, '[^0-9]', '') as phone_cleaned,
        
        salary,
        record_date,
        
        -- 3. æ ‡è®°å¼‚å¸¸å·¥èµ„å€¼
        CASE 
            WHEN ABS(salary - (SELECT AVG(salary) FROM employee_cleanup)) > 
                 3 * (SELECT STDDEV(salary) FROM employee_cleanup) THEN 'å¼‚å¸¸'
            ELSE 'æ­£å¸¸'
        END as salary_status,
        
        -- ç”¨äºå»é‡çš„è¡Œå·
        ROW_NUMBER() OVER (
            PARTITION BY emp_id, name 
            ORDER BY record_date DESC
        ) as rn
    FROM employee_cleanup
)
SELECT 
    emp_id,
    name,
    department_filled as department,
    CASE 
        WHEN LENGTH(phone_cleaned) = 11 THEN 
            CONCAT(SUBSTR(phone_cleaned, 1, 3), '-', 
                   SUBSTR(phone_cleaned, 4, 4), '-', 
                   SUBSTR(phone_cleaned, 8, 4))
        ELSE phone_cleaned
    END as phone,
    salary,
    salary_status,
    record_date
FROM cleaned_data
-- 4. åˆ é™¤é‡å¤è®°å½•ï¼ˆä¿ç•™æœ€æ–°è®°å½•ï¼‰
WHERE rn = 1
ORDER BY emp_id, record_date;
```



# æµ‹è¯•æ•°æ®

## æ•°æ®æ¸…æ´—è„šæœ¬æµ‹è¯•æ•°æ®

1. ### ç¼ºå¤±å€¼å¤„ç†

   **æµ‹è¯•æ•°æ®ï¼ˆstundents.csvï¼‰ï¼š**

   ```csv
   id,name,subject,score,exam_date
   1,å¼ ä¸‰,æ•°å­¦,85,2023-06-01
   2,æå››,æ•°å­¦,,2023-06-01
   3,ç‹äº”,è‹±è¯­,92,
   4,,æ•°å­¦,78,2023-06-01
   5,èµµå…­,è‹±è¯­,88,2023-06-02
   ```

2. ### é‡å¤æ•°æ®å¤„ç†

   **æµ‹è¯•æ•°æ®ï¼ˆstundent_records.csvï¼‰:**

   ```csv
   student_id,name,subject,score,record_date
   1001,å¼ ä¸‰,æ•°å­¦,85,2023-01-15
   1001,å¼ ä¸‰,æ•°å­¦,90,2023-02-20
   1002,æå››,è‹±è¯­,78,2023-01-15
   1001,å¼ ä¸‰,æ•°å­¦,85,2023-01-15
   1003,ç‹äº”,æ•°å­¦,92,2023-01-16
   1002,æå››,è‹±è¯­,82,2023-02-18
   ```

3. ### æ•°æ®ç±»å‹è½¬æ¢

   **æµ‹è¯•æ•°æ®ï¼ˆmixed_data.csvï¼‰ï¼š**

   ```csv
   id,date_str,price_str,quantity
   1,2023-01-15,125.50,10
   2,2023/02/20,abc,5
   3,2023-03-25,78.00,8
   4,invalid_date,45.30,12
   5,2023-05-10,120.00,15
   ```

4. ### å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†

   **æµ‹è¯•æ•°æ®ï¼ˆscores.csvï¼‰ï¼š**

   ```csv
   student_id,math_score,english_score,physics_score
   1,85,92,78
   2,120,88,85
   3,95,-5,92
   4,65,72,150
   5,88,91,76
   6,45,38,42
   7,99,105,98
   ```

5. ### æ–‡æœ¬æ•°æ®æ¸…æ´—

   **æµ‹è¯•æ•°æ®ï¼ˆuser_comments.csvï¼‰ï¼š**

   ```csv
   user_id,comment,phone,email,comment_date
   1,è¿™ä¸ªäº§å“<em>å¾ˆå¥½</em>!,13812345678,user1@example.com,2023/01/15
   2,ä¸æ»¡æ„!!!,135-1234-5678,user2@gmail.com,2023å¹´2æœˆ20æ—¥
   3,ä¸€èˆ¬èˆ¬,123456,user3,2023-03-25
   4,<div>æ¨èè´­ä¹°</div>,13800138000,user4@qq.com,2023-04-10
   ```

6. ### æ•°æ®æ ‡å‡†åŒ–

   **æµ‹è¯•æ•°æ®ï¼ˆproduct_data.csvï¼‰ï¼š**

   ```csv
   product_id,price,sales_volume,rating
   1,299,1500,4.5
   2,899,800,4.8
   3,1599,300,4.2
   4,499,1200,4.6
   5,1299,450,4.4
   ```

7. ### å¤šæºæ•°æ®æ•´åˆ

   **CSVæ•°æ®ï¼ˆsales_q1.csvï¼‰ï¼š**

   ```csv
   order_id,customer_id,product,amount,order_date
   1001,C001,äº§å“A,299,2023-01-15
   1002,C002,äº§å“B,899,2023-02-20
   ```

   **Excelæ•°æ®ï¼ˆsales_q2.xlsxï¼‰ï¼š**

   ```text
   è®¢å•ID,å®¢æˆ·ID,äº§å“åç§°,é‡‘é¢,ä¸‹å•æ—¥æœŸ
   1003,C003,äº§å“C,1599,2023-04-10
   1004,C001,äº§å“A,299,2023-05-15
   ```

   **JSONæ•°æ®ï¼ˆsales_q3.jsonï¼‰ï¼š**

   ```json
   [
     {"order_id": "1005", "customer_id": "C002", "product": "äº§å“B", "amount": 899, "order_date": "2023-07-20"},
     {"order_id": "1006", "customer_id": "C004", "product": "äº§å“D", "amount": 499, "order_date": "2023-08-15"}
   ]
   ```

## SQLè¯­æ³•å­¦ä¹ æµ‹è¯•æ•°æ®

**å»ºè¡¨è¯­å¥å’Œæµ‹è¯•æ•°æ®**

```sql
-- åˆ›å»ºæ•°æ®åº“å’Œè¡¨
CREATE DATABASE practice_db;
USE practice_db;

-- å‘˜å·¥è¡¨
CREATE TABLE employees (
    emp_id INT PRIMARY KEY,
    name VARCHAR(50),
    age INT,
    gender VARCHAR(10),
    salary DECIMAL(10,2),
    department VARCHAR(50),
    position VARCHAR(50),
    hire_date DATE,
    manager_id INT
);

-- éƒ¨é—¨è¡¨
CREATE TABLE departments (
    dept_id INT PRIMARY KEY,
    dept_name VARCHAR(50),
    location VARCHAR(50)
);

-- é¡¹ç›®è¡¨
CREATE TABLE projects (
    project_id INT PRIMARY KEY,
    project_name VARCHAR(100),
    start_date DATE,
    end_date DATE,
    budget DECIMAL(12,2)
);

-- å‘˜å·¥é¡¹ç›®å…³è”è¡¨
CREATE TABLE employee_projects (
    emp_id INT,
    project_id INT,
    role VARCHAR(50),
    join_date DATE,
    PRIMARY KEY (emp_id, project_id)
);

-- æ’å…¥æµ‹è¯•æ•°æ®
INSERT INTO employees VALUES
(1, 'å¼ ä¸‰', 28, 'ç”·', 60000, 'æŠ€æœ¯éƒ¨', 'å·¥ç¨‹å¸ˆ', '2020-03-15', NULL),
(2, 'æå››', 35, 'ç”·', 80000, 'æŠ€æœ¯éƒ¨', 'ç»ç†', '2018-06-01', NULL),
(3, 'ç‹äº”', 26, 'å¥³', 55000, 'å¸‚åœºéƒ¨', 'ä¸“å‘˜', '2021-08-20', 5),
(4, 'èµµå…­', 32, 'ç”·', 75000, 'æŠ€æœ¯éƒ¨', 'é«˜çº§å·¥ç¨‹å¸ˆ', '2019-02-10', 2),
(5, 'é’±ä¸ƒ', 40, 'å¥³', 90000, 'å¸‚åœºéƒ¨', 'æ€»ç›‘', '2016-11-05', NULL),
(6, 'å­™å…«', 29, 'å¥³', 58000, 'äººåŠ›èµ„æºéƒ¨', 'ä¸“å‘˜', '2020-09-12', 8),
(7, 'å‘¨ä¹', 45, 'ç”·', 95000, 'æŠ€æœ¯éƒ¨', 'æ€»ç›‘', '2015-04-20', NULL),
(8, 'å´å', 38, 'å¥³', 82000, 'äººåŠ›èµ„æºéƒ¨', 'ç»ç†', '2017-07-30', NULL),
(9, 'éƒ‘åä¸€', 31, 'ç”·', 68000, 'å¸‚åœºéƒ¨', 'ç»ç†', '2019-12-01', 5),
(10, 'ç‹ä¼Ÿ', 27, 'ç”·', 52000, 'æŠ€æœ¯éƒ¨', 'å·¥ç¨‹å¸ˆ', '2022-01-15', 2),
(11, 'å¼ ä¼Ÿ', 33, 'å¥³', 72000, 'è´¢åŠ¡éƒ¨', 'ç»ç†', '2018-09-10', NULL),
(12, 'æä¼Ÿ', 41, 'ç”·', 88000, 'æŠ€æœ¯éƒ¨', 'æ¶æ„å¸ˆ', '2017-03-25', 7);

INSERT INTO departments VALUES
(1, 'æŠ€æœ¯éƒ¨', 'åŒ—äº¬'),
(2, 'å¸‚åœºéƒ¨', 'ä¸Šæµ·'),
(3, 'äººåŠ›èµ„æºéƒ¨', 'åŒ—äº¬'),
(4, 'è´¢åŠ¡éƒ¨', 'å¹¿å·'),
(5, 'è¿è¥éƒ¨', 'æ·±åœ³');

INSERT INTO projects VALUES
(1, 'ç”µå•†å¹³å°å‡çº§', '2023-01-01', '2023-06-30', 500000),
(2, 'ç§»åŠ¨Appå¼€å‘', '2023-02-15', '2023-08-31', 300000),
(3, 'å¸‚åœºæ¨å¹¿æ´»åŠ¨', '2023-03-01', '2023-05-31', 150000),
(4, 'HRç³»ç»Ÿé‡æ„', '2023-04-01', '2023-09-30', 200000),
(5, 'æ•°æ®åˆ†æå¹³å°', '2023-05-15', '2023-12-31', 400000);

INSERT INTO employee_projects VALUES
(1, 1, 'å¼€å‘å·¥ç¨‹å¸ˆ', '2023-01-01'),
(1, 2, 'åç«¯å¼€å‘', '2023-02-20'),
(2, 1, 'é¡¹ç›®ç»ç†', '2023-01-01'),
(3, 3, 'æ´»åŠ¨ç­–åˆ’', '2023-03-01'),
(4, 1, 'æŠ€æœ¯è´Ÿè´£äºº', '2023-01-01'),
(4, 2, 'æ¶æ„å¸ˆ', '2023-02-15'),
(5, 3, 'é¡¹ç›®æ€»ç›‘', '2023-03-01'),
(6, 4, 'éœ€æ±‚åˆ†æ', '2023-04-01'),
(7, 5, 'æŠ€æœ¯é¡¾é—®', '2023-05-15'),
(8, 4, 'é¡¹ç›®ç»ç†', '2023-04-01'),
(9, 3, 'æ‰§è¡Œç»ç†', '2023-03-01'),
(10, 2, 'å‰ç«¯å¼€å‘', '2023-02-15');
```

**SQLç»ƒä¹ é¢˜å¯¹åº”çš„æ•°æ®è¯´æ˜**

*åŸºç¡€æŸ¥è¯¢ï¼ˆ1-5é¢˜ï¼‰*

ä½¿ç”¨`employees`è¡¨æ•°æ®

*èšåˆå‡½æ•°ï¼ˆ6-10é¢˜ï¼‰*

ä½¿ç”¨`employees`è¡¨å’Œåˆ†ç»„ç»Ÿè®¡

*å¤šè¡¨è¿æ¥ï¼ˆ11-15é¢˜ï¼‰*

ä½¿ç”¨`employees`,`departments`,`projects`,`employee_projects`è¡¨

*å­æŸ¥è¯¢å’Œé«˜çº§åŠŸèƒ½ï¼ˆ16-20é¢˜ï¼‰*

ä½¿ç”¨æ‰€æœ‰è¡¨è¿›è¡Œå¤æ‚æŸ¥è¯¢

**é™„åŠ æ•°æ®æ¸…æ´—SQLé¢˜ç›®æ•°æ®**

```sql
-- åˆ›å»ºéœ€è¦æ¸…æ´—çš„æ•°æ®è¡¨
CREATE TABLE employee_cleanup (
    emp_id INT,
    name VARCHAR(50),
    department VARCHAR(50),
    phone VARCHAR(20),
    salary DECIMAL(10,2),
    record_date DATE
);

INSERT INTO employee_cleanup VALUES
(1, 'å¼ ä¸‰', 'æŠ€æœ¯éƒ¨', '13812345678', 60000, '2023-01-15'),
(1, 'å¼ ä¸‰', NULL, '138-1234-5678', 60000, '2023-01-16'),
(2, 'æå››', 'å¸‚åœºéƒ¨', '1351234567', 80000, '2023-01-15'),
(3, 'ç‹äº”', NULL, '13612345678', 55000, '2023-01-17'),
(4, 'èµµå…­', 'æŠ€æœ¯éƒ¨', '13912345678', 250000, '2023-01-15'),
(5, 'é’±ä¸ƒ', 'è´¢åŠ¡éƒ¨', '1371234abcd', 90000, '2023-01-18'),
(2, 'æå››', 'å¸‚åœºéƒ¨', '1351234567', 80000, '2023-01-19');
```













